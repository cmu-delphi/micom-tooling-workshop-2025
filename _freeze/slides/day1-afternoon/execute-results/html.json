{
  "hash": "3b391540d698371db96f846f655a7c45",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntalk-title: \"Data Cleaning, Versioning, Nowcasting With `{epiprocess}`\"\ntalk-short-title: \"Nowcasting\"\ntalk-subtitle: \"InsightNet Forecasting Workshop 2024\"\ntalk-date: \"11 December -- Afternoon\"\nformat: revealjs\n---\n\n---\n---\n\n\n\\DeclareMathOperator*{\\minimize}{minimize}\n\n\n\n\n\n\n\n\n\n::: flex\n::: w-20\n\n:::\n::: w-80\n## {{< meta talk-title >}} {background-image=\"gfx/cover-art-1.svg\" background-position=\"bottom\"}\n\n### {{< meta talk-subtitle >}}\n\n<br>\n\n#### {{< meta author >}} \n[with huge thanks to Logan Brooks, Xueda Shen, and also to Nat DeFries, Dmitry Shemetov, and David Weber]{.fstyle}\n\n\n{{< meta talk-date >}}\n\n\n:::\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Outline\n\n1. Essentials of `{dplyr}` and `{tidyr}` \n\n1. Epiverse Software ecosystem\n\n1. Panel and Versioned Data in the epiverse\n\n1. Basic Nowcasting using `{epiprocess}`\n\n1. Nowcasting with One Variable\n\n1. Nowcasting with Two Variables\n\n1. Case Study - Nowcasting Cases Using %CLI\n\n\n# Essentials of `{dplyr}` and `{tidyr}` \n\n## Down with spreadsheets for data manipulation\n\n* Spreadsheets make it difficult to rerun analyses consistently.\n* Using R (and `{dplyr}`) allows for:\n  * Reproducibility \n  * Ease of modification\n* [**Recommendation**]{.primary}: Avoid manual edits; instead, use code for transformations.\n* Let's see what we mean by this...\n\n## Introduction to `dplyr`\n\n* `dplyr` is a powerful package in R for data manipulation.\n* It is part of the `tidyverse`, which includes a collection of packages designed to work together... Here's some of its greatest hits:\n\n<div style=\"text-align: center;\">\n![](gfx/tidyverse_packages.png){style=\"width: 30%; display: block; margin-left: auto; margin-right: auto;\"}\n<br>\n<small>[Source](https://laddem.github.io/courses/tidyverse/)</small>\n</div>\n\n## Introduction to `dplyr`\n\nTo load `dplyr`, you may simply load the `tidyverse` package:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)  \n```\n:::\n\n\n\nOur focus will be on basic operations like selecting and filtering data.\n\n![](gfx/dplyr_and_fun.png){style=\"width: 60%;\"}\n<div style=\"text-align: center;\">\n<small>[Source](https://towardsdatascience.com/data-manipulation-in-r-with-dplyr-3095e0867f75)</small>\n</div>\n\n## Downloading JHU CSSE COVID-19 case data\n\n* Let's start with something familiar... Here's a task for you:\n* Use `pub_covidcast()` to download [**JHU CSSE COVID-19 confirmed case data**]{.primary} (`confirmed_incidence_num`) for CA, NC, and NY from March 1, 2022 to March 31, 2022 as of January 1, 2024.\n* Try this for yourself. Then click the dropdown on the next slide to check your work...\n\n## Downloading JHU CSSE COVID-19 case data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(epidatr)\n\ncases_df_api <- pub_covidcast(\n  source = \"jhu-csse\",\n  signals = \"confirmed_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ca,nc,ny\",\n  time_values = epirange(20220301, 20220331),\n  as_of = as.Date(\"2024-01-01\")\n)\n```\n:::\n\n\n\nNow we only really need a few columns here...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Base R way for now...\ncases_df <- cases_df_api[,c(\"geo_value\", \"time_value\", \"value\")] \nnames(cases_df)[names(cases_df) == \"value\"] <- \"raw_cases\"\n```\n:::\n\n\n\n## Ways to inspect the dataset\n\nUse `head()` to view the first six row of the data \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(cases_df)  # First 6 rows\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  <chr>     <date>         <dbl>\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n4 ca        2022-03-02      7044\n5 nc        2022-03-02      2243\n6 ny        2022-03-02      1889\n```\n\n\n:::\n:::\n\n\n\nand tail to view the last six\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  <chr>     <date>         <dbl>\n1 ca        2022-03-30      3785\n2 nc        2022-03-30      1067\n3 ny        2022-03-30      3127\n4 ca        2022-03-31      4533\n5 nc        2022-03-31      1075\n6 ny        2022-03-31      4763\n```\n\n\n:::\n:::\n\n\n\n<!-- ## Ways to inspect the dataset\n\nUse `glimpse()` to get a compact overview of the dataset.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglimpse(cases_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 93\nColumns: 3\n$ geo_value  <chr> \"ca\", \"nc\", \"ny\", \"ca\", \"nc\", \"ny\", \"ca\", \"nc\", \"ny\", \"ca\",…\n$ time_value <date> 2022-03-01, 2022-03-01, 2022-03-01, 2022-03-02, 2022-03-02…\n$ raw_cases  <dbl> 4310, 1231, 1487, 7044, 2243, 1889, 7509, 2377, 2390, 3586,…\n```\n\n\n:::\n:::\n\n\n-->\n\n<!-- ## Creating tibbles\n\n* [**Tibbles**]{.primary}: Modern data frames with enhanced features.\n* Rows represent [**observations**]{.primary} (or cases).\n* Columns represent [**variables**]{.primary} (or features).\n* You can create tibbles manually using the `tibble()` function.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = letters, y = 1:26)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 26 × 2\n   x         y\n   <chr> <int>\n 1 a         1\n 2 b         2\n 3 c         3\n 4 d         4\n 5 e         5\n 6 f         6\n 7 g         7\n 8 h         8\n 9 i         9\n10 j        10\n# ℹ 16 more rows\n```\n\n\n:::\n:::\n\n\n-->\n\n## Selecting columns with `select()`\n\nThe `select()` function is used to pick specific columns from your dataset.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nselect(cases_df, time_value, raw_cases)  # Select the 'time_value' and 'raw_cases' columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 93 × 2\n   time_value raw_cases\n   <date>         <dbl>\n 1 2022-03-01      4310\n 2 2022-03-01      1231\n 3 2022-03-01      1487\n 4 2022-03-02      7044\n 5 2022-03-02      2243\n 6 2022-03-02      1889\n 7 2022-03-03      7509\n 8 2022-03-03      2377\n 9 2022-03-03      2390\n10 2022-03-04      3586\n# ℹ 83 more rows\n```\n\n\n:::\n:::\n\n\n\n## Selecting columns with `select()`\n\nYou can exclude columns by prefixing the column names with a minus sign `-`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nselect(cases_df, -geo_value)  # Exclude the 'geo_value' column from the dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 93 × 2\n   time_value raw_cases\n   <date>         <dbl>\n 1 2022-03-01      4310\n 2 2022-03-01      1231\n 3 2022-03-01      1487\n 4 2022-03-02      7044\n 5 2022-03-02      2243\n 6 2022-03-02      1889\n 7 2022-03-03      7509\n 8 2022-03-03      2377\n 9 2022-03-03      2390\n10 2022-03-04      3586\n# ℹ 83 more rows\n```\n\n\n:::\n:::\n\n\n\n<!-- ## Extracting columns with `pull()`\n\n* `pull()`: Extract a column as a vector.\n* Let's try this with the `cases` column...\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npull(cases_df, raw_cases) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 4310 1231 1487 7044 2243 1889 7509 2377 2390 3586 2646  350 1438    0 3372\n[16] 6465    0 2343 6690 4230 1033 3424  894 1025 4591 1833 1691 5359 1783 1747\n[31] 2713 1849 2229 1623    0 1396 5151    0 2202 4826 3130  982 1831  649 3128\n[46] 3706    0 2039 6143 2742 2356 4204 1740 2052 3256    0 2188 4659    0 2667\n[61] 5499 2508 1177 3004  819 1603 3943 1602  551 3550 1288 6596 1960 1224 3542\n[76] 1035    0    0 3384    0 5908 2811 2291 2286 1846  624 2394 3785 1067 3127\n[91] 4533 1075 4763\n```\n\n\n:::\n:::\n\n\n-->\n\n## Subsetting rows with `filter()`\n\n<div style=\"text-align: center;\">\n![](gfx/dplyr_filter.png){style=\"width: 65%; display: block; margin-left: auto; margin-right: auto;\"}\n<br>\n<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>\n</div>\n\n## Subsetting rows with `filter()`\n\n* The `filter()` function allows you to subset rows that meet specific conditions.\n* Conditions regard column values, such as filtering for only NC or cases higher than some threshold.\n* This enables you to narrow down your dataset to focus on relevant data.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfilter(cases_df, geo_value == \"nc\", raw_cases > 500)  # Filter for NC with raw daily cases > 500\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 22 × 3\n   geo_value time_value raw_cases\n   <chr>     <date>         <dbl>\n 1 nc        2022-03-01      1231\n 2 nc        2022-03-02      2243\n 3 nc        2022-03-03      2377\n 4 nc        2022-03-04      2646\n 5 nc        2022-03-07      4230\n 6 nc        2022-03-08       894\n 7 nc        2022-03-09      1833\n 8 nc        2022-03-10      1783\n 9 nc        2022-03-11      1849\n10 nc        2022-03-14      3130\n# ℹ 12 more rows\n```\n\n\n:::\n:::\n\n\n\n## Combining `select()` and `filter()` functions\n\n* You can further combine `select()` and `filter()` to further refine the dataset.\n* Use `select()` to choose columns and `filter()` to narrow down rows.\n* This helps in extracting the exact data needed for analysis.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nselect(filter(cases_df, geo_value == \"nc\", raw_cases > 500), time_value, raw_cases) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  time_value raw_cases\n  <date>         <dbl>\n1 2022-03-01      1231\n2 2022-03-02      2243\n3 2022-03-03      2377\n4 2022-03-04      2646\n5 2022-03-07      4230\n6 2022-03-08       894\n```\n\n\n:::\n:::\n\n\n\n## Using the pipe operator `|>`\n\n* The pipe operator (`|>`) makes code more readable by chaining multiple operations together.\n* The output of one function is automatically passed to the next function.\n* This allows you to perform multiple steps (e.g., `filter()` followed by `select()`) in a clear and concise manner.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This code reads more like poetry!\ncases_df |> \n  filter(geo_value == \"nc\", raw_cases > 500) |> \n  select(time_value, raw_cases) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  time_value raw_cases\n  <date>         <dbl>\n1 2022-03-01      1231\n2 2022-03-02      2243\n3 2022-03-03      2377\n4 2022-03-04      2646\n5 2022-03-07      4230\n6 2022-03-08       894\n```\n\n\n:::\n:::\n\n\n\n## Grouping data with `group_by()`\n\n* Use `group_by()` to group data by one or more columns.\n* Allows performing operations on specific groups of data.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_df |>\n  group_by(geo_value) |>\n  filter(raw_cases == max(raw_cases, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n# Groups:   geo_value [3]\n  geo_value time_value raw_cases\n  <chr>     <date>         <dbl>\n1 ca        2022-03-03      7509\n2 nc        2022-03-07      4230\n3 ny        2022-03-24      6596\n```\n\n\n:::\n:::\n\n\n\n## Creating new columns with `mutate()`\n\n<div style=\"text-align: center;\">\n![](gfx/dplyr_mutate.jpg){style=\"width: 45%; display: block; margin-left: auto; margin-right: auto;\"}\n<br>\n<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>\n</div>\n\n## Creating new columns with `mutate()`\n\n* `mutate()` is used to create new columns.\n* Perform calculations using existing columns and assign to new columns.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nny_subset = cases_df |>\n  filter(geo_value == \"ny\")\n\nny_subset |> \n  mutate(cumulative_cases = cumsum(raw_cases)) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value raw_cases cumulative_cases\n  <chr>     <date>         <dbl>            <dbl>\n1 ny        2022-03-01      1487             1487\n2 ny        2022-03-02      1889             3376\n3 ny        2022-03-03      2390             5766\n4 ny        2022-03-04       350             6116\n5 ny        2022-03-05      3372             9488\n6 ny        2022-03-06      2343            11831\n```\n\n\n:::\n:::\n\n\n\n<!-- ## Creating new columns with `mutate()`\n\n* `mutate()` can create multiple new columns in one step.\n* Logical comparisons (e.g., `over_5000 = raw_cases > 5000`) can be used within `mutate()`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nny_subset |> \n  mutate(over_5000 = raw_cases > 5000,\n         cumulative_cases = cumsum(raw_cases)) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  geo_value time_value raw_cases over_5000 cumulative_cases\n  <chr>     <date>         <dbl> <lgl>                <dbl>\n1 ny        2022-03-01      1487 FALSE                 1487\n2 ny        2022-03-02      1889 FALSE                 3376\n3 ny        2022-03-03      2390 FALSE                 5766\n4 ny        2022-03-04       350 FALSE                 6116\n5 ny        2022-03-05      3372 FALSE                 9488\n6 ny        2022-03-06      2343 FALSE                11831\n```\n\n\n:::\n:::\n\n\n-->\n\n## Combining `group_by()` and `mutate()`\n\n* First, group data using `group_by()`.\n* Then, use `mutate` to perform the calculations for each group.\n* Finally, use `arrange` to display the output by `geo_value`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_df |>\n  group_by(geo_value) |>\n  mutate(cumulative_cases = cumsum(raw_cases)) |> \n  arrange(geo_value) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n# Groups:   geo_value [1]\n  geo_value time_value raw_cases cumulative_cases\n  <chr>     <date>         <dbl>            <dbl>\n1 ca        2022-03-01      4310             4310\n2 ca        2022-03-02      7044            11354\n3 ca        2022-03-03      7509            18863\n4 ca        2022-03-04      3586            22449\n5 ca        2022-03-05      1438            23887\n6 ca        2022-03-06      6465            30352\n```\n\n\n:::\n:::\n\n\n\n<!-- ## Conditional calculations with `if_else()`\n* `if_else()` allows conditional logic within `mutate()`.\n* Perform different operations depending on conditions, like \"high\" or \"low.\"\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt <- 5000\n\ncases_df |>\n  mutate(high_low_cases = if_else(raw_cases > t, \"high\", \"low\")) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value raw_cases high_low_cases\n  <chr>     <date>         <dbl> <chr>         \n1 ca        2022-03-01      4310 low           \n2 nc        2022-03-01      1231 low           \n3 ny        2022-03-01      1487 low           \n4 ca        2022-03-02      7044 high          \n5 nc        2022-03-02      2243 low           \n6 ny        2022-03-02      1889 low           \n```\n\n\n:::\n:::\n\n\n-->\n\n## Summarizing data with `summarise()`\n* `summarise()` reduces data to summary statistics (e.g., mean, median).\n* Typically used after `group_by()` to summarize each group.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_df |>\n  group_by(geo_value) |>\n  summarise(median_cases = median(raw_cases))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  geo_value median_cases\n  <chr>            <dbl>\n1 ca                3785\n2 nc                1224\n3 ny                2188\n```\n\n\n:::\n:::\n\n\n\n<!-- ## Using `count()` to aggregate data\n`count()` is a shortcut for grouping and summarizing the data.\n\nFor example, if we want to get the total number of complete rows for each state, then\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_count <- cases_df |>\n  drop_na() |> # Removes rows where any value is missing (from tidyr)\n  group_by(geo_value) |>\n  summarize(count = n())\n```\n:::\n\n\n<br>\nis equivalent to \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_count <- cases_df |>\n  drop_na() |> \n  count(geo_value)\n\ncases_count # Let's see what the counts are.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  geo_value     n\n  <chr>     <int>\n1 ca           31\n2 nc           31\n3 ny           31\n```\n\n\n:::\n:::\n\n\n\n-->\n\n## Key practices learned\n\n* Use `head()` and `tail()` for quick data inspection.\n* Use `select()` and `filter()` for data manipulation.\n* Chain functions with `|>`.\n* Use `group_by()` to group data by one or more variables before applying functions.\n* Use `mutate` to create new columns or modify existing ones by applying functions to existing data.\n* Use `summarise` to reduce data to summary statistics (e.g., mean, median).\n\n## 1 to 2 word summaries of the `dplyr` functions\n\nHere are 1 to 2 word summaries of the key `dplyr` functions:\n\n1. `select()`: Choose columns\n\n1. `filter()`: Subset rows\n\n1. `mutate()`: Create columns\n\n1. `group_by()`: Group by\n\n1. `summarise()`: Numerical summary\n\n## Tidy data and Tolstoy\n\n> \"Happy families are all alike; every unhappy family is unhappy in its own way.\" — Leo Tolstoy  \n\n![](gfx/tidy_messy_data.jpg){style=\"width: 40%;\"}\n\n<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>\n\n## What is tidy data?\n\n* Tidy data follows a consistent structure: [**each row represents one observation, and each column represents one variable.**]{.primary}\n* `cases_df` is one classic example of tidy data.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  <chr>     <date>         <dbl>\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n4 ca        2022-03-02      7044\n5 nc        2022-03-02      2243\n6 ny        2022-03-02      1889\n```\n\n\n:::\n:::\n\n\n\n* To convert between tidy and messy data, we can use the `tidyr` package in the tidyverse.\n\n## `pivot_wider()` and  `pivot_longer()`\n<div style=\"text-align: center;\">\n![](gfx/pivot_wider_longer.jpg){style=\"width: 40%; display: block; margin-left: auto; margin-right: auto;\"}\n<br>\n<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>\n</div>\n\n## Making data wider with `pivot_wider()`\n* To convert data from long format to wide/messy format use `pivot_wider()`.\n* For example, let's try creating a column for each time value in `cases_df`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmessy_cases_df <- cases_df |>\n  pivot_wider(\n    names_from = time_value,   # Create new columns for each unique date\n    values_from = raw_cases    # Fill those columns with the raw_case values\n  )\n\n# View the result\nmessy_cases_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 32\n  geo_value `2022-03-01` `2022-03-02` `2022-03-03` `2022-03-04` `2022-03-05`\n  <chr>            <dbl>        <dbl>        <dbl>        <dbl>        <dbl>\n1 ca                4310         7044         7509         3586         1438\n2 nc                1231         2243         2377         2646            0\n3 ny                1487         1889         2390          350         3372\n# ℹ 26 more variables: `2022-03-06` <dbl>, `2022-03-07` <dbl>,\n#   `2022-03-08` <dbl>, `2022-03-09` <dbl>, `2022-03-10` <dbl>,\n#   `2022-03-11` <dbl>, `2022-03-12` <dbl>, `2022-03-13` <dbl>,\n#   `2022-03-14` <dbl>, `2022-03-15` <dbl>, `2022-03-16` <dbl>,\n#   `2022-03-17` <dbl>, `2022-03-18` <dbl>, `2022-03-19` <dbl>,\n#   `2022-03-20` <dbl>, `2022-03-21` <dbl>, `2022-03-22` <dbl>,\n#   `2022-03-23` <dbl>, `2022-03-24` <dbl>, `2022-03-25` <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n##  Tidying messy data with `pivot_longer()`\n* Use `pivot_longer()` to convert data from [**wide format**]{.primary} (multiple columns for the same variable) to [**long format**]{.primary} (one column per variable).\n* Let's try turning `messy_cases_df` back into the original tidy `cases_df`!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy_cases_df <- messy_cases_df |>\n  pivot_longer(\n    cols = -geo_value,          # Keep the 'geo_value' column as it is\n    names_to = \"time_value\",    # Create a new 'time_value' column from the column names\n    values_to = \"raw_cases\"     # Values from the wide columns should go into 'raw_cases'\n  )\n\n# View the result\nhead(tidy_cases_df, n = 3) # Notice the class of time_value here\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  geo_value time_value raw_cases\n  <chr>     <chr>          <dbl>\n1 ca        2022-03-01      4310\n2 ca        2022-03-02      7044\n3 ca        2022-03-03      7509\n```\n\n\n:::\n:::\n\n\n\n## Introduction to joins in `dplyr`\n* Joining datasets is a powerful tool for combining info. from multiple sources.\n* In R, `dplyr` provides several functions to perform different types of joins.\n* We'll demonstrate joining a subset of `cases_df` (our case counts dataset) with `state_census`.\n* [**Motivation**]{.primary}: We can scale the case counts by population to make them comparable across regions of different sizes.\n\n## Subset `cases_df`\n\nTo simplify things, let's use `filter()` to only grab one date of `cases_df`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_df_sub <- cases_df |> filter(time_value == \"2022-03-01\")\ncases_df_sub\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  geo_value time_value raw_cases\n  <chr>     <date>         <dbl>\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n```\n\n\n:::\n:::\n\n\n\nThough note that what we're going to do can be applied to the entirety of `cases_df`.\n\n## Load state census data\n\nThe `state_census` dataset from `epidatasets` contains state populations from the 2019 census.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# State census dataset from epidatasets\nlibrary(epidatasets)\nstate_census = state_census |> select(abbr, pop) |> filter(abbr != \"us\")\n\nstate_census |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  abbr       pop\n  <chr>    <dbl>\n1 al     4903185\n2 ak      731545\n3 az     7278717\n4 ar     3017804\n5 ca    39512223\n6 co     5758736\n```\n\n\n:::\n:::\n\n\n\nNotice that this includes many states that are not in `cases_df_sub`.\n\n## Left Join: Keep all rows from the first dataset\n\n* A [**left join**]{.primary} keeps all rows from the [**first dataset**]{.primary} (`cases_df_sub`), and adds matching data from the second dataset (`state_census`).\n* So [**all rows from the first dataset**]{.primary} (`cases_df_sub`) will be preserved.\n* The datasets are joined by matching the `geo_value` column, specified by the by argument.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Left join: combining March 1, 2022 state case data with the census data\ncases_sub_left_join <- cases_df_sub |>\n  left_join(state_census, join_by(geo_value == abbr))\n\ncases_sub_left_join\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  geo_value time_value raw_cases      pop\n  <chr>     <date>         <dbl>    <dbl>\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n```\n\n\n:::\n:::\n\n\n\n## Right Join: Keep all rows from the second dataset\n* A [**right join**]{.primary} keeps all rows from the [**second dataset**]{.primary} (`state_census`), and adds matching data from the first dataset (`cases_df_sub`).\n* If a row in the second dataset doesn't have a match in the first, then the columns from the first will be filled with NA. \n* For example, can see this for the `al` row from `state_census`...\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Right join: keep all rows from state_census\ncases_sub_right_join <- cases_df_sub |>\n  right_join(state_census, join_by(geo_value == abbr))\n\nhead(cases_sub_right_join)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value raw_cases      pop\n  <chr>     <date>         <dbl>    <dbl>\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n4 al        NA                NA  4903185\n5 ak        NA                NA   731545\n6 az        NA                NA  7278717\n```\n\n\n:::\n:::\n\n\n\n## Inner Join: Only keep matching rows\n* An [**inner join**]{.primary} will only keep rows where there is a match in both datasets.\n* So, if a state in `state_census` does not have a corresponding entry in `cases_df_sub`, then that row will be excluded.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Inner join: only matching rows are kept\ncases_sub_inner_join <- cases_df_sub |>\n  inner_join(state_census, join_by(geo_value == abbr))\n\ncases_sub_inner_join\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  geo_value time_value raw_cases      pop\n  <chr>     <date>         <dbl>    <dbl>\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n```\n\n\n:::\n:::\n\n\n\n## Full Join: Keep all rows from both datasets\n\n* A [**full join**]{.primary} will keep all rows from both datasets.\n* If a state in either dataset has no match in the other, the missing values will be filled with NA.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Full join: keep all rows from both datasets\ncases_sub_full_join <- cases_df_sub |>\n  full_join(state_census, join_by(geo_value == abbr))\n\nhead(cases_sub_full_join)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value raw_cases      pop\n  <chr>     <date>         <dbl>    <dbl>\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n4 al        NA                NA  4903185\n5 ak        NA                NA   731545\n6 az        NA                NA  7278717\n```\n\n\n:::\n:::\n\n\n\n## Pictorial summary of the four join functions\n\n<!--* **Left join:** All rows from the left dataset and matching rows from the right dataset.\n* **Right join:** All rows from the right dataset and matching rows from the left dataset.\n* **Inner join:** Only matching rows from both datasets.\n* **Full join:** All rows from both datasets, with NA where no match exists.-->\n\n![](gfx/join_funs_cheatsheet.png){style=\"width: 40%; display: block; margin-left: auto; margin-right: auto;\"}\n<div style=\"text-align: center;\">\n<small>[Source](https://ohi-science.org/data-science-training/dplyr.html)</small>\n</div>\n\n<!-- ## Final thoughts on joins\n* Joins are an essential part of data wrangling in R.\n* The choice of join depends on the analysis you need to perform:\n    + Use [**left joins**]{.primary} when you want to keep all data from the first dataset.\n    + Use [**right joins**]{.primary} when you want to keep all data from the second dataset.\n    + Use [**inner joins**]{.primary} when you're only interested in matching rows.\n    + Use [**full joins**]{.primary} when you want to preserve all information from both datasets.\n-->\n\n## Two review questions\n\n**Q1)**: What join function should you use if your goal is to scale the cases by population in `cases_df`?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Either left_join\ncases_left_join <- cases_df |>\n  left_join(state_census, join_by(geo_value == abbr))\n\ncases_left_join\n\n# Or inner_join\ncases_inner_join <- cases_df |>\n  inner_join(state_census, join_by(geo_value == abbr))\n\ncases_inner_join\n```\n:::\n\n\n\n**Q2)**: Lastly, please create a new column in `cases_df` where you scale the cases by population and multiply by `1e5` to get cases / 100k.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ncase_rates_df <- cases_inner_join |>\n  mutate(scaled_cases = raw_cases / pop * 1e5) # cases / 100K\nhead(case_rates_df)\n```\n:::\n\n\n\nCongratulations for making it through this crash course! That's all for this `glimpse()` into the tidyverse.\n\n# Epiverse Software Ecosystem\n\n\n## Epi. data processing with `epiprocess`\n\n* `epiprocess` is a package that offers additional functionality to pre-process epidemiological data.\n* You can work with an `epi_df` like you can with a tibble by using dplyr verbs.\n* For example, on `cases_df`, we can easily use `epi_slide_mean()` to calculate trailing 14 day averages of cases:\n\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\ncase_rates_df <- case_rates_df |>\n  as_epi_df(as_of = as.Date(\"2024-01-01\")) |>\n  group_by(geo_value) |>\n  epi_slide_mean(\n    scaled_cases, \n    .window_size = 14, \n    na.rm = TRUE\n  ) |>\n  rename(smoothed_scaled_cases = scaled_cases_14dav)\ncase_rates_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 93 x 6 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-01-01\n\n# A tibble: 93 × 6\n# Groups:   geo_value [3]\n   geo_value time_value raw_cases      pop scaled_cases smoothed_scaled_cases\n * <chr>     <date>         <dbl>    <dbl>        <dbl>                 <dbl>\n 1 ca        2022-03-01      4310 39512223        10.9                   10.9\n 2 ca        2022-03-02      7044 39512223        17.8                   14.4\n 3 ca        2022-03-03      7509 39512223        19.0                   15.9\n 4 ca        2022-03-04      3586 39512223         9.08                  14.2\n 5 ca        2022-03-05      1438 39512223         3.64                  12.1\n 6 ca        2022-03-06      6465 39512223        16.4                   12.8\n 7 ca        2022-03-07      6690 39512223        16.9                   13.4\n 8 ca        2022-03-08      3424 39512223         8.67                  12.8\n 9 ca        2022-03-09      4591 39512223        11.6                   12.7\n10 ca        2022-03-10      5359 39512223        13.6                   12.8\n# ℹ 83 more rows\n```\n\n\n:::\n:::\n\n\n\n## Epi. data processing with `epiprocess`\nIt is easy to produce an autoplot of the smoothed confirmed daily cases for each `geo_value`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncase_rates_df |>\n  autoplot(smoothed_scaled_cases)\n```\n\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/autoplot-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Epi. data processing with `epiprocess`\n\nAlternatively, we can display both the smoothed and the original daily case rates:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/smoothed-original-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\nNow, before exploring some more features of `epiprocess`, let's have a look at the epiverse software ecosystem it's part of...\n\n\n\n## The epiverse ecosystem\nInterworking, community-driven, packages for epi tracking & forecasting.\n\n![](gfx/epiverse_packages_flow.jpg){style=\"width: 60%; display: block; margin-left: auto; margin-right: auto;\"}\n\n<!-- 1. Fetch data: epidatr, epidatpy, and other sources, 2. Explore, clean, transform & backtest 3. Pre-built forecasters, modular forecasting framework: epipredict -->\n  \n  \n  \n# Panel and Versioned Data in the Epiverse\n  \n## What is panel data?\n\n* Recall that [panel data](https://en.wikipedia.org/wiki/Panel_data), or longitudinal data, \ncontain cross-sectional measurements of subjects over time. \n* Built-in example: [`covid_case_death_rates`](\n  https://cmu-delphi.github.io/epidatasets/reference/covid_case_death_rates.html) \ndataset, which is a snapshot [**as of**]{.primary} May 31, 2022 that contains daily state-wise measures of `case_rate` and `death_rate` for COVID-19 over 2021:\n  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n  <chr>     <date>         <dbl>      <dbl>\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 az        2020-12-31      76.8      1.10 \n5 ca        2020-12-31      96.0      0.751\n6 co        2020-12-31      35.8      0.649\n```\n\n\n:::\n:::\n\n\n\n* How do we store & work with such snapshots in the epiverse software ecosystem?\n\n  \n  \n## `epi_df`: Snapshot of a dataset\n\n* You can convert panel data into an `epi_df` with the required `geo_value` and `time_value` columns\n\nTherefore, an `epi_df` is...\n\n* a tibble that requires columns `geo_value` and `time_value`.\n\n* arbitrary additional columns containing [measured values]{.primary}\n\n* additional [keys]{.primary} to index (`age_group`, `ethnicity`, etc.)\n\n::: {.callout-note}\n## `epi_df`\n\nRepresents a [snapshot]{.primary} that\ncontains the most [up-to-date values]{.primary} of the signal variables, [as of]{.primary} a given time.\n:::\n\n## `epi_df`: Snapshot of a dataset\n\n* Consider the same dataset we just encountered on JHU daily COVID-19 cases and deaths rates from all states [as of]{.primary} May 31, 2022.\n\n* We can see that it meets the criteria `epi_df` (has `geo_value` and `time_value` columns) and that it contains additional metadata (i.e. `geo_type`, `time_type`, `as_of`, and `other_keys`).\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedf |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2022-05-31\n\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n* <chr>     <date>         <dbl>      <dbl>\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 az        2020-12-31      76.8      1.10 \n5 ca        2020-12-31      96.0      0.751\n6 co        2020-12-31      35.8      0.649\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nattr(edf, \"metadata\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$geo_type\n[1] \"state\"\n\n$time_type\n[1] \"day\"\n\n$as_of\n[1] \"2022-05-31\"\n\n$other_keys\ncharacter(0)\n```\n\n\n:::\n:::\n\n\n\n::: \n\n:::: \n\n## Examples of preprocessing\n\n### EDA features\n\n1. Making locations commensurate (per capita scaling)\n1. Correlating signals across location or time \n1. Computing growth rates\n1. Detecting and removing outliers\n1. Calculating summaries with rolling windows\n1. Dealing with revisions \n\n## Features - Correlations at different lags\n\n<!-- * There are always at least two ways to compute correlations in an `epi_df`: grouping by `time_value`, and by `geo_value`. \n\n* The latter is obtained by setting `cor_by = geo_value`. -->\n\n* The below plot addresses the question: \"For each state, are case and death rates linearly associated across all days?\"\n\n* To explore **lagged correlations** and how case rates associate with future death rates, we can use the `dt1` parameter in `epi_cor()` to shift case rates by a specified number of days. \n\n<!--  * For example, setting `dt1 = -14` means that case rates on June 1st will be correlated with death rates on June 15th, assessing how past case rates influence future death rates. -->\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor0 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value)\ncor14 <- epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -14)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-corr-lags-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* We can see that, in general, lagging the case rates back by 14 days improves the correlations.\n\n\n## Features - Systematic lag analysis\n\nThe analysis helps identify the lag at which case rates from the past have the strongest correlation with future death rates.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-sys-lag-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\nThe strongest correlation occurs at a lag of about 23 days, indicating that case rates are best correlated with death rates 23 days from now.\n\n## Features - Compute growth rates\n\n* Growth rate measures the relative change in a signal over time. <!-- indicating how quickly a quantity (like case rates) is increasing or decreasing. -->\n\n* We can compute time-varying growth rates for the two states, and see how this cases evolves over time.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedfg <- filter(edf, geo_value %in% c(\"ut\", \"ca\")) |>\n  group_by(geo_value) |>\n  mutate(gr_cases = growth_rate(time_value, case_rate, method = \"trend_filter\")) |>\n  ungroup()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-growth-rates-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* As expected, the peak growth rates for both states occurred during the January 2022 Omicron wave, reflecting the sharp rise in cases over that period.\n\n## Features - Outlier detection\n\n<!-- * There are multiple outliers in these data that a modeler may want to detect and correct. -->\n\n* The `detect_outlr()` function offers multiple outlier detection methods on a signal.\n\n* The simplest is `detect_outlr_rm()`, which works by calculating an outlier threshold using the rolling median and the rolling Interquartile Range (IQR) for each time point:\n\n**Threshold = Rolling Median ± (Detection Multiplier × Rolling IQR)**\n\n* Note that the default number of time steps to use in the rolling window by default is 21 and is centrally aligned. \n* The detection multiplier default is 2 and controls how far away a data point must be from the median to be considered an outlier.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nedfo <- filter(edf, geo_value %in% c(\"ca\", \"ut\")) |>\n  select(geo_value, time_value, case_rate) |>\n  as_epi_df() |>\n  group_by(geo_value) |>\n  mutate(outlier_info = detect_outlr_rm(\n    x = time_value, y = case_rate\n  )) |>\n  ungroup()\n```\n:::\n\n\n\n## Features - Outlier detection\n\n* Several data points that deviate from the expected case cadence have been flagged as outliers, and may require further investigation.\n\n* However, the peak in Jan. 2022 has also been flagged as an outlier. This highlights the importance of manual inspection before correcting the data, as these may represent valid events (e.g., a genuine surge in cases).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-outlier-ex-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Features -- sliding a computation on an `epi_df`\n\n* It is often useful to compute rolling summaries of signals. \n\n* These depend on the reference time, and are computed separately over geographies (and other groups). \n\n* For example, a trailing average can smooth out daily variation.\n\n* In `epiprocess`, this is achieved by `epi_slide()`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepi_slide(\n  .x,\n  .f,\n  ...,\n  .window_size = NULL,\n  .align = c(\"right\", \"center\", \"left\"),\n  .ref_time_values = NULL,\n  .new_col_name = NULL,\n  .all_rows = FALSE\n)\n```\n:::\n\n\n\nFor example, we can use `epi_slide()` to compute a trailing 7-day average. \n\n## Features -- sliding a computation on an `epi_df`\n\n* The simplest way to use `epi_slide` is tidy evaluation. \n\n* For a grouped `epi_df`, `epi_slide()` applies the computation to groups [separately]{.primary}. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncases_7dav <- epi_slide(\n  .x = cases_edf,\n  cases_7dav = mean(raw_cases, na.rm = TRUE),\n  .window_size = 7,\n  .align = \"right\"\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value raw_cases cases_7dav\n  <chr>     <date>         <dbl>      <dbl>\n1 ca        2022-03-01      4310       4310\n2 ca        2022-03-02      7044       5677\n3 nc        2022-03-01      1231       1231\n4 nc        2022-03-02      2243       1737\n5 ny        2022-03-01      1487       1487\n6 ny        2022-03-02      1889       1688\n```\n\n\n:::\n:::\n\n\n\n\n\n## Features -- sliding a computation on an `epi_df`\n\n`epi_slide` also accepts custom functions of a certain form. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncustom_function <- function(x, g, t, ...) {\n  \n  # Function body\n  \n}\n```\n:::\n\n\n\n* `x`: the data frame with all the columns with original object [except]{.primary} groupping vars. \n* `g`: the one-row tibble with values of gropping vars of the given group. \n* `t`: the `.ref_time_value` of the current window. \n* `...`: additional arguments. \n\n\n## Features -- sliding a computation on an `epi_df`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|9-17\"}\nmean_by_hand <- function(x, g, t, ...) {\n  data.frame(cases_7dav = mean(x$raw_cases, na.rm = TRUE))\n}\n\ncases_mean_custom_f = epi_slide(\n    .x = cases_edf,\n    .f = mean_by_hand,\n    .window_size = 7,\n    .align = \"right\"\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  geo_value time_value raw_cases cases_7dav\n  <chr>     <date>         <dbl>      <dbl>\n1 ca        2022-03-01      4310       4310\n2 nc        2022-03-01      1231       1231\n3 ny        2022-03-01      1487       1487\n```\n\n\n:::\n:::\n\n\n\n\n\n## `epi_archive`: Collection of `epi_df`s\n\n* full version history of a data set\n* acts like a bunch of `epi_df`s --- but stored [compactly]{.primary}\n* allows similar functionality as `epi_df` but using only [data that would have been available at the time]{.primary}\n\n::: {.callout-note}\n## Revisions\n\nEpidemiology data gets revised frequently.\n\n* We may want to use the data [as it looked in the past].{.primary} \n* or we may want to examine [the history of revisions]{.primary}.\n:::\n\n## `epi_archive`: Collection of `epi_df`s\n\nSubset of daily COVID-19 doctor visits (Optum) and cases (JHU CSSE) from all U.S. states in `archive` format:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narchive_cases_dv_subset_all_states |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$DT\nKey: <geo_value, time_value, version>\n         geo_value time_value    version percent_cli case_rate_7d_av\n            <char>     <Date>     <Date>       <num>           <num>\n      1:        ak 2020-06-01 2020-06-02          NA        1.145652\n      2:        ak 2020-06-01 2020-06-06    0.136815        1.145652\n      3:        ak 2020-06-01 2020-06-08    0.136249        1.145652\n      4:        ak 2020-06-01 2020-06-09    0.106744        1.145652\n      5:        ak 2020-06-01 2020-06-10    0.106676        1.145652\n     ---                                                            \n1514485:        wy 2021-11-26 2021-11-29    3.739819       23.207343\n1514486:        wy 2021-11-27 2021-11-28          NA       23.207343\n1514487:        wy 2021-11-28 2021-11-29          NA       23.207343\n1514488:        wy 2021-11-29 2021-11-30          NA       25.071781\n1514489:        wy 2021-11-30 2021-12-01          NA       25.464294\n\n$geo_type\n[1] \"state\"\n\n$time_type\n[1] \"day\"\n\n$other_keys\ncharacter(0)\n\n$clobberable_versions_start\n[1] NA\n\n$versions_end\n[1] \"2021-12-01\"\n```\n\n\n:::\n:::\n\n\n\n## Features -- sliding computation over `epi_df`s\n\n* We can apply a computation over different snapshots in an `epi_archive`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepix_slide(\n  .x,\n  .f,\n  ...,\n  .before = Inf,\n  .versions = NULL,\n  .new_col_name = NULL,\n  .all_versions = FALSE\n)\n```\n:::\n\n\n\nThis functionality is very helpful in version aware forecasting. We will return with a concrete example. \n  \n\n## Features -- summarize revision behavior\n\n* `revision_summary()` is a helper function that summarizes revision behavior of an `epix_archive`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrevision_data <- revision_summary(\n  archive_cases_dv_subset,\n  case_rate_7d_av,\n  drop_nas = TRUE,\n  print_inform = FALSE, # NOT the default, to save space\n  min_waiting_period = as.difftime(60, units = \"days\"),\n  within_latest = 0.2,\n  quick_revision = as.difftime(3, units = \"days\"),\n  few_revisions = 3,\n  abs_spread_threshold = NULL,\n  rel_spread_threshold = 0.1,\n  compactify_tol = .Machine$double.eps^0.5,\n  should_compactify = TRUE\n)\n```\n:::\n\n\n\n## Features -- summarize revision behavior\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(revision_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 11\n  time_value geo_value n_revisions min_lag max_lag  time_near_latest spread\n  <date>     <chr>           <dbl> <drtn>  <drtn>   <drtn>            <dbl>\n1 2020-06-01 ca                 12 1 days  546 days 1 days           0.248 \n2 2020-06-02 ca                 12 1 days  545 days 1 days           0.416 \n3 2020-06-03 ca                 11 1 days  544 days 1 days           0.115 \n4 2020-06-04 ca                 11 1 days  543 days 1 days           0.342 \n5 2020-06-05 ca                  7 1 days  520 days 1 days           0.0982\n6 2020-06-06 ca                  8 1 days  519 days 1 days           0.188 \n# ℹ 4 more variables: rel_spread <dbl>, min_value <dbl>, max_value <dbl>,\n#   median_value <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n\n## Visualize revision patterns\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/plot-revision-patterns-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Finalized data\n\n* Counts are revised as time proceeds\n* Want to know the [final]{.primary} value \n* Often not available until weeks/months later\n\n  Forecasting\n: At time $t$, predict the final value for time $t+h$, $h > 0$\n  \n  <br>\n  \n  Backcasting\n: At time $t$, predict the final value for time $t-h$, $h < 0$\n\n  <br>\n  \n  Nowcasting\n: At time $t$, predict the final value for time $t$\n\n\n\n# Basic Nowcasting in the Epiverse\n\n<!-- predicting a finalized value from a provisional value and making predictions. -->\n## Backfill Canadian edition\n  \n* Every week the [BC CDC releases COVID-19 hospitalization data](http://www.bccdc.ca/health-info/diseases-conditions/covid-19/archived-b-c-covid-19-data).\n\n* Following week they revise the number upward (by ~25%) due to lagged reports.\n\n![](gfx/bc_hosp_admissions_ex.jpg){style=\"width: 45%; display: block; margin-left: auto; margin-right: auto;\"}\n<!-- Newest iteration of \"backfill”, Canada edition. Every week the BC CDC releases hospitalization data. The following week they revise the number upward (by about 25%) due to lagging reports. Every single week, the newspaper says “hospitalizations have declined”. This week the BC CDC’s own report said “hospitalizations have declined”. The takeaway in the news is that hospitalizations ALWAYS fall from the previous week, but once backfilled, they’re rarely down -->\n\n* [**Takeaway**]{.primary}: Once the data is backfilled, hospitalizations rarely show a decline, challenging the common media narrative.\n\n## Backfill American edition\n\n* Again, we can see a similar systematic underestimation problem for COVID-19 mortality rates in CA. <!-- 2023-2024 -->\n\n* This plot also illustrates the [**revision process**]{.primary} - how the reported mortality changes & increases across multiple updates until it stabilizes at the final value (black line).\n\n![](gfx/am_mortality_revisions_ex.jpg){style=\"width: 45%; display: block; margin-left: auto; margin-right: auto;\"}\n\n* These two examples show the problem and now we need a solution...\n\n## Nowcasting and its mathematical setup\n\n* **Nowcasting**: Predict a finalized value from a provisional value.\n\n* Suppose today is time $t$\n\n* Let $y_i$ denote a series of interest observed at times $i=1,\\ldots, t$.\n\n::: {.callout-important icon=\"false\"}\n## Our goal\n\n* Produce a [**point nowcast**]{.primary} for the finalized values of $y_t$.\n* Accompany with time-varying prediction intervals\n\n:::\n\n* We may also have access to $p$ other time series \n$x_{ij},\\; i=1,\\ldots,t, \\; j = 1,\\ldots, p$ which may be subject to revisions.\n\n\n\n## Case study: NCHS mortality\n\n* In this example, we'll demonstrate the concept of nowcasting using [**NHCS mortality data**]{.primary}.\n(the number of weekly new deaths with confirmed or presumed COVID-19, per 100,000 population).\n* We will work with [**provisional**]{.primary} data (real-time reports) and compare them to **finalized** data (final reports).\n* The goal is to estimate or [**nowcast the mortality rate**]{.primary} for weeks when only provisional data is available.\n  \n## Fetch versioned data\n\nLet's fetch versioned mortality data from the API (`pub_covidcast`) for CA (`geo_values = \"ca\"`) and the signal of interest (`deaths_covid_incidence_num`) over early 2024.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fetch the versioned NCHS mortality data (weekly)\nnchs_archive <- pub_covidcast(\n  source = \"nchs-mortality\",\n  signals = \"deaths_covid_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"week\",\n  geo_values = c(\"ca\", \"ut\"),  \n  time_values = epirange(202001, 202440),  \n  issues = \"*\"\n) |> \n  select(geo_value, time_value, version = issue, mortality = value) |> \n  as_epi_archive(compactify = TRUE)\n```\n:::\n\n\n\n\n## Analysis of versioning behavior \n\nRecall, we need to watch out for:\n\n* [**Latency**]{.primary} the time difference between date of reference and date of the initial report\n* [**Backfill**]{.primary} how data for a given date is updated after initial report. \n\n`revision_summary()` provides a summary of both aspects.  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrevision_data = revision_summary(nchs_archive, mortality, print_inform = FALSE)\n```\n:::\n\n\n\n\n## Versioning analysis -- latency\n\n* [**Question:**]{.primary} What is the latency of NCHS data? \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrevision_data |> select(geo_value, time_value, min_lag) |> slice_sample(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n   geo_value time_value min_lag \n   <chr>     <date>     <drtn>  \n 1 ca        2020-07-12 147 days\n 2 ca        2021-08-29  14 days\n 3 ut        2020-09-27  70 days\n 4 ut        2022-01-30   7 days\n 5 ca        2023-07-09   7 days\n 6 ut        2022-01-02   7 days\n 7 ca        2023-02-12   7 days\n 8 ca        2022-01-02   7 days\n 9 ut        2024-04-07   7 days\n10 ca        2024-09-22   7 days\n```\n\n\n:::\n:::\n\n\n\n* We randomly sampled some dates to check if there is a consistent latency pattern. \n* Understanding latency prevents us from using data that we shouldn't have access to. \n\n## Versioning analysis -- backfill\n\n* [**Question:**]{.primary} How long does it take for the reported value to be close to the finalized value? \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrevision_data |> select(geo_value, time_value, time_near_latest) |> slice_sample(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n   geo_value time_value time_near_latest\n   <chr>     <date>     <drtn>          \n 1 ut        2023-05-28   7 days        \n 2 ca        2021-08-15  28 days        \n 3 ut        2020-03-08 273 days        \n 4 ut        2021-07-25  28 days        \n 5 ca        2020-05-24 196 days        \n 6 ca        2020-06-21 168 days        \n 7 ca        2020-01-26 315 days        \n 8 ca        2020-07-19 140 days        \n 9 ca        2023-10-15  21 days        \n10 ca        2020-11-01  35 days        \n```\n\n\n:::\n:::\n\n\n* It generally takes at least 4 weeks for reported value to be within 20\\% (default in `revision_summary()`) of the finalized value. \n* We can change the threshold of percentage difference by specifying the `within_latest` argument of `revision_summary()`. \n\n## Versioning analysis - backfill \n\n* [**Question:**]{.primary} When is the [**finalized value**]{.primary} first attained for each date? Would we have access to any in real-time?\n* How fast are the final values attained & what's the pattern for these times, if any?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  geo_value time_value min_version time_to_final\n  <chr>     <date>     <date>      <drtn>       \n1 ca        2020-01-26 2020-12-06  315 days     \n2 ca        2020-02-09 2020-12-06  301 days     \n3 ca        2020-02-23 2020-12-06  287 days     \n4 ca        2020-03-15 2020-12-06  266 days     \n5 ca        2020-03-22 2021-05-16  420 days     \n6 ca        2020-03-29 2021-04-04  371 days     \n```\n\n\n:::\n:::\n\n\n\n\n* [**Conclusion**]{.primary}: The revision behavior is pretty long-tailed. Value reported 4 weeks later is reasonably close to the finalized value. \n\n\n## Revision pattern visualization  \nThis shows the finalized rates in comparison to [**multiple revisions**]{.primary} to see how the data changes over time:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/final-vs-revisions-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Revision pattern visualization \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nchs-plot-val-different-ver-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n<!--\n## Aside: Do we need a burn-in/training set? \n\n* Typical stat-ML practise suggests a train, validation, test split.\n* But our exploratory analysis covered all available data, is that fine?\n\n\n* Generally, for exploratory analysis, it is fine to not do train/test split. \n  + These analyses do not involve model fitting, we have little risk of an overly optimistic performance evaluation (no overfitting on test data).\n* However, for a [**psuedo-prospective analysis**]{.primary}, the best practise is to do a train/test split.\n  + In such analysis, one would be fitting and validating many models, a train/test split provides a more rigorous control on overfitting to test set. \n-->\n\n## Ratio nowcaster: jumping from provisional to finalized value\n\n* Recall, the goal of nowcast at date $t$ is to use project the [*finalized value*]{.primary} of $y_t,$ given the information available on date $t$. \n* A very simple nowcaster is the ratio between finalized and provisional value. \n\nHow can we sensibly estimate this quantity? \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Ratio nowcaster: building training samples\n\n* At nowcast date $t,$ would have received reports with versions up to and including $t.$\n* We need to build training samples, which\n  + correctly aligns finalized value against provisional value \n  + uses features that would have been available at test time \n  + have enough samples to ensure sensible estimation results\n\n::: {.fragment .fade-in}\n* Build training samples by treating dates prior to date $t$ as actual nowcast dates.\n  + What is the provisional data on that date?\n  + Have we received finalized value for that date? \n:::\n\n## Ratio nowcaster: building training samples\n\n* At an earlier nowcast date $t_0,$ we define \n  + [**Provisional value**]{.primary} as the reported value of $Y_{s_0}$ with version $t_0.$ Here $s_0$ is the largest occurence date among all values reported up until $t_0.$\n  + [**Finalized value**]{.primary} as the (potentially unobserved) finalized value of $Y_{s_0}.$\n    - We only know in *hindsight* when reported value of $Y_{s_0}$ is finalized -- need an approximation. \n\n## Revisiting `revision_summary()`\n\nRecall, `revision_summary()` reports the number of days to be within 20\\% (default value) of finalized value\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  geo_value time_value time_near_latest\n  <chr>     <date>     <drtn>          \n1 ut        2023-04-09  7 days         \n2 ut        2022-03-06 49 days         \n3 ca        2021-01-24 49 days         \n4 ut        2020-09-20 77 days         \n5 ca        2021-01-17 49 days         \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime differences in days\n  0%  25%  50%  75% 100% \n   7   21   28   49  315 \n```\n\n\n:::\n:::\n\n\n\n::: {.fragment .fade-in}\nLet's say data reported 49 days after reference date is good enough to be considered finalized. \n:::\n\n## Ratio nowcaster: test time feature\n\n* Due to latency, provisional values may not be available at lag 0\n* We use last-observation-carried-forward (LOCF) to impute missing values at test time\n* Precisely, at test time $t,$ we use last observed data point (among all those reported up through time $t$)\n\n## Nowcasting at a single date: building training samples \n\n::: {.fragment .fade-in}\n* Searching for provisional values, at previous hypothetical nowcast dates.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnowcast_date <- as.Date(\"2022-01-02\"); window_length = 180\n\ninitial_data <- nchs_archive$DT |>\n  group_by(geo_value, time_value) |>\n  filter(version == min(version)) |>\n  rename(initial_val = mortality) |>\n  select(geo_value, time_value, initial_val)\n```\n:::\n\n\n:::\n\n\n::: {.fragment .fade-in}\n* Searching for finalized values, at previous hypothetical nowcast dates.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfinalized_data <- epix_as_of(nchs_archive, nowcast_date) |>\n  filter(time_value >= nowcast_date - approx_final_lag - window_length & time_value <= nowcast_date - approx_final_lag) |>\n  rename(finalized_val = mortality) |>\n  select(geo_value, time_value, finalized_val)\n```\n:::\n\n\n:::\n\n## Nowcasting at a single date: estimating ratio model\n\n* After searching for both provisional and finalized values, we merge them together and estimate the ratio. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nratio <- finalized_data |>\n  inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |>\n  mutate(ratio = finalized_val / initial_val) |>\n  pull(ratio) |>\n  median(na.rm = TRUE)\n```\n:::\n\n\n\n## Nowcasting at a single date: test feature construction\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlast_avail <- epix_as_of(nchs_archive, nowcast_date) |>\n  slice_max(time_value) |>\n  pull(mortality) \n```\n:::\n\n\n\n## Nowcasting at a single date: producing the nowcast\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnowcast <- last_avail * ratio\nfinalized_val <- epix_as_of(nchs_archive, nchs_archive$versions_end) |>\n  filter(time_value == nowcast_date) |>\npull(mortality)\n\nnowcast_final = data.frame(Nowcast = nowcast, `Finalized value` = finalized_val, check.names=FALSE)\nknitr::kable(nowcast_final)\n```\n\n::: {.cell-output-display}\n\n\n|  Nowcast| Finalized value|\n|--------:|---------------:|\n| 305.1608|             946|\n\n\n:::\n:::\n\n\n\n\n## Nowcasting for multiple dates\n\n* All previous manipulations should really be seen as a template for all nowcast dates. \n* The template computation sould be applied over all nowcast dates, [**but we must respect data versioning**]{.primary}! \n* `epix_slide()` is designed just for this! It behaves similarly to `epi_slide`.\n* Key exception: `epix_slide()` is version aware: the sliding computation at any reference time $t$ is performed on [**data that would have been available as of t**]{.primary}.\n\n\n## Nowcasting for multiple dates via `epix_slide()`\n\nWe begin by templatizing our previous operations. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnowcaster <- function(x, g, t, wl=180, appx=approx_final_lag) {\n  initial_data <- x$DT |>\n    group_by(geo_value, time_value) |>\n    filter(version ==  min(version)) |>\n    filter(time_value >= t - wl - appx & time_value <= t - appx) |>\n    rename(initial_val = mortality) |>\n    select(geo_value, time_value, initial_val)\n  finalized_data <- x$DT |>\n    group_by(geo_value, time_value) |>\n    filter(version ==  max(version)) |>\n    filter(time_value >= t - wl - appx & time_value <= t - appx) |>\n    rename(finalized_val = mortality) |>\n    select(geo_value, time_value, finalized_val)\n  ratio <- finalized_data |>\n    inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |>\n    mutate(ratio = finalized_val / initial_val) |>\n    pull(ratio) |>\n    median(na.rm = TRUE)\n  last_avail <-  epix_as_of(x, t) |>\n    slice_max(time_value) |>\n    pull(mortality) \n  tibble(geo_value = x$geo_value, target_date = t, nowcast = last_avail * ratio)\n}\n```\n:::\n\n\n\n## Sanity check of `epix_slide()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nslided_nowcast_1d = epix_slide(\n  .x = nchs_archive,\n  .f = nowcaster,\n  .before = Inf,\n  .versions = nowcast_date,\n  .all_versions = TRUE\n)\n\nnowcast_check = data.frame(`Manual nowcast` = nowcast, `Slided nowcast` = slided_nowcast_1d$nowcast, check.names = FALSE)\nknitr::kable(nowcast_check)\n```\n\n::: {.cell-output-display}\n\n\n| Manual nowcast| Slided nowcast|\n|--------------:|--------------:|\n|       305.1608|       305.1608|\n\n\n:::\n:::\n\n\n\n\n## Nowcasting for multiple dates via `epix_slide()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnowcasts = nchs_archive |>\n  group_by(geo_value) |>\n  epix_slide(\n    nowcaster,\n    .before=Inf,\n    .versions = all_nowcast_dates,\n    .all_versions = TRUE\n)\n```\n:::\n\n\n\n\n\n## Details of `epix_slide()`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepix_slide(\n  .x,\n  .f,\n  ...,\n  .before = Inf,\n  .versions = NULL,\n  .new_col_name = NULL,\n  .all_versions = FALSE\n)\n```\n:::\n\n\n\n\n* `.f` in `epix_slide()` can be specified with the same form of custom function as `epi_slide()`. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfunction(x, g, t) {\n  # function body\n}\n```\n:::\n\n\n\n* Mandatory variables of `.f` would have different forms depending on the value of `.all_versions`. \n\n\n## Details of `epix_slide()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|8\"}\nepix_slide(\n  .x,                          \n  .f,                         \n  ...,                        \n  .before = Inf,             \n  .versions = NULL,           \n  .new_col_name = NULL,      \n  .all_versions = FALSE\n)\n```\n:::\n\n\n\n::: {.fragment .fade-in}\n* When `.all_versions = FALSE`, `epix_slide()` essentially iterates the templatized computation over snapshots. \n* Said differently, when `.all_versions = FALSE`, data accessed at any sliding iteration [**only involves a single version**]{.primary}. \n:::\n\n::: {.fragment .fade-in}\n* Hence: \n  + `x`: an `epi_df` with same column names as archive's `DT`, minus the `version` column.\n  +  `g`: a one-row tibble containing the values of groupping variables of the associated group.\n  + `t`: the `ref_time_value` of the current window.\n  + `...`: additional arguments. \n:::\n\n## Details of `epix_slide()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|8\"}\nepix_slide(\n  .x,                          \n  .f,                         \n  ...,                        \n  .before = Inf,             \n  .versions = NULL,           \n  .new_col_name = NULL,      \n  .all_versions = TRUE\n)\n```\n:::\n\n\n\n::: {.fragment .fade-in}\n* When `.all_versions = FALSE`, data accessed at any sliding iteration involves versions [**up to and including .version**]{.primary}. \n:::\n\n::: {.fragment .fade-in}\n* Hence: \n  + `x`: an `epi_archive`, with version up to and including `.version`. \n  +  `g`: a one-row tibble containing the values of groupping variables of the associated group.\n  + `t`: the `.version` of the current window.\n  + `...`: additional arguments. \n:::\n\n\n## Details of `epix_slide()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|7\"}\nnowcasts <- nchs_archive |>\n  group_by(geo_value) |>\n  epix_slide(\n    nowcaster,\n    .before=Inf,\n    .versions = all_nowcast_dates,\n    .all_versions = TRUE\n)\n```\n:::\n\n\n\n## Details of `epix_slide()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|3,9\"}\nnowcaster <- function(x, g, t, wl=180, appx=approx_final_lag) {\n  initial_data <- x$DT |>\n    group_by(geo_value, time_value) |>\n    filter(version ==  min(version)) |>\n    filter(time_value >= t - wl - appx & time_value <= t - appx) |>\n    rename(initial_val = mortality) |>\n    select(geo_value, time_value, initial_val)\n  finalized_data <- x$DT |>\n    group_by(geo_value, time_value) |>\n    filter(version ==  max(version)) |>\n    filter(time_value >= t - wl - appx & time_value <= t - appx) |>\n    rename(finalized_val = mortality) |>\n    select(geo_value, time_value, finalized_val)\n  ratio <- finalized_data |>\n    inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |>\n    mutate(ratio = finalized_val / initial_val) |>\n    pull(ratio) |>\n    median(na.rm=TRUE)\n  last_avail <- epix_as_of(x, t) |>\n    slice_max(time_value) |>\n    pull(mortality) \n  tibble(geo_value = x$geo_value, target_date = t, nowcast = last_avail * ratio)\n}\n```\n:::\n\n\n\n\n\n## Visualize nowcasts\n\n\nWe are now finally able to compare nowcasts against first available reports:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcast-fun-plot-results-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* The real-time counts tend to be biased below the finalized counts. Nowcasted values tend to provide a much better approximation of the truth (at least for these dates).\n\n\n## Smoothing nowcasts\n\n* Nowcasts are quite volatile, reflecting the provisional counts are far from complete. \n* We can use a trailing average to smooth them.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsmoothed_nowcasts <- epi_slide(\n  nowcasts |> as_epi_df(),\n  smoothed_nowcasts = mean(nowcast, na.rm = TRUE),\n  .window_size = as.difftime(3, units = \"weeks\")\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcast-smoothed-vis-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Evaluation using MAE\n\n* Assume we have prediction $\\hat y_{t}$ for the provisional value at time $t$.\n\n* Then for $y_{t}$ over times $t = 1, \\dots, N$, then we may compute error metrics like mean absolute error (MAE).\n\n\n* MAE measures the average absolute difference between the nowcast and finalized values. \n\n$$MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{t}- \\hat y_{t}|$$\n\n* Note that it's scale-dependent, meaning it can vary depending on the units of the data (e.g., cases, deaths, etc.).\n\n## Evaluation using MAE\n\nLet's numerically evaluate our point nowcasts for the provisional values of a time series (e.g., COVID-19 mortality) using MAE.\n\n<!-- Accuracy of nowcast is assessed by how close provisional estimates are to the finalized values to gauge the model's performance. -->\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n| Smoothed MAE| Unsmoothed nowcast MAE| Provisional value MAE|\n|------------:|----------------------:|---------------------:|\n|     161.6591|               172.4131|              199.6061|\n\n\n:::\n:::\n\n\n\n\n# Nowcasting with Regression\n\n## Nowcasting: Moving from one predictor to multiple\n\n* The ratio model predicts the finalized value of $Y_t$ from $Y_{s}$, the last value included in the version $t$ report.\n* $Y_s$ is the closest in time we can get to $Y_t$, but we also expect it to be the least reliable value in version $t$.\n* Can we add $Y_{s - 1}$, $Y_{s - 2}$, and even other data sources to the model to try to find a good mix of relevant and reliable signals?\n* Regressions models will let us do that.\n* Let's start \"simple\": predicting $Y_t$ with whichever of $Y_{t - 1}$ and $Y_{t -\n  2}$ are available in version $t$.\n\n## Start with a single nowcast date\n\nWe're only looking at California in this example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnchs_ca_archive <- nchs_archive$DT[geo_value == \"ca\",] |>\n  as_epi_archive()\n```\n:::\n\n\n\nWe'll start experimenting with just a single nowcast date:\n\n::: {.notes}\nEarlier nowcast dates often encounter extra difficulties regarding data\navailability, so let's make sure we work on the first one.\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrial_nowcast_date <- all_nowcast_dates[[1]]\n```\n:::\n\n\n\nWhat data would we have had available then?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# This is the version history we'd have seen at that point:\nnchs_ca_past_archive <- nchs_ca_archive |>\n  epix_as_of(trial_nowcast_date, all_versions = TRUE)\n\n# And this is what the latest version was at that point:\nnchs_ca_past_latest <- nchs_ca_past_archive |>\n  epix_as_of(trial_nowcast_date)\n```\n:::\n\n\n\n\n\n## What predictors were available at test time?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# At version t, our target is finalized Y_t:\ntarget_time_value <- trial_nowcast_date\n\n# Check which of Y_{t-1} and Y_{t-2} are available, assign distinct names:\npredictor_descriptions <- nchs_ca_past_latest |>\n  filter(as.integer(target_time_value - time_value) <= 2 * 7) |>\n  drop_na(mortality) |>\n  transmute(\n    varname = \"mortality\",\n    lag_days = as.integer(trial_nowcast_date - time_value),\n    predictor_name = paste0(varname, \"_lag\", lag_days, \"_realtime\")\n  )\npredictor_descriptions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  varname   lag_days predictor_name          \n  <chr>        <int> <chr>                   \n1 mortality       14 mortality_lag14_realtime\n2 mortality        7 mortality_lag7_realtime \n```\n\n\n:::\n:::\n\n\n\n## Line up with training data\n\nWe need to make sure to line up our predictors in `nchs_ca_past_latest` with\ntraining data that is analogous (e.g., \"equally unreliable\").\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(data.table)\nget_predictor_training_data <- function(archive, varname, lag_days, predictor_name) {\n  ...\n  ...\n  ...\n  return (training_data_edf_for_this_predictor)\n}\n```\n:::\n\n\n\nActual implementation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(data.table)\nget_predictor_training_data <- function(archive, varname, lag_days, predictor_name) {\n  epikeytime_names <- setdiff(key(archive$DT), \"version\")\n  requests <- unique(archive$DT, by = epikeytime_names, cols = character())[\n  , version := time_value + ..lag_days\n  ]\n  setkeyv(requests, c(epikeytime_names, \"version\"))\n  result <- archive$DT[\n    requests, c(key(archive$DT), varname), roll = TRUE, nomatch = NULL, allow.cartesian = TRUE, with = FALSE\n  ][\n  , time_value := version\n  ][\n  , version := NULL\n  ]\n  nms <- names(result)\n  nms[[match(varname, nms)]] <- predictor_name\n  setnames(result, nms)\n  setDF(result)\n  as_tibble(result)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_predictor_training_data(nchs_ca_past_archive, \"mortality\", 7, \"mortality_lag7_realtime\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 36 × 3\n   geo_value time_value mortality_lag7_realtime\n   <chr>     <date>                       <dbl>\n 1 ca        2020-12-06                      13\n 2 ca        2021-02-28                      78\n 3 ca        2021-03-07                      54\n 4 ca        2021-03-14                      43\n 5 ca        2021-03-21                      19\n 6 ca        2021-03-28                      21\n 7 ca        2021-04-04                      23\n 8 ca        2021-04-11                      20\n 9 ca        2021-04-18                      18\n10 ca        2021-04-25                      20\n# ℹ 26 more rows\n```\n\n\n:::\n:::\n\n\nThe first value here is a version of $Y_{\\text{2020-11-30}}$ as it was reported in version 2020-12-06.  We expect it to have similar characteristics as $Y_{t - 7\\text{ days}}$ as reported in version $t$ for other values of $t$.\n\n## Get multiple predictors\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredictors <- predictor_descriptions |>\n  pmap(function(varname, lag_days, predictor_name) {\n    get_predictor_training_data(nchs_ca_past_archive, varname, lag_days, predictor_name)\n  }) |>\n  reduce(full_join, by = c(\"geo_value\", \"time_value\"))\npredictors\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 4\n   geo_value time_value mortality_lag14_realtime mortality_lag7_realtime\n   <chr>     <date>                        <dbl>                   <dbl>\n 1 ca        2020-12-06                       66                      13\n 2 ca        2020-12-13                       13                      NA\n 3 ca        2021-02-14                      557                      NA\n 4 ca        2021-02-21                      474                      NA\n 5 ca        2021-02-28                      478                      78\n 6 ca        2021-03-07                      415                      54\n 7 ca        2021-03-14                      282                      43\n 8 ca        2021-03-21                      176                      19\n 9 ca        2021-03-28                      164                      21\n10 ca        2021-04-04                      117                      23\n# ℹ 40 more rows\n```\n\n\n:::\n:::\n\n\n\n- A full join is nice to show differences in missingness\n- But before training we're going to `drop_na()` and end up with something more\n  like an inner join\n\n## Combine with target data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntarget <- nchs_ca_past_latest |>\n  filter(time_value <= max(time_value) - 49) |>\n  rename(mortality_semistable = mortality)\n```\n:::\n\n\n\nFor each training time $t'$, approximate finalized $Y_{t'}$ with $Y_{t'}$ as\nreported at our trial nowcast date $t$.\n* Based on earlier analysis, we shouldn't really trust this for $t'$ within 49\n  days of $t$, so filter those training times out.\n\n\n## Fit the regression model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntraining_test <- full_join(predictors, target, by = c(\"geo_value\", \"time_value\"))\n\ntraining <- training_test |> drop_na()\ntest <- training_test |> filter(time_value == trial_nowcast_date)\n\nfit <- training |>\n  select(all_of(predictor_descriptions$predictor_name), mortality_semistable) |>\n  lm(formula = mortality_semistable ~ .)\n\npred <- tibble(\n  nowcast_date = trial_nowcast_date,\n  target_date = target_time_value,\n  prediction = unname(predict(fit, test))\n)\n\npred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  nowcast_date target_date prediction\n  <date>       <date>           <dbl>\n1 2022-01-02   2022-01-02        483.\n```\n\n\n:::\n:::\n\n\n\n## Our prediction\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  nowcast_date target_date prediction\n  <date>       <date>           <dbl>\n1 2022-01-02   2022-01-02        483.\n```\n\n\n:::\n:::\n\n\n\n## Backtesting our nowcaster\n\nWe'll wrap our nowcasting code in a function and `epix_slide()` again.\n\n* And get an error --- some versions $t$ don't include a value $Y_{t-1}$ or $Y_{t-2}$ (e.g., version 2022-06-26 doesn't).\n    * So let's try looking farther into the past at $Y_{t-3}$, etc.\n    * ... but don't look too far: $Y_{t-5}$ is the limit.\n    * The same regression approach applies to models with 3 or more features.\n    * Including more features tends to improve performance, up to a point.\n\n## Some other modifications\n\n* Add some basic checks throughout our nowcasting function.\n* Make sure we have \"enough\" training data to fit a model.\n* Add ability to look not just at provisional $Y_{t-k}$, but also provisional\n  $Z_{t-k}$ for some other signal $Z$.\n    * $Z$ here is HHS/NHSN COVID-19 hospitalization reporting.\n      * This was daily-resolution and daily-reporting-cadence for some time;\n        it's possible but a bit tricky to combine with our weekly-resolution\n        weekly-cadence archive.\n    * Exclude a potential predictor if it doesn't have much training data available.\n* Allow for linear regression or quantile regression at the median level (tau = 0.5)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nregression_nowcaster <- function(archive, settings, return_info = FALSE) {\n  if (!inherits(archive, \"epi_archive\")) {\n    stop(\"`archive` isn't an `epi_archive`\")\n  }\n  if (length(unique(archive$DT$geo_value)) != 1L) {\n    stop(\"Expected exactly one unique `geo_value`\")\n  }\n  if (archive$time_type == \"day\") {\n    archive <- thin_daily_to_weekly_archive(archive)\n  }\n\n  nowcast_date <- archive$versions_end\n  target_time_value <- nowcast_date\n  latest_edf <- archive |> epix_as_of(nowcast_date)\n  # print(nowcast_date)\n\n  predictor_descriptions <-\n    latest_edf |>\n    mutate(lag_days = as.integer(nowcast_date - time_value)) |>\n    select(-c(geo_value, time_value)) |>\n    pivot_longer(-lag_days, names_to = \"varname\", values_to = \"value\") |>\n    drop_na(value) |>\n    inner_join(settings$predictors, by = \"varname\", unmatched = \"error\") |>\n    filter(abs(lag_days) <= max_abs_shift_days) |>\n    arrange(varname, abs(lag_days)) |>\n    group_by(varname) |>\n    filter(seq_len(n()) <= max_n_shifts[[1]]) |>\n    ungroup() |>\n    mutate(predictor_name = paste0(varname, \"_lag\", lag_days, \"_realtime\")) |>\n    select(varname, lag_days, predictor_name)\n\n  predictor_edfs <- predictor_descriptions |>\n    pmap(function(varname, lag_days, predictor_name) {\n      get_predictor_training_data(archive, varname, lag_days, predictor_name)\n    }) |>\n    lapply(na.omit) |>\n    keep(~ nrow(.x) >= settings$min_n_training_per_predictor)\n\n  if (length(predictor_edfs) == 0) {\n    stop(\"Couldn't find acceptable predictors in the latest data.\")\n  }\n\n  predictors <- predictor_edfs |>\n    reduce(full_join, by = c(\"geo_value\", \"time_value\"))\n\n  target <- latest_edf |>\n    filter(time_value <= max(time_value) - settings$days_until_target_semistable) |>\n    select(geo_value, time_value, mortality_semistable = mortality)\n\n  training_test <- full_join(predictors, target, by = c(\"geo_value\", \"time_value\"))\n\n  training <- training_test |>\n    drop_na() |>\n    slice_max(time_value, n = settings$max_n_training_intersection)\n\n  test <- training_test |>\n    filter(time_value == nowcast_date)\n\n  if (isTRUE(settings$median)) {\n    fit <- training |>\n      select(any_of(predictor_descriptions$predictor_name), mortality_semistable) |>\n      quantreg::rq(formula = mortality_semistable ~ ., tau = 0.5)\n  } else {\n    fit <- training |>\n      select(any_of(predictor_descriptions$predictor_name), mortality_semistable) |>\n      lm(formula = mortality_semistable ~ .)\n  }  \n\n  pred <- tibble(\n    geo_value = \"ca\",\n    nowcast_date = nowcast_date,\n    target_date = target_time_value,\n    prediction = unname(predict(fit, test))\n  )\n\n  if (return_info) {\n    return(tibble(\n      coefficients = list(coef(fit)),\n      predictions = list(pred)\n    ))\n  } else {\n    return(pred)\n  }\n}\n\n# We can apply this separately for each nowcast_date to ensure that we consider\n# the latest possible value for every signal, though whether that is advisable\n# or not may depend on revision characteristics of the signals.\nthin_daily_to_weekly_archive <- function(archive) {\n  key_nms <- key(archive$DT)\n  val_nms <- setdiff(names(archive$DT), key_nms)\n  update_tbl <- as_tibble(archive$DT)\n  val_nms |>\n    lapply(function(val_nm) {\n      update_tbl[c(key_nms, val_nm)] |>\n        # thin out to weekly, making sure that we keep the max time_value with non-NA value:\n        filter(as.POSIXlt(time_value)$wday == as.POSIXlt(max(time_value[!is.na(.data[[val_nm]])]))$wday) |>\n        # re-align:\n        mutate(\n          time_value = time_value - as.POSIXlt(time_value)$wday, # Sunday of same epiweek\n          old_version = version,\n          version = version - as.POSIXlt(version)$wday # Sunday of same epiweek\n        ) |>\n        slice_max(old_version, by = all_of(key_nms)) |>\n        select(-old_version) |>\n        as_epi_archive(other_keys = setdiff(key_nms, c(\"geo_value\", \"time_value\", \"version\")),\n                       compactify = TRUE)\n    }) |>\n    reduce(epix_merge, sync = \"locf\")\n}\n\n# Baseline model:\nlocf_nowcaster <- function(archive) {\n  nowcast_date <- archive$versions_end\n  target_time_value <- nowcast_date\n  latest_edf <- archive |> epix_as_of(nowcast_date)\n\n  latest_edf |>\n    complete(geo_value, time_value = target_time_value) |>\n    arrange(geo_value, time_value) |>\n    group_by(geo_value) |>\n    fill(mortality) |>\n    ungroup() |>\n    filter(time_value == target_time_value) |>\n    transmute(\n      geo_value,\n      nowcast_date = nowcast_date,\n      target_date = time_value,\n      prediction = mortality\n    )\n}\n```\n:::\n\n\n\n## Model settings\n\nAfter fixing, enhancing, and parameterizing our regression nowcaster, we'll\ncompare two different configurations:\n\n* one with just mortality-based predictions\n* one that also uses hospitalizations as a predictor\n* and two that use quantile reg instead of linear reg\n\n## Model settings\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreg1_settings <- list(\n  predictors = tribble(\n    ~varname,    ~max_abs_shift_days, ~max_n_shifts,\n    \"mortality\",                  35,             3,\n    ),\n  min_n_training_per_predictor = 30, # or else exclude predictor\n  days_until_target_semistable = 7 * 7, # filter out unstable when training (and evaluating)\n  min_n_training_intersection = 20, # or else raise error\n  max_n_training_intersection = Inf # or else filter down rows\n)\n\nreg2_settings <- list(\n  predictors = tribble(\n    ~varname,     ~max_abs_shift_days, ~max_n_shifts,\n    \"admissions\",                  35,             3,\n    \"mortality\",                   35,             3,\n    ),\n  min_n_training_per_predictor = 30, # or else exclude predictor\n  days_until_target_semistable = 7 * 7, # filter out unstable when training (and evaluating)\n  min_n_training_intersection = 20, # or else raise error\n  max_n_training_intersection = Inf # or else filter down rows\n)\n\nreg3_settings <- c(reg1_settings, median = TRUE)\nreg4_settings <- c(reg2_settings, median = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Comparison: linear regression\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/regression-nowcast-plot-linreg-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Comparison: quantile regression\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/regression-nowcast-plot-quantreg-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Evaluations\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_models <- length(unique(nowcast_comparison$Nowcaster))\nnowcast_comparison |>\n  # Filter evaluation based on target stability\n  filter(target_date <= nchs_ca_archive$versions_end - 49) |>\n  # Filter evaluated tasks to those with all models available\n  group_by(target_date) |>\n  filter(sum(!is.na(prediction)) == n_models) |>\n  ungroup() |>\n  summarize(.by = Nowcaster,\n            MAE = mean(abs(mortality - prediction)),\n            MAPE = 100*mean(abs(mortality - prediction)/abs(mortality)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  Nowcaster           MAE  MAPE\n  <chr>             <dbl> <dbl>\n1 LOCF              210.   77.7\n2 LOCF ratio model  177.   62.6\n3 Regression model  168.   94.2\n4 Regression + hosp  99.9  52.5\n5 QuantReg model    108.   47.7\n6 QuantReg + hosp    92.0  43.8\n```\n\n\n:::\n:::\n\n\n\n## Mea culpa\n\nThis quickly became complicated and we've glossed over some core concepts.\nWe'll explain concepts of regression, lagged features, and evaluation more\ncarefully tomorrow.\n\n## Aside on nowcasting\n\n* To some Epis, \"nowcasting\" can be equated with \"estimate the time-varying instantaneous reproduction number, $R_t$\"\n\n* Ex. using the number of reported COVID-19 cases in British Columbia between Jan. 2020 and Apr. 15, 2023. \n\n<!-- This data is the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. The values are.up-to-date as of August 2023. -->\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](day1-afternoon_files/figure-revealjs/nowcasting-1.svg){fig-align='center' height=400px}\n:::\n:::\n\n\n\n* Group built [`{rtestim}`](https://dajmcdon.github.io/rtestim) doing for this nonparametrically.\n\n* We may come back to this later...\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}