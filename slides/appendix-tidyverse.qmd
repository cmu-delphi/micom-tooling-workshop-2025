---
talk-title: "Background on `{tidyverse}`"
talk-short-title: "Background on `{tidyverse}`"
talk-subtitle: "MICOM Tooling Workshop 2025"
talk-date: "12 August 2025"
format: revealjs
---

{{< include _titleslide.qmd >}}

```{r theme-load-pkg}
library(tidyverse)
library(epidatr)
library(epipredict)
library(epidatasets)
```

# Essentials of `{dplyr}` and `{tidyr}`

## Down with spreadsheets for data manipulation

* Spreadsheets make it difficult to rerun analyses consistently.
* Using R (and `{dplyr}`) allows for:
  * Reproducibility
  * Ease of modification
* [**Recommendation**]{.primary}: Avoid manual edits; instead, use code for transformations.
* Let's see what we mean by this...

## Introduction to `dplyr`

* `dplyr` is a powerful package in R for data manipulation.
* It is part of the `tidyverse`, which includes a collection of packages designed to work together... Here's some of it's greatest hits:

<div style="text-align: center;">
![](gfx/tidyverse_packages.png){style="width: 30%; display: block; margin-left: auto; margin-right: auto;"}
<br>
<small>[Source](https://laddem.github.io/courses/tidyverse/)</small>
</div>

## Introduction to `dplyr`

![](gfx/dplyr.png){style="height: 450px"}

To load `dplyr` you may simply load the `tidyverse` package:

```{r load-tidyverse}
#| echo: true
# install.packages("tidyverse")
library(tidyverse)  # Load tidyverse, which includes dplyr & tidyr
```

## Introduction to `dplyr`
Our focus will be on basic operations like selecting and filtering data.

![](gfx/dplyr_and_fun.png){style="width: 70%;"}
<div style="text-align: center;">
<small>[Source](https://towardsdatascience.com/data-manipulation-in-r-with-dplyr-3095e0867f75)</small>
</div>


## Downloading JHU CSSE COVID-19 case data

* Let's start with something familiar... Here's a task for you:
* Use `pub_covidcast()` to download [**JHU CSSE COVID-19 confirmed case data**]{.primary} (`confirmed_incidence_num`) for CA, NC, and NY from March 1, 2022 to March 31, 2022 as of January 1, 2024.
* Try this for yourself. Then click the dropdown on the next slide to check your work...

## Downloading JHU CSSE COVID-19 case data

```{r fetch-jhu-dplyr-demo-data}
#| echo: true
#| code-fold: true
library(epidatr)

cases_df <- pub_covidcast(
  source = "jhu-csse",
  signals = "confirmed_incidence_num",
  geo_type = "state",
  time_type = "day",
  geo_values = "ca,nc,ny",
  time_values = epirange(20220301, 20220331),
  as_of = as.Date("2024-01-01")
)
```

Now we only really need a few columns here...
```{r head-jhu-dplyr-demo-data}
#| echo: true
cases_df <- cases_df |>
  select(geo_value, time_value, raw_cases = value) # We'll talk more about this soon :)
```

## Ways to inspect the dataset

Use `head()` to view the first six row of the data
```{r head}
#| echo: true
head(cases_df)  # First 6 rows
```

and tail to view the last six

```{r tail}
tail(cases_df)  # Last 6 rows
```

## Ways to inspect the dataset
Now, for our first foray into the `tidyverse`...

Use `glimpse()` to get a compact overview of the dataset.

```{r glimpse}
#| echo: true
glimpse(cases_df)
```

## Creating tibbles

* [**Tibbles**]{.primary}: Modern data frames with enhanced features.
* Rows represent [**observations**]{.primary} (or cases).
* Columns represent [**variables**]{.primary} (or features).
* You can create tibbles manually using the `tibble()` function.

```{r create-tibble}
#| echo: true
tibble(x = letters, y = 1:26)
```

## Selecting columns with `select()`

The `select()` function is used to pick specific columns from your dataset.

```{r select-columns}
#| echo: true
select(cases_df, geo_value, time_value)  # Select the 'geo_value' and 'time_value' columns
```

## Selecting columns with `select()`

You can exclude columns by prefixing the column names with a minus sign `-`.

```{r select-columns-exclude}
#| echo: true
select(cases_df, -raw_cases)  # Exclude the 'raw_cases' column from the dataset
```

## Extracting columns with `pull()`

* `pull()`: Extract a column as a vector.
* Let's try this with the `cases` column...

```{r pull-column-direct}
#| echo: true
pull(cases_df, raw_cases)
```

## Subsetting rows with `filter()`

<div style="text-align: center;">
![](gfx/dplyr_filter.png){style="width: 65%; display: block; margin-left: auto; margin-right: auto;"}
<br>
<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>
</div>

## Subsetting rows with `filter()`

* The `filter()` function allows you to subset rows that meet specific conditions.
* Conditions regard column values, such as filtering for only NC or cases higher than some threshold.
* This enables you to narrow down your dataset to focus on relevant data.

```{r filter-rows}
#| echo: true
filter(cases_df, geo_value == "nc", raw_cases > 500)  # Filter for NC with raw daily cases > 500
```

## Combining `select()` and `filter()` functions

* You can further combine `select()` and `filter()` to further refine the dataset.
* Use `select()` to choose columns and `filter()` to narrow down rows.
* This helps in extracting the exact data needed for analysis.

```{r select-filter-combine}
#| echo: true
select(filter(cases_df, geo_value == "nc", raw_cases > 1000), time_value, raw_cases) |>
  head()
```

## Using the pipe operator `|>`

* The pipe operator (`|>`) makes code more readable by chaining multiple operations together.
* The output of one function is automatically passed to the next function.
* This allows you to perform multiple steps (e.g., `filter()` followed by `select()`) in a clear and concise manner.

```{r pipe-operator}
#| echo: true
# This code reads more like poetry!
cases_df |>
  filter(geo_value == "nc", raw_cases > 1000) |>
  select(time_value, raw_cases) |>
  head()
```

## Key practices in `dplyr`

* Use [**tibbles**]{.primary} for easier data handling.
* Use `select()` and `filter()` for data manipulation.
* Use `pull()` to extract columns as vectors.
* Use `head()`, `tail()`, and `glimpse()` for quick data inspection.
* Chain functions with `|>` for cleaner code.

## Grouping data with `group_by()`

* Use `group_by()` to group data by one or more columns.
* Allows performing operations on specific groups of data.

```{r group-by-ex}
#| echo: true
cases_df |>
  group_by(geo_value) |>
  filter(raw_cases == max(raw_cases, na.rm = TRUE))
```

## Creating new columns with `mutate()`

<div style="text-align: center;">
![](gfx/dplyr_mutate.jpg){style="width: 45%; display: block; margin-left: auto; margin-right: auto;"}
<br>
<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>
</div>

## Creating new columns with `mutate()`

* `mutate()` is used to create new columns.
* Perform calculations using existing columns and assign to new columns.

```{r mutate-one-var-ex}
#| echo: true
ny_subset = cases_df |>
  filter(geo_value == "ny")

ny_subset |>
  mutate(cumulative_cases = cumsum(raw_cases)) |>
  head()
```

## Creating new columns with `mutate()`

* `mutate()` can create multiple new columns in one step.
* Logical comparisons (e.g., `over_5000 = raw_cases > 5000`) can be used within `mutate()`.

```{r mutate-two-var-ex}
#| echo: true
ny_subset |>
  mutate(over_5000 = raw_cases > 5000,
         cumulative_cases = cumsum(raw_cases)) |>
  head()
```

## Combining `group_by()` and `mutate()`

* First, group data using `group_by()`.
* Then, use `mutate` to perform the calculations for each group.
* Finally, use `arrange` to display the output by `geo_value`.

```{r group-by-mutate-combo}
#| echo: true
cases_df |>
  group_by(geo_value) |>
  mutate(cumulative_cases = cumsum(raw_cases)) |>
  arrange(geo_value) |>
  head()
```

## Conditional calculations with `if_else()`
* `if_else()` allows conditional logic within `mutate()`.
* Perform different operations depending on conditions, like "high" or "low."

```{r cond-calc-if-else}
#| echo: true
t <- 5000

cases_df |>
  mutate(high_low_cases = if_else(raw_cases > t, "high", "low")) |>
  head()
```

## Summarizing data with `summarise()`
* `summarise()` reduces data to summary statistics (e.g., mean, median).
* Typically used after `group_by()` to summarize each group.

```{r summarise-median-one-var}
#| echo: true
cases_df |>
  group_by(geo_value) |>
  summarise(median_cases = median(raw_cases))
```

## Using `count()` to aggregate data
`count()` is a shortcut for grouping and summarizing the data.

For example, if we want to get the total number of complete rows for each state, then
```{r summarise-count}
#| echo: true
cases_count <- cases_df |>
  drop_na() |> # Removes rows where any value is missing (from tidyr)
  group_by(geo_value) |>
  summarize(count = n())
```
<br>
is equivalent to

```{r count-fun}
#| echo: true
cases_count <- cases_df |>
  drop_na() |>
  count(geo_value)

cases_count # Let's see what the counts are.
```

## Key practices in `dplyr`: Round 2

* Use `group_by()` to group data by one or more variables before applying functions.
* Use `mutate` to create new columns or modify existing ones by applying functions to existing data.
* Use `summarise` to reduce data to summary statistics (e.g., mean, median).
* `count()` is a convenient shortcut for counting rows by group without needing `group_by()` and `summarise()`.

## Tidy data and Tolstoy

> "Happy families are all alike; every unhappy family is unhappy in its own way." â€” Leo Tolstoy

* [**Tidy datasets**]{.primary} are like happy families: consistent, standardized, and easy to work with.
* [**Messy datasets**]{.primary} are like unhappy families: each one messy in its own unique way.
In this section:
* We'll define what makes data *tidy* and how to transform between the tidy and messy formats.

## Tidy data and Tolstoy

![](gfx/tidy_messy_data.jpg){style="width: 60%;"}

<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>


## What is tidy data?

* Tidy data follows a consistent structure: [**each row represents one observation, and each column represents one variable.**]{.primary}
* `cases_df` is one classic example of tidy data.

```{r head-tidy-ex}
head(cases_df)
```

* To convert between tidy and messy data, we can use the `tidyr` package in the tidyverse.

## `pivot_wider()` and  `pivot_longer()`
<div style="text-align: center;">
![](gfx/pivot_wider_longer.jpg){style="width: 40%; display: block; margin-left: auto; margin-right: auto;"}
<br>
<small>[Artwork by @allison_horst](https://x.com/allison_horst)</small>
</div>

## Making data wider with `pivot_wider()`
* To convert data from long format to wide/messy format use`pivot_wider()`.
* For example, let's try creating a column for each time value in `cases_df`:

<!-- Example. Spreadsheet from hell -->

```{r pivot-wider-ex}
#| echo: true
messy_cases_df <- cases_df |>
  pivot_wider(
    names_from = time_value,   # Create new columns for each unique date
    values_from = raw_cases    # Fill those columns with the raw_case values
  )

# View the result
messy_cases_df
```

##  Tidying messy data with `pivot_longer()`
* Use `pivot_longer()` to convert data from [**wide format**]{.primary} (multiple columns for the same variable) to [**long format**]{.primary} (one column per variable).
* Let's try turning `messy_cases_df` back into the original tidy `cases_df`!

```{r pivot-longer-ex}
#| echo: true
tidy_cases_df <- messy_cases_df |>
  pivot_longer(
    cols = -geo_value,          # Keep the 'geo_value' column as it is
    names_to = "time_value",    # Create a new 'time_value' column from the column names
    values_to = "raw_cases"     # Values from the wide columns should go into 'raw_cases'
  )

# View the result
head(tidy_cases_df, n = 3) # Notice the class of time_value here
```

##  Tidying messy data with `pivot_longer()`

* When we used `pivot_longer()`, the `time_value` column is converted to a character class because the column names are treated as strings.
* So, to truly get the original `cases_df` we need to convert `time_value` back to the `Date` class.
* Then, we can use `identical()` to check if the two data frames are exactly the same.
```{r check-identical}
#| echo: true
tidy_cases_df = tidy_cases_df |> mutate(time_value = as.Date(time_value))

identical(tidy_cases_df |> arrange(time_value), cases_df)
```

Great. That was a success!

## Missing data
* Sometimes you may have missing data in your time series.
* Can be due to actual missing data, or it can be due to the fact that the data is only reported on certain days.
* Let's create a dataset with missing data & consider each of those cases:

```{r slice-to-subset}
#| echo: true
ca_missing <- cases_df |>
  filter(geo_value == "ca") |>
  slice(1:2, 4:6) # Subset rows 1 to 2 and 4 to 6; ie. omit 2022-03-03

ca_missing
```

## `complete()` and `fill()` to handle missing data

A simple workflow to handle missing data relies on one or both of these functions:

1. `complete()`: Adds missing rows for combinations of specified variables.

2. `fill()`: Fills missing values in columns, typically from previous or next available values (default is LOCF).

## Data only reported on certain days

* If the data is only reported on certain days, it is often useful to fill in the missing data with explicit zeros.
* `complete()` is enough to handle this:

```{r complete-zero-ex}
#| echo: true
# First, use complete() to add missing time_value (2022-03-03)
ca_complete <- ca_missing |>
  complete(geo_value, time_value = seq(min(time_value), max(time_value), by = "day"),
           fill = list(raw_cases = 0))
ca_complete
```
<!-- Using complete(time_value) doesn't work as expected because it doesn't automatically generate a sequence of missing dates between the min and max values of time_value. Instead, it simply tries to match the unique values in time_value and doesn't infer a complete range. To ensure complete() does what you want for time, it is best practice to explicitly create a sequence of dates that covers the entire range of time_value. -->

## Data is genuinely missing

* If the data is truly missing, then there are multiple options (ex. omission, single imputation, multiple imputation).
* A common single imputation method  used to handle missing data in time series or longitudinal datasets is LOCF.
* We can easily perform LOCF using `complete()` followed by `fill()`.
* Start with `complete()`:

```{r complete-NA-ex}
#| echo: true
# First, use complete() to add missing time_value (2022-03-03)
ca_complete <- ca_missing |>
  complete(geo_value, time_value = seq(min(time_value), max(time_value), by = "day"))
head(ca_complete, n = 4) # notice no fill with 0s this time, NA by default
```

## Data is genuinely missing
Then, use `fill()` to fill the counts using LOCF (default):

```{r fill-ex}
#| echo: true
ca_complete |>
  fill(raw_cases)
```

## Introduction to joins in `dplyr`
* Joining datasets is a powerful tool for combining info. from multiple sources.
* In R, `dplyr` provides several functions to perform different types of joins.
* We'll demonstrate joining a subset of `cases_df` (our case counts dataset) with `state_census`.
* [**Motivation**]{.primary}: We can scale the case counts by population to make them comparable across regions of different sizes.

## Subset `cases_df`

To simplify things, let's use `filter()` to only grab one date of `cases_df`:
```{r cases-df-one-date}
#| echo: true
cases_df_sub = cases_df |> filter(time_value == "2022-03-01")
cases_df_sub
```

Though note that what we're going to do can be applied to the entirety of `cases_df`.

## Load state census data

The `state_census` dataset from `epidatasets` contains state populations from the 2019 census.
```{r state-census}
#| echo: true
# State census dataset from epidatasets
library(epidatasets)
state_census = state_census |> select(abbr, pop) |> filter(abbr != "us")

state_census |> head()
```

Notice that this includes many states that are not in `cases_df_sub`.

## Left Join: Keep all rows from the first dataset

* A [**left join**]{.primary} keeps all rows from the [**first dataset**]{.primary} (`cases_df_sub`), and adds matching data from the second dataset (`state_census`).
* So [**all rows from the first dataset**]{.primary} (`cases_df_sub`) will be preserved.
* The datasets are joined by matching the `geo_value` column, specified by the by argument.

```{r left-join}
#| echo: true
# Left join: combining March 1, 2022 state case data with the census data
cases_left_join <- cases_df_sub |>
  left_join(state_census, join_by(geo_value == abbr))

cases_left_join
```

## Right Join: Keep all rows from the second dataset
* A [**right join**]{.primary} keeps all rows from the [**second dataset**]{.primary} (`state_census`), and adds matching data from the first dataset (`cases_df_sub`).
* If a row in the second dataset doesn't have a match in the first, then the columns from the first will be filled with NA.
* For example, can see this for the `al` row from `state_census`...

```{r right-join}
#| echo: true
# Right join: keep all rows from state_census
cases_right_join <- cases_df_sub |>
  right_join(state_census, join_by(geo_value == abbr))

head(cases_right_join)
```

## Inner Join: Only keeping matching rows
* An [**inner join**]{.primary} will only keep rows where there is a match in both datasets.
* So, if a state in `state_census` does not have a corresponding entry in `cases_df_sub`, then that row will be excluded.
```{r inner-join}
#| echo: true
# Inner join: only matching rows are kept
cases_inner_join <- cases_df_sub |>
  inner_join(state_census, join_by(geo_value == abbr))

cases_inner_join
```

## Full Join: Keeping all rows from both datasets

* A [**full join**]{.primary} will keep all rows from both datasets.
* If a state in either dataset has no match in the other, the missing values will be filled with NA.
```{r full-join}
#| echo: true
# Full join: keep all rows from both datasets
cases_full_join <- cases_df_sub |>
  full_join(state_census, join_by(geo_value == abbr))

head(cases_full_join)
```

## Pictorial summary of the four join functions

<!--* **Left join:** All rows from the left dataset and matching rows from the right dataset.
* **Right join:** All rows from the right dataset and matching rows from the left dataset.
* **Inner join:** Only matching rows from both datasets.
* **Full join:** All rows from both datasets, with NA where no match exists.-->

![](gfx/join_funs_cheatsheet.png){style="width: 40%; display: block; margin-left: auto; margin-right: auto;"}
<div style="text-align: center;">
<small>[Source](https://ohi-science.org/data-science-training/dplyr.html)</small>
</div>

## Final thoughts on joins
* Joins are an essential part of data wrangling in R.
* The choice of join depends on the analysis you need to perform:
    + Use [**left joins**]{.primary} when you want to keep all data from the first dataset.
    + Use [**right joins**]{.primary} when you want to keep all data from the second dataset.
    + Use [**inner joins**]{.primary} when you're only interested in matching rows.
    + Use [**full joins**]{.primary} when you want to preserve all information from both datasets.

## Three review questions

**Q1)**: What can we use to fill in the missing `time_value` for the states in `cases_full_join`?

```{r question-1}
#| echo: true
#| code-fold: true
#| results: hide
cases_full_join |>
     fill(time_value)
```
**Q2)**: Now, what join function should you use if your goal is to scale the cases by population in `cases_df`?
```{r question-2}
#| echo: true
#| code-fold: true
#| results: hide
# Either left_join
cases_left_join <- cases_df |>
  left_join(state_census, join_by(geo_value == abbr))

cases_left_join
cases_df = cases_left_join

# Or inner_join
cases_inner_join <- cases_df |>
  inner_join(state_census, join_by(geo_value == abbr))

cases_inner_join
```

**Q3)**: Finally, please create a new column in `cases_df` where you scale the cases by population and multiply by `1e5` to get cases / 100k.
```{r question-3}
#| echo: true
#| code-fold: true
#| results: hide
cases_df <- cases_df |>
  mutate(scaled_cases = raw_cases / pop * 1e5) # cases / 100K
head(cases_df)
```

