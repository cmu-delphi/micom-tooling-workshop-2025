---
talk-title: "Forecasting With `{epipredict}`"
talk-short-title: "Forecasting"
talk-subtitle: "MICOM Tooling Workshop 2025"
talk-date: "12 August 2025"
format: revealjs
---


{{< include _titleslide.qmd >}}


```{r ggplot-theme}
#| cache: false
ggplot2::theme_set(ggplot2::theme_bw())
```

## Outline

1. Overview of `{epipredict}`

2. Preprocessing

3. Basic Usage of `arx_forecaster()`

4. Extending `arx_forecaster()`

# Overview of `{epipredict}`
## `{epipredict}`

<https://cmu-delphi.github.io/epipredict>

#### Installation

```{r install, eval=FALSE}
#| echo: true
# Stable version
# We're using this.
pak::pkg_install("cmu-delphi/epipredict@main")


# Development version
pak::pkg_install("cmu-delphi/epipredict@dev")
```

## What `{epipredict}` provides (i)

Basic and easy to use ["canned" forecasters]{.primary}. There are several "basline" forecasters:

  * Baseline flat forecaster

  * CDC FluSight flatline forecaster
  
  * Climatological model
  
As well as two flexible autoregressive forecasters:

  * Autoregressive classifier

  * Autoregressive forecaster (ARX)


:::{.notes}
Planning on adding more.
:::

## What `{epipredict}` provides (ii)

* A framework for creating [custom forecasters]{.primary} out of [modular]{.primary} components.

* This is highly customizable, extends `{tidymodels}` to panel data

* Good for building a new forecaster from scratch

* Used to construct

* There are four types of components:

  1. [Preprocessor]{.primary}: do things to the data before model training

  1. [Trainer]{.primary}: train a model on data, resulting in a fitted model object

  1. [Predictor]{.primary}: make predictions, using a fitted model object

  1. [Postprocessor]{.primary}: do things to the predictions before returning

If you want examples of usage, see <https://cmu-delphi.github.io/epipredict/articles/custom_epiworkflows.html>

# Preprocessing
## Examples of pre-processing

::: {.fragment .fade-in-then-semi-out}

### Exploratory Data Analysis (EDA)

1. Making locations/signals commensurate (scaling)
1. Dealing with revisions
1. Detecting and removing outliers
1. Imputing or removing missing data

:::

::: {.fragment .fade-in-then-semi-out}

### Feature engineering

1. Creating lagged predictors
1. Day of Week effects
1. Rolling averages for smoothing
1. Lagged differences
1. Growth rates instead of raw signals
1. The sky's the limit

:::

```{r load-data}
source(here::here("_code/cases_deaths.R"))
cases_deaths <- full_join(cases, deaths, by = c("time_value", "geo_value")) |>
  as_epi_df()
```

## Creating a dataset to forecast with: first, fetch data

```{r get-data}
#| echo: true
#| eval: false
library(epidatr)
library(epiprocess)
library(epipredict)

cases <- pub_covidcast(
  source = "jhu-csse",
  signals = "confirmed_incidence_num",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20200401, 20230401),
  geo_values = "*") |>
  select(geo_value, time_value, cases = value)

deaths <- pub_covidcast(
  source = "jhu-csse",
  signals = "deaths_incidence_num",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20200401, 20230401),
  geo_values = "*") |>
  select(geo_value, time_value, deaths = value)

cases_deaths <- full_join(cases, deaths, by = c("time_value", "geo_value")) |>
  as_epi_df()
```



## Pre-processing: data scaling

Scale cases and deaths by population and multiply by 100K

```{r scale-data}
#| echo: true
cases_deaths <- left_join(
  x = cases_deaths,
  y = state_census |> select(pop, abbr),   # state_census is available in epipredict
  by = join_by(geo_value == abbr)
) |>
  mutate(
    cases = cases / pop * 1e5,
    deaths = deaths / pop * 1e5
  ) |>
  select(-pop)
```


## Scaled COVID cases and deaths

```{r autoplot-deaths}
#| echo: true
#| code-fold: true
#| fig-width: 7
cases_deaths |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  autoplot(cases, deaths) +
  scale_color_delphi(name = "") +
  xlab("Reference date")
```

## Pre-processing: smoothing 

Smooth the data by computing 7-day averages of cases and deaths for each state

```{r 7dav-data}
#| echo: true
cases_deaths <- cases_deaths |>
  group_by(geo_value) |>
  epi_slide(
    cases_7dav = mean(cases, na.rm = TRUE),
    deaths_7dav = mean(deaths, na.rm = TRUE),
    .window_size = 7
  ) |>
  ungroup() |>
  mutate(cases = NULL, deaths = NULL) |>
  rename(cases = cases_7dav, deaths = deaths_7dav)
```

## Scaled and smoothed COVID cases deaths

```{r autoplot-7dav-deaths}
#| echo: true
#| code-fold: true
#| fig-width: 7
cases_deaths |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  autoplot(cases, deaths)  +
  scale_color_delphi(name = "") +
  xlab("Reference date")
```



## Pre-processing: fix outliers and negative values

```{r outliers-deaths}
#| echo: true
#| code-fold: true
deaths_outlr <- cases_deaths |>
  group_by(geo_value) |>
  mutate(outlr = detect_outlr_rm(time_value, deaths, detect_negatives = TRUE)) |>
  unnest(outlr) |>
  ungroup()
```

```{r outliers-fig}
#| width: 7
deaths_outlr |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  ggplot(aes(x = time_value)) +
  geom_line(aes(y = deaths, col = geo_value), alpha = .2, key_glyph = "timeseries") +
  geom_line(aes(y = replacement, col = geo_value), , key_glyph = "timeseries") +
  geom_hline(yintercept = 0) +
  facet_wrap(vars(geo_value), scales = "free_y") +
  scale_color_delphi(name = "") +
  labs(x = "", y = "Covid-19 deaths per 100k people")
```

```{r outlier-cases, message=FALSE}
cases_outlr <- cases_deaths |>
  group_by(geo_value) |>
  mutate(outlr = detect_outlr_rm(time_value, cases, detect_negatives = TRUE)) |>
  unnest(outlr) |>
  ungroup()


cases_deaths$deaths <- deaths_outlr$replacement
cases_deaths$cases <- cases_outlr$replacement
```




# Basic Usage of `arx_forecaster`

## Fit `arx_forecaster` on training set

* Back to the [ARX(1)]{.primary} model for COVID deaths:
$\quad \hat y_{t+28} = \hat\phi + \hat\phi_0 y_{t} + \hat\beta_0 x_{t}$

* Using `{epipredict}`

```{r epipredict-arx}
#| echo: true
#| code-line-numbers: "|5-12"
# threshold data at our forecast date, to be historically accurate
t0_date <- as.Date('2021-04-01')
train <- cases_deaths %>% epix_as_of(version = t0_date)

# fit ARX
epi_arx <- arx_forecaster(
  epi_data = train |> as_epi_df(),
  outcome = "deaths",
  predictors = c("cases", "deaths"),
  trainer = quantile_reg(),
  args_list = arx_args_list(lags = 0, ahead = 28)
)
```

```{r}
#| echo: false
train <- cases_deaths |> filter(time_value <= t0_date)
test <- cases_deaths |> filter(time_value > t0_date)
```

## `arx_forecaster` outputs the predictions and a workflow object we can reuse{.smaller}

* A [forecast]{.primary} (point prediction + quantiles) for 28 days after the
last available time value in the data (`$predictions`).

```{r epi-pred-arx-output}
#| echo: true
epi_arx$predictions
```

* A [workflow]{.primary} object which could be used to create forecasts using
  the same training data on new observations (`$epi_workflow`).

<div class="scrollable-output">

```{r epi-workflow-ex, message=TRUE}
#| echo: true

epi_arx$epi_workflow
```

</div>


:::{.notes}
* All necessary preprocessing; both the sequence of steps, and any necessary
  statistics
* The fitted model object
* The sequence of steps for postprocessing
* typically, if you're making a forecast at a later date, you'd want to
  retrain using the newly released data however!

:::

## Extract predictions

`.pred_distin` is a distribution, which we can extract the distribution into a “long” `epi_df`:

```{r epi-pred-quantile-longer}
#| echo: true
epi_arx$predictions |>
  pivot_quantiles_longer(.pred_distn) |>
  select(-.pred) |>
  filter(geo_value == "ca")
```

or into a "wide" `epi_df`

```{r epi-pred-quantile-wider}
#| echo: true
epi_arx$predictions |>
  pivot_quantiles_wider(.pred_distn) |>
  filter(geo_value == "ca")
```


## `arx_forecaster` output{visibility="hidden"}

```{r output-arx, message=TRUE}
#| echo: true
epi_arx
```

## Predict with ARX (when re-fitting)

* Could use `predict(epi_arx$epi_workflow, new_data)` but better to retrain on latest data

* We fit and predict combining `arx_forecaster` with `epix_slide`

* From now on, we will only used [versioned data]{.primary}, and make predictions once a week

## Predict with ARX (re-fitting on trailing window)

```{r source-versioned-data}
source(here::here("_code/versioned_data.R"))
```

```{r epipredict-cv-trailing}
#| code-line-numbers: "8-13|"
#| echo: true
h <- 28         # horizon

# Specify the forecast dates
fc_time_values <- seq(from = t0_date, to = as.Date("2023-02-09"), by = "1 week")

# Slide the arx_forecaster over the epi_archive
pred_arx <- covid_archive |> epix_slide(
  ~ arx_forecaster(epi_data = .x,
                   outcome = "deaths",
                   predictors = c("cases", "deaths"),
                   trainer = quantile_reg(),
                   args_list = arx_args_list(lags = 0, ahead = h)
  )$predictions |>
    pivot_quantiles_wider(.pred_distn),
  .before = Inf,
  .versions = fc_time_values
)
```

:::{.notes}
Not a great way to read the output, but the new column is `version`, which `forecast_date` is always before.
:::

## Predict with ARX (re-fitting on trailing window)

<div class="large-output">

```{r epipredict-cv-trailing-head}
#| echo: true
pred_arx |> filter(geo_value == "ca")
```

</div>

## Predict with ARX (re-fitting on trailing window)

```{r arx-plot-cv-predictions}
#| fig-align: left
ca <- cases_deaths |> filter(geo_value == "ca")
pred_arx |>
  filter(geo_value == "ca") |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca, aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) +
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +
  geom_vline(xintercept = t0_date + h, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```

Aligned with the `target_date`, forecasted 4 weeks earlier on the `forecast_date`.


:::{.notes}
- First date gives the distance between `forecast_date` and `target_date`.
- Bands are the 10-90 quantile interval.
- Does reasonably well outside of early 2022, where it was overly pessimistic; also training on data from other states.
:::

## Customizing `arx_forecaster()`

```{r print-model-1}
#| echo: true
#| eval: false
#| code-line-numbers: "|4"
arx_forecaster(
  epi_data = train,
  outcome = "deaths",
  predictors = c("cases", "deaths"),
  trainer = quantile_reg(),
  args_list = arx_args_list(lags = 0, ahead = 28)
)
```

::: {.fragment .fade-in}
* Modify `predictors` to add/drop predictors

  * <span class="inner-list">e.g. drop `cases` to get AR model, or drop `deaths`
  for regression with a lagged predictor</span>

  * <span class="inner-list">default: `predictors = outcome`</span>

:::


## Customizing `arx_forecaster()`

```{r print-model-3}
#| echo: true
#| eval: false
#| code-line-numbers: "6-7"
arx_forecaster(
  epi_data = train,
  outcome = "deaths",
  predictors = c("cases", "deaths"),
  trainer = quantile_reg(),
  args_list = arx_args_list(lags = 0, ahead = 28)
)
```

* Modify `arx_args_list` to change lags, horizon, quantile levels, latency adjustment, ...

::: {.fragment .fade-in}
```{r arx_args_list}
#| echo: true
#| eval: false
arx_args_list(
  lags = c(0L, 7L, 14L),
  ahead = 7L,
  n_training = Inf,
  forecast_date = NULL,
  target_date = NULL,
  adjust_latency = c("none", "extend_ahead", "extend_lags", "locf"),
  warn_latency = TRUE,
  quantile_levels = c(0.05, 0.95),
  symmetrize = TRUE,
  nonneg = TRUE,
  quantile_by_key = character(0L),
  check_enough_data_n = NULL,
  check_enough_data_epi_keys = NULL,
  ...
)
```
:::

## Customizing `arx_forecaster`

### Change predictors: doctor visits instead of cases

```{r get-doctor-visits-data}
#| echo: true
#| eval: false
dv_archive <- pub_covidcast(
  source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20200401, 20230401),
  geo_values = "*",
  issues = epirange(20200401, 20230401)) |>
  select(geo_value, time_value, version = issue, doctor_visits = value) |>
  arrange(geo_value, time_value) |>
  as_epi_archive(compactify = FALSE)
```

## Customizing `arx_forecaster`

### Change predictors: doctor visits instead of cases

```{r get-archives, warning=FALSE}
ca_archive_dv <- covid_archive_dv$DT |>
  filter(geo_value == "ca") |>
  as_epi_archive()

usa_archive_dv <- covid_archive_dv$DT |>
  as_epi_archive()

usa_archive <- covid_archive$DT |>
  as_epi_archive()
```


```{r arx-with-dv}
#| echo: true
#| code-line-numbers: "4"
pred_arx_hosp <- covid_archive_dv |> epix_slide(
  ~ arx_forecaster(epi_data = .x,
                   outcome = "deaths",
                   predictors = c("deaths", "doctor_visits"),
                   trainer = quantile_reg(),
                   args_list = arx_args_list(lags = 0, ahead = 28)
  )$predictions |>
    pivot_quantiles_wider(.pred_distn),
  .before = Inf,
  .versions = fc_time_values
)
```

## Predictions (doctor visits instead of cases in predictor set)

```{r arx-with-dv-plot}
#| fig-align: left
pred_arx_hosp |>
  filter(geo_value == "ca") |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca, aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) +
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +
  geom_vline(xintercept = t0_date + h, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```

```{r function errors}
MSE <- function(truth, prediction) {
  mean((truth - prediction)^2)}

MAE <- function(truth, prediction) {
  mean(abs(truth - prediction))}

MAPE <- function(truth, prediction) {
  100 * mean(abs(truth - prediction) / truth)}

MASE <- function(truth, prediction) {
  100 * MAE(truth, prediction) / mean(abs(diff(truth)))}

getErrors <- function(truth, prediction, type) {
  return(data.frame(#"MSE" = MSE(truth, prediction), 
                    "MAE"= MAE(truth, prediction), 
                    #"MAPE" = MAPE(truth, prediction), 
                    "MASE" = MASE(truth, prediction), 
                    row.names = type))
}

getAccuracy = function(finalized, predictions, row_name = "") {
  observed = (finalized |> 
                filter(time_value %in% predictions$target_date))$deaths
  return(cbind(
    getErrors(observed, predictions$.pred, ""),
    "Coverage" = mean(observed >= predictions$`0.1` & observed <= predictions$`0.9`), 
    row.names = row_name))
}
```

```{r error-arx}
getAccuracy(ca, pred_arx)
```

Quite different predictions!

## Customizing `arx_forecaster`

### Add more lags

```{r arx-with-more-lags}
#| echo: true
#| code-line-numbers: "7"
pred_arx_more_lags <- covid_archive_dv |> epix_slide(
  ~ arx_forecaster(epi_data = .x,
                   outcome = "deaths",
                   predictors = c("deaths", "doctor_visits"),
                   trainer = quantile_reg(quantile_levels = c(0.1, 0.5, 0.9)),
                   args_list = arx_args_list(
                     lags = list(deaths=c(0, 7, 14), doctor_visits=c(0, 7)),
                     ahead = 28,
                     quantile_levels=c(0.1, 0.5, 0.9)
                   )
  )$predictions |>
    pivot_quantiles_wider(.pred_distn),
  .before = Inf,
  .versions = fc_time_values
)
```

## Predictions (more lags)

```{r arx-with-more-lags-plot}
#| fig-align: left
pred_arx_hosp |>
  filter(geo_value == "ca") |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = ca, aes(x = time_value, y = deaths), inherit.aes = FALSE, col = base) +
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +
  geom_vline(xintercept = t0_date + h, lty = 2) +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.title = element_blank())
```

More isn't always better

# Extending `arx_forecaster`
## `arx_forecaster` for multiple horizons
### Multiple horizons

```{r arx-multiple-h}
#| echo: true
#| code-line-numbers: "1-2,11,20-21"
forecast_times <- seq(from = t0_date, to = as.Date("2023-02-23"), by = "2 month")
pred_h_days_ahead <- function(epi_archive, ahead = 7) {
  epi_archive |>
    epix_slide(
      ~ arx_forecaster(epi_data = .x,
                       outcome = "deaths",
                       predictors = c("deaths", "doctor_visits"),
                       trainer = quantile_reg(quantile_levels = c(.1, .5, .9)),
                       args_list = arx_args_list(
                         lags = 0,
                         ahead = ahead,
                         quantile_levels =  c(.1, .5, .9)
                       )
      )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = Inf,
  .versions = forecast_times
  )
}
h <- 7 * c(0, 1, 2, 3, 4, 5, 6)
forecasts <- bind_rows(map(h, ~ pred_h_days_ahead(ca_archive_dv, ahead = .x)))
```

## Predictions (multiple horizons)

```{r arx-multiple-h-plot}
#| fig-width: 8
#| fig-height: 5
ggplot(data = forecasts, aes(x = target_date, group = forecast_date)) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`, fill = factor(forecast_date)),
              alpha = 0.4) +
  geom_vline(aes(xintercept = forecast_date, color = factor(forecast_date)),
             lty = 2) +
  geom_line(data = ca,
    aes(x = time_value, y = deaths),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5
  ) +
  geom_line(aes(y = .pred, color = factor(forecast_date))) +
  geom_point(aes(y = .pred, color = factor(forecast_date))) +
  scale_color_viridis_d() +
  scale_fill_viridis_d() +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  labs(x = "", y = "Deaths per 100k people") +
  theme(legend.position = "none")
```

Color corresponds to `forecast_date`

## Geo-pooling

* When we observe data over time from [multiple locations]{.primary}
(e.g. states or counties), we could:

  * Estimate coefficients [separately]{.primary} for each location, or
  * Fit one model using all locations together at each time point ([geo-pooling]{.primary}).

Epipredict geo-pools

## Predictions with geo-pooling (reprise)


We've been geo-pooling in the previous examples, here's the version using
doctor's visits.

```{r error-arx-geo-pooling}
#| eval: false
#| echo: true
pred_arx_hosp <- covid_archive_dv |> epix_slide(
  ~ arx_forecaster(epi_data = .x,
                   outcome = "deaths",
                   predictors = c("deaths", "doctor_visits"),
                   trainer = quantile_reg(),
                   args_list = arx_args_list(lags = 0, ahead = 28)
  )$predictions |>
    pivot_quantiles_wider(.pred_distn),
  .before = Inf,
  .versions = fc_time_values
)
```

## Predictions (geo-pooling, $h=28$)

```{r finalized-ma-ny-tx}
ma <- cases_deaths |> filter(geo_value == "ma")
ny <- cases_deaths |> filter(geo_value == "ny")
tx <- cases_deaths |> filter(geo_value == "tx")
```

```{r arx-geo-pooling-plot}
#| fig-width: 7
pred_arx_hosp |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = rbind(ca, ma, ny, tx),  aes(x = time_value, y = deaths),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5) +
  geom_line(col = tertiary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = tertiary) +
  geom_vline(xintercept = t0_date) +
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  facet_wrap(vars(geo_value), scales = 'free_y') +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.position = "none")

```



```{r error-geo-pooling-all-states}
rbind(getAccuracy(ca,
                  pred_arx_geo_pool |>
                    filter(geo_value == "ca" & target_date %in% ca$time_value),
                  "CA"),
      getAccuracy(ma,
                  pred_arx_geo_pool |>
                    filter(geo_value == "ma" & target_date %in% ma$time_value),
                  "MA"),
      getAccuracy(ny,
                  pred_arx_geo_pool |>
                    filter(geo_value == "ny" & target_date %in% ny$time_value),
                  "NY"),
      getAccuracy(tx,
                  pred_arx_geo_pool |>
                    filter(geo_value == "tx" & target_date %in% tx$time_value),
                  "TX"))
```

## Predict without geo-pooling

```{r arx-no-geo-pooling}
#| echo: true
#| code-line-numbers: "4,23"
pred_arx_no_geo_pool <- function(archive, ahead = 28, lags = 0){
  archive |>
    epix_slide(
      ~ group_by(.x, geo_value) |>
        group_map(.keep = TRUE, function(group_data, group_key) {
          arx_forecaster(epi_data = group_data,
                         outcome = "deaths",
                         predictors = c("deaths", "doctor_visits"),
                         trainer = quantile_reg(quantile_levels=c(0.1, 0.9)),
                         args_list = arx_args_list(
                           lags = lags,
                           ahead = ahead,
                           quantile_levels = c(0.1, 0.9))
                         )$predictions |>
            pivot_quantiles_wider(.pred_distn)
        }) |>
        list_rbind(),
    .before = Inf,
    .versions = fc_time_values
    )}

pred_no_geo_pool_28 <- pred_arx_no_geo_pool(usa_archive_dv$DT |>
                                              filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
                                              as_epi_archive())
```

## Predictions (without geo-pooling, $h=28$)

```{r arx-no-geo-pooling-plot}
#| fig-width: 7
pred_no_geo_pool_28 |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = rbind(ca, ma, ny, tx),  aes(x = time_value, y = deaths),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5) +
  geom_line(col = primary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = primary) +
  geom_vline(xintercept = t0_date) +
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  facet_wrap(vars(geo_value), scales = 'free_y') +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.position = "none")

```

```{r error-no-geo-pooling-all-states}
rbind(getAccuracy(ca,
                  pred_no_geo_pool_28 |>
                    filter(geo_value == "ca" & target_date %in% ca$time_value),
                  "CA"),
      getAccuracy(ma,
                  pred_no_geo_pool_28 |>
                    filter(geo_value == "ma" & target_date %in% ma$time_value),
                  "MA"),
      getAccuracy(ny,
                  pred_no_geo_pool_28 |>
                    filter(geo_value == "ny" & target_date %in% ny$time_value),
                  "NY"),
      getAccuracy(tx,
                  pred_no_geo_pool_28 |>
                    filter(geo_value == "tx" & target_date %in% tx$time_value),
                  "TX"))
```

## Geo-pooling or not?

* Geo-pooled predictions tend to be [more stable]{.primary}

* Generally with [wider intervals]{.primary} (and better coverage)

* Meanwhile, predictions from state-wise models tend to be [more volatile]{.primary}

The extent to which this occurs differs based on the horizon.

## What are these ARX intervals?{visibility="hidden"}

* `{epipredict}` takes quantiles of training residuals to form its prediction intervals
* In comparison to traditional (parametric) intervals from `lm()`, this is more flexible
* It can in principle adapt to asymmetric or heavy-tailed error distributions

<br>

Taking quantiles of training residuals can be problematic if the model is overfit.

<br>

Quantile regression provides an alternative, wherein we estimate these quantiles directly

Technically, `grf_quantiles` was using Quantile Loss with Random Forests

## Quantile regression{visibility="hidden"}

Now we directly target conditional quantiles of the outcome over time.

Estimating tail quantiles [requires more data]{.primary}, so

  * unsuitable for settings with small training set (e.g. trailing window on one state)

  * can benefit by combination with geo-pooling (much more data to train on)

```{r qr-geo-pooling}
#| echo: true
#| code-line-numbers: "|8"
library(quantreg)

pred_qr_geo_pool <- usa_archive_dv |>
  epix_slide(
    ~ arx_forecaster(epi_data = .x,
                     outcome = "deaths",
                     predictors = c("deaths", "doctor_visits"),
                     trainer = quantile_reg(),
                     args_list = arx_args_list(
                       lags = 0,
                       ahead = 28,
                       quantile_levels = c(0.1, 0.9))
                     )$predictions |>
        pivot_quantiles_wider(.pred_distn),
  .before = w,
  .versions = fc_time_values
)
```

## Predictions (geo-pooling + quantile regression, $h=28$){visibility="hidden"}

```{r qr-geo-pooling-plot}
#| fig-width: 7
pred_qr_geo_pool |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = rbind(ca, ma, ny, tx),  aes(x = time_value, y = deaths),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5) +
  geom_line(col = tertiary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = tertiary) +
  geom_vline(xintercept = t0_date) +
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  facet_wrap(vars(geo_value), scales = 'free_y') +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.position = "none")

```

```{r error-qr-geo-pooling-all-states}
rbind(getAccuracy(ca,
                  pred_qr_geo_pool |>
                    filter(geo_value == "ca" & target_date %in% ca$time_value),
                  "CA"),
      getAccuracy(ma,
                  pred_qr_geo_pool |>
                    filter(geo_value == "ma" & target_date %in% ma$time_value),
                  "MA"),
      getAccuracy(ny,
                  pred_qr_geo_pool |>
                    filter(geo_value == "ny" & target_date %in% ny$time_value),
                  "NY"),
      getAccuracy(tx,
                  pred_qr_geo_pool |>
                    filter(geo_value == "tx" & target_date %in% tx$time_value),
                  "TX"))
```

## Predictions (geo-pooling + linear regression, $h=28$){visibility="hidden"}

```{r arx-geo-pooling-plot-lm}
#| fig-width: 7
pred_arx_geo_pool |>
  filter(geo_value %in% c("ca", "ma", "ny", "tx")) |>
  ggplot(aes(target_date, .pred)) +
  geom_line(data = rbind(ca, ma, ny, tx),  aes(x = time_value, y = deaths),
    inherit.aes = FALSE, na.rm = TRUE, alpha = .5) +
  geom_line(col = tertiary) +
  geom_ribbon(aes(ymin = `0.1`, ymax = `0.9`), alpha = .3, fill = tertiary) +
  geom_vline(xintercept = t0_date) +
  geom_vline(xintercept = t0_date + 28, lty = 2) +
  facet_wrap(vars(geo_value), scales = 'free_y') +
  labs(x = "", y = "Deaths per 100k people") +
  scale_y_continuous(expand = expansion(c(0, .05))) +
  theme(legend.position = "none")

```

```{r error-geo-pooling-all-states-lm}
rbind(getAccuracy(ca,
                  pred_arx_geo_pool |>
                    filter(geo_value == "ca" & target_date %in% ca$time_value),
                  "CA"),
      getAccuracy(ma,
                  pred_arx_geo_pool |>
                    filter(geo_value == "ma" & target_date %in% ma$time_value),
                  "MA"),
      getAccuracy(ny,
                  pred_arx_geo_pool |>
                    filter(geo_value == "ny" & target_date %in% ny$time_value),
                  "NY"),
      getAccuracy(tx,
                  pred_arx_geo_pool |>
                    filter(geo_value == "tx" & target_date %in% tx$time_value),
                  "TX"))
```


## Plot our forecasts{visibility="hidden"}

```{r plot-prod}
#| fig-width: 7
preds_wide <- preds |> pivot_quantiles_wider(.pred) |>
  filter(geo_value %in% geos)
ggplot() +
  geom_linerange(data = preds_wide, aes(x = target_date, ymin = `0.1`, ymax = `0.9`),
                 color = primary, alpha = .2, linewidth = 2) +
  geom_linerange(data = preds_wide, aes(x = target_date, ymin = `0.25`, ymax = `0.75`),
                 color = primary, alpha = .4, linewidth = 2) +
  geom_line(data = flu_data |> filter(time_value > "2024-09-01", geo_value %in% geos),
            aes(time_value, hhs)) +
  geom_point(data = preds_wide, aes(x = target_date, y = `0.5`), color = base) +
  facet_wrap(~geo_value, scales = "free_y") +
  geom_vline(xintercept = max_time_value + days(7), color = "grey40") +
  scale_y_continuous(expand = expansion(c(0, .05)))
```


# Ensembling{visibility="hidden"}

## Ensembling{visibility="hidden"}

Instead of choosing one model, we can [combine]{.primary} the predictions from multiple base models. Ensemble types:

* [untrained]{.primary}: combine base models, agnostic to past performance

* [trained]{.primary}: weight base models, accounting for past performance

Simplest untrained method: simple average of base model forecasts

$$
\hat{y}^{\text{avg}}_{t+h|t} = \frac{1}{p} \sum_{j=1}^p \hat{y}^j_{t+h|t}
$$

A more robust option: simple median of base model forecasts

$$
\hat{y}^{\text{med}}_{t+h|t} = \mathrm{median}\Big\{ \hat{y}^j_{t+h|t} : j = 1,\dots,p \Big\}
$$

## Example from the Covid-19 Forecast Hub{visibility="hidden"}

![](gfx/cramer.png)

## Two key goals of ensembling{visibility="hidden"}

1 [Compete-with-best]{.primary}: ensemble should have accuracy competitive with best individual constituent model

2. [Robustness-over-all]{.primary}: ensemble should have greater robustness than any individual constituent model

Typically these are hard to accomplish simultaneously, and untrained methods
excel at point 2, whereas trained methods can achieve point 1
