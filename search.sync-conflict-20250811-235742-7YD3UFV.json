[
  {
    "objectID": "slides/day2-morning.html#section",
    "href": "slides/day2-morning.html#section",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Quick Tour of Time Series Forecasting",
    "text": "Quick Tour of Time Series Forecasting\nMICOM Tooling Workshop 2025\n\nDavid Weber, Nat DeFries\nAdapted from slides by Alice Cima, Rachel Lobay, Daniel McDonald, Ryan Tibshirani, with huge thanks to Logan Brooks, Xueda Shen, and Dmitry Shemetov\n12 August 2025"
  },
  {
    "objectID": "slides/day2-morning.html#forecasting-is-not-magic",
    "href": "slides/day2-morning.html#forecasting-is-not-magic",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Forecasting is not magic",
    "text": "Forecasting is not magic\n\nForecasts are generally comprised of two parts: trend and seasonality\nMethods for detecting and projecting trends are not magic; in general they’re not qualitatively that different from what you can do with your eyeballs\nThat said, assimilating information from exogenous features (ideally, leading indicators) can lead to highly nontrivial gains, beyond the eyeballs\nRemember … good data is just as (more?) important as a good model!\nSeasonality can help short-term forecasts. Long-term forecasts, absent of strong seasonality, are generally not very tractable"
  },
  {
    "objectID": "slides/day2-morning.html#basics-of-linear-regression",
    "href": "slides/day2-morning.html#basics-of-linear-regression",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Basics of linear regression",
    "text": "Basics of linear regression\n\nAssume we observe a predictor \\(x_i\\) and an outcome \\(y_i\\) for \\(i = 1, \\dots, n\\).\nLinear regression seeks coefficients \\(\\beta_0\\) and \\(\\beta_1\\) such that\n\n\\[y_i \\approx \\beta_0 + \\beta_1 x_i\\]\nis a good approximation for every \\(i = 1, \\dots, n\\).\n\nIn R, the coefficients are found by running lm(y ~ x), where y is the vector of responses and x the vector of predictors."
  },
  {
    "objectID": "slides/day2-morning.html#multiple-linear-regression",
    "href": "slides/day2-morning.html#multiple-linear-regression",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nGiven \\(p\\) different predictors, we seek \\((p+1)\\) coefficients such that\n\n\\[y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\\] is a good approximation for every \\(i = 1, \\dots, n\\)."
  },
  {
    "objectID": "slides/day2-morning.html#linear-regression-with-lagged-predictor",
    "href": "slides/day2-morning.html#linear-regression-with-lagged-predictor",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Linear regression with lagged predictor",
    "text": "Linear regression with lagged predictor\n\nIn time series, outcomes and predictors are usually indexed by time \\(t\\).\n\n\n\nGoal: predicting future \\(y\\), given present \\(x\\).\n\n\n\n\nModel: linear regression with lagged predictor\n\n\\[\\hat y_t = \\hat \\beta + \\hat \\beta_0 x_{t-k}\\]\ni.e. regress the outcome \\(y\\) at time \\(t\\) on the predictor \\(x\\) at time \\(t-k\\).\n\n\n\nEquivalent way to write the model:\n\n\\[\\hat y_{t+k} = \\hat \\beta + \\hat \\beta_0 x_t\\]"
  },
  {
    "objectID": "slides/day2-morning.html#example-predicting-covid-deaths",
    "href": "slides/day2-morning.html#example-predicting-covid-deaths",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Example: predicting COVID deaths",
    "text": "Example: predicting COVID deaths\n\nDuring the pandemic, interest in predicting COVID deaths 7, 14, 21, 28 days ahead.\nCan we reasonably predict COVID deaths 28 days ahead by just using cases today?\n\n\n\nIf we let\n\n\\[y_{t+28} = \\text{deaths at time } t+28 \\quad\\quad x_{t} = \\text{cases at time } t\\] is the following a good model?\n\\[\\hat y_{t+28} = \\hat\\beta_0 + \\hat\\beta_1 x_{t}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#example-covid-cases-and-deaths-in-california",
    "href": "slides/day2-morning.html#example-covid-cases-and-deaths-in-california",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Example: COVID cases and deaths in California",
    "text": "Example: COVID cases and deaths in California\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhead(ca)\n\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2024-11-06 01:50:44.00687\n\n# A tibble: 6 × 4\n# Groups:   geo_value [1]\n  geo_value time_value cases deaths\n  &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 ca        2020-04-01  3.17 0.0734\n2 ca        2020-04-02  3.48 0.0835\n3 ca        2020-04-03  3.44 0.0894\n4 ca        2020-04-04  3.05 0.0778\n5 ca        2020-04-05  3.28 0.0876\n6 ca        2020-04-06  3.37 0.0848\n\n\n\n\n\n\n\nNote\n\n\nCases seem highly correlated with deaths several weeks later (but relation cases-deaths changes over time)."
  },
  {
    "objectID": "slides/day2-morning.html#checking-correlation",
    "href": "slides/day2-morning.html#checking-correlation",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Checking correlation",
    "text": "Checking correlation\n\nLet’s split the data into a training and a test set (before/after 2021-04-01).\nOn training set: large correlation between cases and deaths 28 days ahead (&gt; 0.95).\n\n\n\n\nLet’s use (base) R to prepare the data and fit\n\n\\[\\hat y_{t+28} = \\hat\\beta + \\hat\\beta_0 x_{t}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#preparing-the-data",
    "href": "slides/day2-morning.html#preparing-the-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Preparing the data",
    "text": "Preparing the data\n\nca$lagged_cases &lt;- dplyr::lag(ca$cases, n = k)     # Add column with cases lagged by k\nt0_date &lt;- as.Date('2021-04-01')                   # Split into train and test (before/after t0_date)\ntrain &lt;- ca |&gt; filter(time_value &lt;= t0_date)\ntest &lt;- ca |&gt; filter(time_value &gt; t0_date)\n\nCheck if deaths is approximately linear in lagged_cases:"
  },
  {
    "objectID": "slides/day2-morning.html#fitting-lagged-linear-regression-in-r",
    "href": "slides/day2-morning.html#fitting-lagged-linear-regression-in-r",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Fitting lagged linear regression in R",
    "text": "Fitting lagged linear regression in R\n\nreg_lagged = lm(deaths ~ lagged_cases, data = train)\ncoef(reg_lagged)\n\n (Intercept) lagged_cases \n   0.1171839    0.0112714"
  },
  {
    "objectID": "slides/day2-morning.html#different-error-metrics-have-pros-and-cons",
    "href": "slides/day2-morning.html#different-error-metrics-have-pros-and-cons",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Different error metrics have pros and cons",
    "text": "Different error metrics have pros and cons\n\n\nFour commonly used error metrics are:\n\nmean squared error (MSE)\nmean absolute error (MAE)\nmean absolute percentage error (MAPE)\nmean absolute scaled error (MASE)"
  },
  {
    "objectID": "slides/day2-morning.html#estimating-the-prediction-error",
    "href": "slides/day2-morning.html#estimating-the-prediction-error",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Estimating the prediction error",
    "text": "Estimating the prediction error\n\nGiven an error metric, we want to estimate the prediction error under that metric.\nThis can be accomplished in different ways, using the\n\nTraining error\nSplit-sample error\nTime series cross-validation error (using all past data or a trailing window)"
  },
  {
    "objectID": "slides/day2-morning.html#training-error",
    "href": "slides/day2-morning.html#training-error",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Training error",
    "text": "Training error\n\nThe easiest but worst approach to estimate the prediction error is to use the training error, i.e. the average error on the training set that was used to fit the model.\nThe training error is\n\ngenerally too optimistic as an estimate of prediction error\nmore optimistic the more complex the model!1\n\n\nMore on this when we talk about overfitting."
  },
  {
    "objectID": "slides/day2-morning.html#training-error-1",
    "href": "slides/day2-morning.html#training-error-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Training error",
    "text": "Training error\nLinear regression of COVID deaths on lagged cases\n\n# Getting the predictions for the training set\npred_train &lt;- predict(reg_lagged)\n\n\n\n\n               MAE     MASE\ntraining 0.0740177 380.9996"
  },
  {
    "objectID": "slides/day2-morning.html#split-sample-error",
    "href": "slides/day2-morning.html#split-sample-error",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Split-sample error",
    "text": "Split-sample error\nTo compute the split-sample error\n\nSplit data into training (up to time \\(t_0\\)), and test set (after \\(t_0\\))\nFit the model to the training data only\nMake predictions for the test set\nCompute the selected error metric on the test set only\n\n\n\n\nNote\n\n\nSplit-sample estimates of prediction error don’t mimic a situation where we would refit the model in the future. They are pessimistic if the relation between outcome and predictors changes over time."
  },
  {
    "objectID": "slides/day2-morning.html#split-sample-error-1",
    "href": "slides/day2-morning.html#split-sample-error-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Split-sample error",
    "text": "Split-sample error\nLinear regression of COVID deaths on lagged cases\n\n# Getting h-step ahead predictions for the test set\nh &lt;- k\ntest_h &lt;- test[-(1:h-1), ] # drop first h-1 rows to avoid data leakage\npred_test &lt;- predict(reg_lagged, newdata = test_h)\n\n\n\n\n                   MAE      MASE\ntraining     0.0740177  380.9996\nsplit-sample 0.3116854 2914.4575\n\n\n\nNote that we are overestimating the peak due to the changed relationship between cases - deaths over time.\nTalk about data leakage."
  },
  {
    "objectID": "slides/day2-morning.html#warning",
    "href": "slides/day2-morning.html#warning",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Warning!",
    "text": "Warning!\n\nPredictions are overshooting the target, especially in early 2022 (Omicron phase).\nThis is because we are predicting deaths using lagged cases, but the relation between the two changes over time."
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Time series cross-validation (CV)",
    "text": "Time series cross-validation (CV)\n\\(h\\)-step ahead predictions\n\nIf we refit in the future once new data are available, a more appropriate way to estimate the prediction error is time series cross-validation.\nTo get \\(h\\)-step ahead predictions, for each time \\(t = t_0, t_0+1, \\dots\\),\n\nFit the model using data up to time \\(t\\)\nMake a prediction for \\(t+h\\)\nRecord the prediction error\n\nThe cross-validation MSE is then\n\n\\[CVMSE = \\frac{1}{n-h-t_0} \\sum_{t = t_0}^{n-h} (\\hat y_{t+h|t} - y_{t+h})^2\\]\nwhere \\(\\hat y_{t+h|t}\\) indicates a prediction for \\(y\\) at time \\(t+h\\) that was made with data available up to time \\(t\\)."
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv-1",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Time series cross-validation (CV)",
    "text": "Time series cross-validation (CV)\nLinear regression of COVID deaths on lagged cases\n\nn &lt;- nrow(ca)                               #length of time series\nh &lt;- k                                      #number of days ahead for which prediction is wanted\npred_all_past &lt;- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to all past data and make h-step ahead prediction\n  reg_all_past = lm(deaths ~ lagged_cases, data = ca, subset = (1:n) &lt;= t)\n  pred_all_past[t+h] = predict(reg_all_past, newdata = data.frame(ca[t+h, ]))\n}\n\n\n\n\nNote\n\n\nWith the current model, we can only predict \\(k\\) days ahead (where \\(k\\) = number of days by which predictor is lagged)!"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cross-validation-cv-2",
    "href": "slides/day2-morning.html#time-series-cross-validation-cv-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Time series cross-validation (CV)",
    "text": "Time series cross-validation (CV)\nLinear regression of COVID deaths on lagged cases\n\n\n\n                     MAE      MASE\ntraining       0.0740177  380.9996\nsplit-sample   0.3116854 2914.4575\ntime series CV 0.2374931 2212.5992\n\n\n\nSome improvement wrt split-sample, but still overestimating peak."
  },
  {
    "objectID": "slides/day2-morning.html#warning-1",
    "href": "slides/day2-morning.html#warning-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Warning!",
    "text": "Warning!\n\nPredictions are still overshooting the target, but error is smaller than split-sample.\nWhy?\n\n 👍 Forecaster is partially learning the change in cases-deaths relation (especially in late 2022)\n👎 We refit on all past data, so predictions are still influenced by old cases-deaths relation\n\n\n\n\n\nIdea 💡\n\n\nIgnore old data when refitting?"
  },
  {
    "objectID": "slides/day2-morning.html#regression-on-a-trailing-window",
    "href": "slides/day2-morning.html#regression-on-a-trailing-window",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Regression on a trailing window",
    "text": "Regression on a trailing window\n\nFit the model on a window of data of length \\(w\\), starting at \\(t-w\\) and ending at \\(t\\).\nAdvantage: if the predictors-outcome relation changes over time, training the forecaster on a window of recent data can better capture the recent relation which might be more relevant to predict the outcome in the near future.\nWindow length \\(w\\) considerations:\n\nif \\(w\\) is too big, the model can’t adapt to the recent predictors-outcome relation \nif \\(w\\) is too small, the fitted model may be too volatile (trained on too little data)"
  },
  {
    "objectID": "slides/day2-morning.html#trailing-window",
    "href": "slides/day2-morning.html#trailing-window",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Trailing window",
    "text": "Trailing window\nLinear regression of COVID deaths on lagged cases\n\n# Getting the predictions through CV with trailing window\nw &lt;- 120                                    #trailing window size\nh &lt;- k                                      #number of days ahead for which prediction is wanted\npred_trailing &lt;- rep(NA, length = n)        #initialize vector of predictions\n\nfor (t in t0:(n-h)) {\n  # fit to a trailing window of size w and make h-step ahead prediction\n  reg_trailing = lm(deaths ~ lagged_cases, data = ca,\n                    subset = (1:n) &lt;= t & (1:n) &gt; (t-w))\n  pred_trailing[t+h] = predict(reg_trailing, newdata = data.frame(ca[t+h, ]))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-all-past-vs-trailing-window",
    "href": "slides/day2-morning.html#time-series-cv-all-past-vs-trailing-window",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Time series CV: all past vs trailing window",
    "text": "Time series CV: all past vs trailing window\nLinear regression of COVID deaths on lagged cases\n\n\n\n                                 MAE      MASE\ntraining                  0.07401770  380.9996\nsplit-sample              0.31168536 2914.4575\ntime series CV            0.23749306 2212.5992\ntime series CV + trailing 0.09932651  925.3734\n\n\n\nA lot of improvement: trailing window allows to adapt to the change in relationship between cases and deaths over time."
  },
  {
    "objectID": "slides/day2-morning.html#autoregressive-exogenous-input-arx-model",
    "href": "slides/day2-morning.html#autoregressive-exogenous-input-arx-model",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Autoregressive exogenous input (ARX) model",
    "text": "Autoregressive exogenous input (ARX) model\n\nIdea: predicting the outcome via a linear combination of its lags and a set of exogenous (i.e. external) input variables\nExample:\n\n\\[\\hat y_{t+h} = \\hat\\phi + \\sum_{i=0}^p \\hat\\phi_i y_{t-i} + \\sum_{j=0}^q \\hat\\beta_j x_{t-j}\\]\n\nNotice: we don’t need to include all contiguous lags, and we could fit e.g.\n\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}\\]"
  },
  {
    "objectID": "slides/day2-morning.html#arx-model-for-covid-deaths",
    "href": "slides/day2-morning.html#arx-model-for-covid-deaths",
    "title": "MICOM EpiData Workshop 2025",
    "section": "ARX model for COVID deaths",
    "text": "ARX model for COVID deaths\n\nLet’s add lagged deaths as a predictor to our previous forecaster:\n\n\\[\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}\\]\n\nWe will refer to this model as ARX(1), as it only includes one lag for each predictor.\n\n\n# Prepare data: add column with deaths lagged by 28\nca$lagged_deaths &lt;- dplyr::lag(ca$deaths, n = k)\n\n\nHow does it compare to the previous model in terms of time series CV?\n\n\n\n\nNote\n\n\nFrom now on, we will only consider regression on a trailing window, since regression on all past data leads to overshooting during Omicron."
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-trailing-arx1-vs-lm-on-lagged-cases",
    "href": "slides/day2-morning.html#time-series-cv-trailing-arx1-vs-lm-on-lagged-cases",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Time series CV (trailing): ARX(1) vs lm on lagged cases",
    "text": "Time series CV (trailing): ARX(1) vs lm on lagged cases\n\n\n\n                          MAE     MASE\nARX(1)             0.07852942 731.6178\nlm on lagged cases 0.09932651 925.3734\n\n\n\nErrors under both metrics are smaller than with previous model."
  },
  {
    "objectID": "slides/day2-morning.html#warning-2",
    "href": "slides/day2-morning.html#warning-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Warning!",
    "text": "Warning!\nRegression on a trailing window can be quite sensitive to data issues."
  },
  {
    "objectID": "slides/day2-morning.html#warning-3",
    "href": "slides/day2-morning.html#warning-3",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Warning!",
    "text": "Warning!\n\nAt the forecast date when the downward dip in deaths is predicted, the coefficients estimated by ARX(1) are\n\n\n\n  (Intercept) lagged_deaths  lagged_cases \n  0.067259206   0.304075294  -0.004285251 \n\n\n\nThe downward dip is explained by the negative coefficient on lagged_cases, and by the fact that at the forecast date\n\nobserved deaths are exactly equal to 0 (data issue)\nobserved cases are increasing"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h",
    "href": "slides/day2-morning.html#predictions-for-different-h",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions for different \\(h\\)",
    "text": "Predictions for different \\(h\\)\n\nSo far we only focused on COVID death predictions 28 days ahead.\nWe will now compare the model with lagged cases as predictor\n\n\\[\\hat y_{t+h} = \\hat\\beta + \\hat\\beta_0 x_t\\]\nto the ARX(1) model\n\\[\\hat y_{t+h} = \\hat\\phi + \\hat\\phi_0 y_t + \\hat\\beta_0 x_t\\]\nfor horizons \\(h = 7, 14, 21, 28\\).\n\nWe will only make forecasts on the \\(1^{st}\\) day of each month, and use a trailing window with \\(w = 120\\)."
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-1",
    "href": "slides/day2-morning.html#predictions-for-different-h-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions for different \\(h\\)",
    "text": "Predictions for different \\(h\\)\n\nh_vals &lt;- c(7, 14, 21, 28)  #horizons\npred_m1 = pred_m2 &lt;- data.frame(matrix(NA, nrow = 0, ncol = 3))  #initialize df for predictions\ncolnames(pred_m1) = colnames(pred_m2) = c(\"forecast_date\", \"target_date\", \"prediction\")\nw &lt;- 120    #trailing window size\n\nca_lags &lt;- ca |&gt; select(!c(lagged_cases, lagged_deaths))\n\n# Create lagged predictors\nfor (i in seq_along(h_vals)) {\n  ca_lags[[paste0(\"lagged_deaths_\", h_vals[i])]] &lt;- dplyr::lag(ca_lags$deaths, n = h_vals[i])\n  ca_lags[[paste0(\"lagged_cases_\", h_vals[i])]] &lt;- dplyr::lag(ca_lags$cases, n = h_vals[i])\n}\n\n# Only forecast on 1st day of the months\nforecast_time &lt;- which(ca_lags$time_value &gt;= t0_date &\n                         ca_lags$time_value &lt; ca_lags$time_value[n-max(h_vals)] &\n                         day(ca_lags$time_value) == 1)\n\nfor (t in forecast_time) {\n  for (i in seq_along(h_vals)) {\n    h = h_vals[i]\n    # formulas including h-lagged variables\n    m1_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h))\n    m2_formula = as.formula(paste0(\"deaths ~ lagged_cases_\", h, \" + lagged_deaths_\", h))\n    # fit to trailing window of data\n    m1_fit = lm(m1_formula, data = ca_lags, subset = (1:n) &lt;= t & (1:n) &gt; (t-w))\n    m2_fit = lm(m2_formula, data = ca_lags, subset = (1:n) &lt;= t & (1:n) &gt; (t-w))\n    # make h-step ahead predictions\n    pred_m1 = rbind(pred_m1,\n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m1_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    pred_m2 = rbind(pred_m2,\n                    data.frame(forecast_date = ca_lags$time_value[t],\n                               target_date = ca_lags$time_value[t+h],\n                               prediction = predict(m2_fit, newdata = data.frame(ca_lags[t+h, ]))))\n    }\n}"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-lm-on-lagged-cases",
    "href": "slides/day2-morning.html#predictions-for-different-h-lm-on-lagged-cases",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions for different \\(h\\), lm on lagged cases",
    "text": "Predictions for different \\(h\\), lm on lagged cases\n\n\n\n                         MAE    MASE\nlm on lagged cases 0.1049742 304.007"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-for-different-h-arx1",
    "href": "slides/day2-morning.html#predictions-for-different-h-arx1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions for different \\(h\\), ARX(1)",
    "text": "Predictions for different \\(h\\), ARX(1)\n\n\n\n              MAE     MASE\nARX(1) 0.04463132 129.2531"
  },
  {
    "objectID": "slides/day2-morning.html#visualizing-predictions-for-multiple-horizons",
    "href": "slides/day2-morning.html#visualizing-predictions-for-multiple-horizons",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Visualizing predictions for multiple horizons",
    "text": "Visualizing predictions for multiple horizons\nDifferent ways to visualize predictions for multiple \\(h\\)\n\nLast slides: group by forecast date, and show prediction “trajectories”\nOther approach: one line and color per horizon \\(h\\)"
  },
  {
    "objectID": "slides/day2-morning.html#predictions-by-horizon-arx1",
    "href": "slides/day2-morning.html#predictions-by-horizon-arx1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions by horizon, ARX(1)",
    "text": "Predictions by horizon, ARX(1)"
  },
  {
    "objectID": "slides/day2-morning.html#arx-models-with-2-and-3-lags",
    "href": "slides/day2-morning.html#arx-models-with-2-and-3-lags",
    "title": "MICOM EpiData Workshop 2025",
    "section": "ARX models with 2 and 3 lags",
    "text": "ARX models with 2 and 3 lags\n\nThe ARX(1) model \\(\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}\\) has good predictive performance\nWe will now try to improve the ARX(1) model by including more lags in the set of predictors\nLet’s consider two extensions: the ARX(2) model\n\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7}\\]\nand the ARX(3) model\n\\[\\hat y_{t+h} = \\hat \\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-7} + \\hat\\phi_2 y_{t-14} +\n\\hat\\beta_0 x_{t} + \\hat\\beta_1 x_{t-7} + \\hat\\beta_2 x_{t-14}\\]\nand fit them using a trailing window with \\(w = 120\\)."
  },
  {
    "objectID": "slides/day2-morning.html#time-series-cv-trailing-arx1-arx2-and-arx3",
    "href": "slides/day2-morning.html#time-series-cv-trailing-arx1-arx2-and-arx3",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Time series CV (trailing): ARX(1), ARX(2), and ARX(3)",
    "text": "Time series CV (trailing): ARX(1), ARX(2), and ARX(3)\n\n\n\n              MAE      MASE\nARX(1) 0.07852942  731.6178\nARX(2) 0.08716160  812.0393\nARX(3) 0.12487694 1163.4135"
  },
  {
    "objectID": "slides/day2-morning.html#warning-4",
    "href": "slides/day2-morning.html#warning-4",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Warning!",
    "text": "Warning!\nAs we add more predictors, forecasts seem more volatile and errors increase."
  },
  {
    "objectID": "slides/day2-morning.html#overfitting-1",
    "href": "slides/day2-morning.html#overfitting-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Overfitting",
    "text": "Overfitting\nWhen we introduce too many predictors in the model\n\nThe estimated coefficients will be chosen to mimic the observed data very closely on the training set, leading to small training error\nThe predictive performance on the test set might be very poor, producing large split-sample and CV error"
  },
  {
    "objectID": "slides/day2-morning.html#extreme-case-arx-model-with-120-predictors",
    "href": "slides/day2-morning.html#extreme-case-arx-model-with-120-predictors",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Extreme case: ARX model with 120 predictors",
    "text": "Extreme case: ARX model with 120 predictors\n\nWhat happens if we increase the number of predictors to 120?\nLet’s fit\n\n\\[\\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\phi_1 y_{t-1} + \\dots +\n\\hat\\phi_{59} y_{t-59} +\n\\hat\\beta_0 x_{t} + \\dots + \\hat\\beta_{t-59} x_{t-59}\\]\nand compare training vs split-sample errors"
  },
  {
    "objectID": "slides/day2-morning.html#extreme-case-predictions-on-training-and-test-set",
    "href": "slides/day2-morning.html#extreme-case-predictions-on-training-and-test-set",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Extreme case: predictions on training and test set",
    "text": "Extreme case: predictions on training and test set\n\n\n\n                   MAE    MASE\nsplit-sample 0.3978198 3706.28\n\n\n\n\n\nNote\n\n\nSome predictions were negative, which doesn’t make sense for count data, so we truncated them at 0."
  },
  {
    "objectID": "slides/day2-morning.html#point-predictions-vs-intervals",
    "href": "slides/day2-morning.html#point-predictions-vs-intervals",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Point predictions vs intervals",
    "text": "Point predictions vs intervals\n\nSo far, we have only considered point predictions, i.e. we have fitted models to provide our best guess on the outcome at time \\(t+h\\).\n\n\n\n\nImportant\n\n\nWhat if we want to provide a measure of uncertainty around the point prediction or a likely range of values for the outcome at time \\(t+h\\)?\n\n\n\n\nFor each target time \\(t+h\\), we can construct prediction intervals, i.e. provide ranges of values that are expected to cover the true outcome value a fixed fraction of times."
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-lm-fits",
    "href": "slides/day2-morning.html#prediction-intervals-for-lm-fits",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Prediction intervals for lm fits",
    "text": "Prediction intervals for lm fits\n\nTo get prediction intervals for the models we previously fitted, we only need to tweak our call to predict by adding as an input:\ninterval = \"prediction\", level = p\nwhere \\(p \\in (0, 1)\\) is the desired coverage.\nThe output from predict will then be a matrix with\n\nfirst column a point estimate\nsecond column the lower limit of the interval\nthird column the upper limit of the interval"
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window",
    "href": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Prediction intervals for ARX (CV, trailing window)",
    "text": "Prediction intervals for ARX (CV, trailing window)\n\n# Initialize matrices to store predictions\n# 3 columns: point estimate, lower limit, and upper limit\npred_interval_lm &lt;- matrix(NA, nrow = n, ncol = 3)\ncolnames(pred_interval_lm) &lt;- c('prediction', 'lower', 'upper')\n\nfor (t in t0:(n-h)) {\n  # Fit ARX and predict\n  arx_trailing = lm(deaths ~ lagged_deaths + lagged_cases, data = ca,\n                    subset = (1:n) &lt;= t & (1:n) &gt; (t-w))\n  pred_interval_lm[t+h, ] = pmax(0,\n                              predict(arx_trailing, newdata = data.frame(ca[t+h, ]),\n                                      interval = \"prediction\", level = 0.8))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window-1",
    "href": "slides/day2-morning.html#prediction-intervals-for-arx-cv-trailing-window-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Prediction intervals for ARX (CV, trailing window)",
    "text": "Prediction intervals for ARX (CV, trailing window)\n\n\n\n                   MAE     MASE\nlm.trailing 0.08932857 832.2278"
  },
  {
    "objectID": "slides/day2-morning.html#expected-vs-actual-coverage",
    "href": "slides/day2-morning.html#expected-vs-actual-coverage",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Expected vs actual coverage",
    "text": "Expected vs actual coverage\n\nWe would expect the ARX model to cover the truth about 80% of the times. Is this actually true in practice?\nThe actual coverage of the predictive intervals is lower:\n\n\n\n         Actual Expected\nCoverage    0.6      0.8\n\n\n\nWe can use calibration to handle under-covering (more on this in the afternoon)"
  },
  {
    "objectID": "slides/day2-morning.html#versioned-data",
    "href": "slides/day2-morning.html#versioned-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Versioned data",
    "text": "Versioned data\nSo far: data never revised (or simply ignored revisions, as_of today)\n\n\n\nImportant\n\n\nHow can we train forecasters when dealing with versioned data?\n\n\n\n\nca_archive\n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-04-01 / 2023-03-09\nℹ First/last version with update: 2020-04-02 / 2023-03-10\nℹ Versions end: 2023-03-10\nℹ A preview of the table (24953 rows x 5 columns):\nKey: &lt;geo_value, time_value, version&gt;\n       geo_value time_value    version case_rate death_rate\n          &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;     &lt;num&gt;      &lt;num&gt;\n    1:        ca 2020-04-01 2020-04-02  3.009195 0.06580240\n    2:        ca 2020-04-01 2020-05-07  3.009195 0.06327156\n    3:        ca 2020-04-01 2020-06-21  3.009195 0.06580242\n    4:        ca 2020-04-01 2020-07-02  2.978825 0.06580242\n    5:        ca 2020-04-01 2020-07-03  2.978825 0.06580242\n   ---                                                     \n24949:        ca 2023-03-07 2023-03-08  0.000000 0.00000000\n24950:        ca 2023-03-07 2023-03-10 27.397832 0.00000000\n24951:        ca 2023-03-08 2023-03-09 21.083071 0.00000000\n24952:        ca 2023-03-08 2023-03-10  0.000000 0.00000000\n24953:        ca 2023-03-09 2023-03-10 22.185487 0.52072650"
  },
  {
    "objectID": "slides/day2-morning.html#version-aware-forecasting",
    "href": "slides/day2-morning.html#version-aware-forecasting",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Version-aware forecasting",
    "text": "Version-aware forecasting\nImportant: when fitting and predicting, only use data in the latest version available at the forecast date!"
  },
  {
    "objectID": "slides/day2-morning.html#version-aware-forecasting-1",
    "href": "slides/day2-morning.html#version-aware-forecasting-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Version-aware forecasting",
    "text": "Version-aware forecasting\n\n# initialize dataframe for predictions\n# 5 columns: forecast date, target date, 10%, 50%, and 90% quantiles\npred_aware &lt;- data.frame(matrix(NA, ncol = 5, nrow = 0))\n\nw &lt;- 120         #trailing window size\nh &lt;- 28          #number of days ahead\n\n# forecast once a week\nfc_time_values &lt;- seq(from = t0_date, to = as.Date(\"2023-02-09\"), by = \"1 week\")\n\nfor (fc_date in fc_time_values) {\n  # get data version as_of forecast date\n  data &lt;- epix_as_of(ca_archive, version = as.Date(fc_date))\n  # create lagged predictors\n  data$lagged_deaths &lt;- dplyr::lag(data$deaths, h)\n  data$lagged_cases &lt;- dplyr::lag(data$cases, h)\n  # perform regression\n  lm_weekly &lt;- lm(deaths ~ lagged_deaths + lagged_cases,\n                  # only consider window of data\n                  data = data |&gt; filter(time_value &gt; (max(time_value) - w)))\n  # construct data.frame with the right predictors for the target date\n  predictors &lt;- data.frame(lagged_deaths = tail(data$deaths, 1),\n                           lagged_cases = tail(data$cases, 1))\n  # make predictions for target date and add them to dataframe of predictions\n  pred_aware &lt;- rbind(pred_aware,\n                      data.frame('forecast_date' = max(data$time_value),\n                                 'target_date' = max(data$time_value) + h,\n                                 t(pmax(0, predict(lm_weekly, newdata = predictors,\n                                                   interval = \"prediction\", level = 0.8)))))\n}"
  },
  {
    "objectID": "slides/day2-morning.html#version-aware-predictions-cv-trailing",
    "href": "slides/day2-morning.html#version-aware-predictions-cv-trailing",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Version-aware predictions (CV, trailing)",
    "text": "Version-aware predictions (CV, trailing)\n\n\n\n                     MAE     MASE\nversion-aware 0.08001814 224.2782"
  },
  {
    "objectID": "slides/day2-morning.html#version-unaware-predictions-cv-trailing",
    "href": "slides/day2-morning.html#version-unaware-predictions-cv-trailing",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Version-unaware predictions (CV, trailing)",
    "text": "Version-unaware predictions (CV, trailing)\n\n\n\n                       MAE     MASE\nversion-unaware 0.07934554 200.4657"
  },
  {
    "objectID": "slides/day1-morning.html#section",
    "href": "slides/day1-morning.html#section",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Introduction to Panel Data in Epidemiology",
    "text": "Introduction to Panel Data in Epidemiology\nMICOM Tooling Workshop 2025\n\nDavid Weber, Nat DeFries\nAdapted from slides by Alice Cima, Rachel Lobay, Daniel McDonald, Ryan Tibshirani, with huge thanks to Logan Brooks, Xueda Shen, and Dmitry Shemetov\n12 August 2025"
  },
  {
    "objectID": "slides/day1-morning.html#what-we-will-cover",
    "href": "slides/day1-morning.html#what-we-will-cover",
    "title": "MICOM EpiData Workshop 2025",
    "section": "What we will cover",
    "text": "What we will cover\n\nCharacteristics of panel data in epidemiology\nTools for processing and plotting panel data\nStatistical background on nowcasting and forecasting\nTools for building nowcasting and forecasting models"
  },
  {
    "objectID": "slides/day1-morning.html#system-setup-required-software",
    "href": "slides/day1-morning.html#system-setup-required-software",
    "title": "MICOM EpiData Workshop 2025",
    "section": "System setup – Required software",
    "text": "System setup – Required software\n\nWe assume you already have\n\n\nR\n\n\n\nAn IDE. We’ll use RStudio, but you can use VSCode or Emacs or Whatnot"
  },
  {
    "objectID": "slides/day1-morning.html#system-setup-downloading-the-materials",
    "href": "slides/day1-morning.html#system-setup-downloading-the-materials",
    "title": "MICOM EpiData Workshop 2025",
    "section": "System setup – Downloading the materials",
    "text": "System setup – Downloading the materials\n\nEasy way:\n\nClick the Big Green Button that says &lt; &gt; Code ▾\nChoose Download Zip\nOpen the Zip directory and then Open micom-tooling-workshop-2025.Rproj\n\nMore expert (local git user):\n\nClick the Big Green Button that says &lt; &gt; Code ▾\nCopy the URL.\nOpen RStudio, select File &gt; New Project &gt; Version Control. Paste there and proceed.\n\nEven more expert (via github remote):\n\nClick the Grey Button that says ⑂ Fork ▾\nProceed along the same lines as above."
  },
  {
    "objectID": "slides/day1-morning.html#system-setup-installing-required-packages",
    "href": "slides/day1-morning.html#system-setup-installing-required-packages",
    "title": "MICOM EpiData Workshop 2025",
    "section": "System setup – Installing required packages",
    "text": "System setup – Installing required packages\n\nWe will use a lot of packages.\nWe’ve tried to make it so you can get them all at once (with the right versions)\n🤞 We hope this works… 🤞 Note that you can “Copy to Clipboard”\n\nIn RStudio:\ninstall.packages(\"renv\")\nrenv::restore()\n\nHopefully, you see: ::: {.cell layout-align=“center”} ::: {.cell-output .cell-output-stderr}\n✔ You should be good to go!\n::: :::\nAsk for help if you see something like: ::: {.cell layout-align=“center”} ::: {.cell-output .cell-output-error}\nError in `verify_setup()`:\n! The following packages do not have the correct version:\nℹ Installed: epipredict 0.2.0.\nℹ Required: epipredict == 0.1.5.\n::: :::"
  },
  {
    "objectID": "slides/day1-morning.html#panel-data-1",
    "href": "slides/day1-morning.html#panel-data-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Panel data",
    "text": "Panel data\n\nPanel data or longitudinal data, contain cross-sectional measurements of subjects over time.\nSince we’re working with aggregated data, the subjects are geographic units (e.g. counties, states).\n\n\n\nIn table form, panel data is a time index + one or more locations/keys.\nEx: The % of outpatient doctor visits that are COVID-related in CA from June 2020 to Dec. 2021 (docs): ::: {.cell layout-align=“center”} ::: {.cell-output .cell-output-stdout}\n\n# A tibble: 549 × 3\n   time_value geo_value percent_cli\n   &lt;date&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1 2020-06-01 ca               2.75\n 2 2020-06-02 ca               2.57\n 3 2020-06-03 ca               2.48\n 4 2020-06-04 ca               2.41\n 5 2020-06-05 ca               2.57\n 6 2020-06-06 ca               2.63\n 7 2020-06-07 ca               2.73\n 8 2020-06-08 ca               3.04\n 9 2020-06-09 ca               2.97\n10 2020-06-10 ca               2.99\n# ℹ 539 more rows\n::: :::"
  },
  {
    "objectID": "slides/day1-morning.html#examples-of-panel-data---covid-19-cases",
    "href": "slides/day1-morning.html#examples-of-panel-data---covid-19-cases",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Examples of panel data - COVID-19 cases",
    "text": "Examples of panel data - COVID-19 cases\nJHU CSSE COVID cases per 100k  estimates the daily number of new confirmed COVID-19 cases per 100,000 population, averaged over the past 7 days.\n\n\n\nWA switch to weekly reporting in 2022\nFL reports “whenever” (weekly, biweekly, three days in a row, then 4 zeros, etc.)\nAPI calculates change from cumulative, so no-report becomes a 0.\nIf state decreases total, then we see a negative."
  },
  {
    "objectID": "slides/day1-morning.html#intro-to-versioned-data",
    "href": "slides/day1-morning.html#intro-to-versioned-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Intro to versioned data",
    "text": "Intro to versioned data\n\nMany epidemic aggregates are subject to reporting delays and revisions\nThis is because individual-level data has delayed availability:\n\nPerson comes to ER → Admitted → Has some tests → Tests come back → Entered into the system → …\n\nSo, a “Hospital admission” may not attributable to a particular condition until a few days have passed (the patient may even have been released)\nAggregated data have a longer pipeline from the incident to the report.\nSo we have to track both: when the event occurred and when it was reported\nAdditionally, various mistakes lead to revisions\nThis means there can be many different values for the same date"
  },
  {
    "objectID": "slides/day1-morning.html#versioned-data-1",
    "href": "slides/day1-morning.html#versioned-data-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Versioned data",
    "text": "Versioned data\n\nThe event time is indicated by time_value (aka reference_date)\nNow, we add a second time index to indicate the data version (aka reporting_date)\nversion = the time at which we saw a particular value associated to a time_value ::: {.cell layout-align=“center”} ::: {.cell-output .cell-output-stdout}\n\n# A tibble: 6 × 4\n  time_value geo_value percent_cli version   \n  &lt;date&gt;     &lt;chr&gt;           &lt;dbl&gt; &lt;date&gt;    \n1 2020-06-01 ca               2.14 2020-06-06\n2 2020-06-01 ca               2.14 2020-06-08\n3 2020-06-01 ca               2.11 2020-06-09\n4 2020-06-01 ca               2.13 2020-06-10\n5 2020-06-01 ca               2.20 2020-06-11\n6 2020-06-01 ca               2.23 2020-06-12\n::: :::\n\nNote that this feature can be indicated in different ways (ex. version, issue, release, as_of)."
  },
  {
    "objectID": "slides/day1-morning.html#versioned-panel-data",
    "href": "slides/day1-morning.html#versioned-panel-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Versioned panel data",
    "text": "Versioned panel data\nEstimated percentage of outpatient visits due to CLI across multiple versions."
  },
  {
    "objectID": "slides/day1-morning.html#latency-and-revision-in-signals",
    "href": "slides/day1-morning.html#latency-and-revision-in-signals",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Latency and revision in signals",
    "text": "Latency and revision in signals\n\nRevised data is updated or corrected after initial publication\n\nExample: COVID-19 case reports are revised reporting backlogs are cleared\n\nLatency the delay between data collection and availability\n\nExample: A signal based on insurance claims may take several days to appear as claims are processed"
  },
  {
    "objectID": "slides/day1-morning.html#revisions",
    "href": "slides/day1-morning.html#revisions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Revisions",
    "text": "Revisions\nMany data sources are subject to revisions:\n\nCase and death counts are frequently corrected or adjusted by authorities\nMedical claims can take weeks to be submitted and processed\n\n\n\nLab tests and medical records can be backlogged\nSurveys are not completed promptly\n\nAn accurate revision log is crucial for researchers building forecasts\n\n\n\n\n\n\nObvious but crucial\n\n\nA forecast that is made today can only use data we have access to today"
  },
  {
    "objectID": "slides/day1-morning.html#three-types-of-revisions",
    "href": "slides/day1-morning.html#three-types-of-revisions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Three types of revisions",
    "text": "Three types of revisions\n\nSources that don’t revise (provisional and final are the same)\n\nGoogle symptoms\n\nPredictable revisions\n\nClaims data (CHNG) and public health reports aligned by test, hospitalization, or death date\nAlmost always revised upward as additional claims enter the pipeline\n\nRevisions that are large and erratic to predict\n\nCOVID cases and deaths\nThese are aligned by report date"
  },
  {
    "objectID": "slides/day1-morning.html#types-of-revisions---comparison-between-2.-and-3.",
    "href": "slides/day1-morning.html#types-of-revisions---comparison-between-2.-and-3.",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Types of revisions - Comparison between 2. and 3.",
    "text": "Types of revisions - Comparison between 2. and 3.\n\nRevision behavior for two indicators in the HRR containing Charlotte, NC.\n\n\n\nDV-CLI signal (left): regularly revised, but effects fade\nJHU CSSE cases (right) remain “as first reported” until a major correction is made on Oct. 19"
  },
  {
    "objectID": "slides/day1-morning.html#key-takeaways",
    "href": "slides/day1-morning.html#key-takeaways",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nMedical claims revisions\n\nMore systematic and predictable\n\n\n\n\nCOVID-19 case report revisions\n\nErratic and often unpredictable\n\n\n\n\nLarge spikes or anomalies can occur as\n\nReporting backlogs are cleared\n\n\nChanges in case definitions are implemented"
  },
  {
    "objectID": "slides/day1-morning.html#what-is-the-epidata-repository",
    "href": "slides/day1-morning.html#what-is-the-epidata-repository",
    "title": "MICOM EpiData Workshop 2025",
    "section": "What is the Epidata repository",
    "text": "What is the Epidata repository\nEpidata: repository of aggregated epi-surveillance time series\nCode is open-source. Most indicators are public.\n\nTo date, it has accumulated over 5 billion records.\nAt the peak of the pandemic, handled millions of API queries per day.\nMany aren’t available elsewhere\n\n\nData from\n\npublic health reporting, medical insurance claims, medical device data, Google search queries, wastewater, app-based mobility patterns.\n\n\n\n\nAdded value\n\nrevision tracking, anomaly detection, trend detection, smoothing, imputation, geo-temporal-demographic disaggregation."
  },
  {
    "objectID": "slides/day1-morning.html#installing-epidatr",
    "href": "slides/day1-morning.html#installing-epidatr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Installing {epidatr}",
    "text": "Installing {epidatr}\n(you already did this, but just for posterity…)\nInstall the CRAN version\n\n# Install the CRAN version\npak::pkg_install(\"epidatr\")\n\n\nor the development version\n\n# Install the development version from the GitHub dev branch\npak::pkg_install(\"cmu-delphi/epidatr@dev\")\n\nThe CRAN listing is here."
  },
  {
    "objectID": "slides/day1-morning.html#api-keys",
    "href": "slides/day1-morning.html#api-keys",
    "title": "MICOM EpiData Workshop 2025",
    "section": "API keys",
    "text": "API keys\n\nAnyone may access the Epidata API anonymously without providing any personal data!!\nAnonymous API access is subject to some restrictions: public datasets only; 60 requests per hour; only two parameters may have multiple selections\nAPI key grants privileged access; can be obtained by registering with us\nPrivileges of registration: no rate limit; no limit on multiple selections\nWe just want to know which signals people care about and ensure we’re providing benefit\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe {epidatr} client automatically searches for the key in the DELPHI_EPIDATA_KEY environment variable.\nWe recommend storing it in your .Renviron file, which R reads by default.\nMore on setting your API key here."
  },
  {
    "objectID": "slides/day1-morning.html#using-epidatr",
    "href": "slides/day1-morning.html#using-epidatr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Using {epidatr}",
    "text": "Using {epidatr}\n\nlibrary(epidatr)\nhhs_flu_nc &lt;- pub_covidcast(\n  source = 'hhs',\n  signals = 'confirmed_admissions_influenza_1d',\n  geo_type = 'state',\n  time_type = 'day',\n  geo_values = 'nc',\n  time_values = c(20240401, 20240405:20240414)\n)\nhead(hhs_flu_nc, n = 3)\n\n\n\n# A tibble: 3 × 15\n  geo_value signal     source geo_type time_type time_value direction issue     \n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;    \n1 nc        confirmed… hhs    state    day       2024-04-01        NA 2024-04-22\n2 nc        confirmed… hhs    state    day       2024-04-05        NA 2024-04-22\n3 nc        confirmed… hhs    state    day       2024-04-06        NA 2024-04-22\n# ℹ 7 more variables: lag &lt;dbl&gt;, missing_value &lt;dbl&gt;, missing_stderr &lt;dbl&gt;,\n#   missing_sample_size &lt;dbl&gt;, value &lt;dbl&gt;, stderr &lt;dbl&gt;, sample_size &lt;dbl&gt;\n\n\n\n\n\n\nThe covidcast endpoint supports * in its time and geo fields.\nSignal values for all available counties: replace geo_values = \"06059\" with geo_values = \"*\"."
  },
  {
    "objectID": "slides/day1-morning.html#returned-data---covidcast-main-endpoint",
    "href": "slides/day1-morning.html#returned-data---covidcast-main-endpoint",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Returned data - COVIDcast main endpoint",
    "text": "Returned data - COVIDcast main endpoint\npub_covidcast() outputs a tibble, where each row represents one observation\nEach observation is aggregated by time and by geographic region\n\ntime_value: time period when the events occurred.\ngeo_value: geographic region where the events occurred.\nvalue: estimated value.\nstderr: standard error of the estimate, usually referring to the sampling error.\nsample_size: number of events used in the estimation."
  },
  {
    "objectID": "slides/day1-morning.html#returned-data---covidcast-main-endpoint-1",
    "href": "slides/day1-morning.html#returned-data---covidcast-main-endpoint-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Returned data - COVIDcast main endpoint",
    "text": "Returned data - COVIDcast main endpoint\nAlso reports\n\nissue: The date this observation was published\nlag: The period between when the events occurred and when the observation was published\n\nTracks the complete revision history of the signal\nAllows for historical reconstructions of information that was available at a specific times\nMore on this soon!"
  },
  {
    "objectID": "slides/day1-morning.html#versioned-data-in-epidatr",
    "href": "slides/day1-morning.html#versioned-data-in-epidatr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Versioned data in {epidatr}",
    "text": "Versioned data in {epidatr}\nEpidata API contains each signal’s estimate, location, date, and update timeline\nRequesting Specific Data Versions:\n\nUse as_of or issues to specify data availability\nas_of always fetches one version\nissues can fetch multiple\nOnly one may be used at a time"
  },
  {
    "objectID": "slides/day1-morning.html#obtaining-data-as-of-a-specific-date",
    "href": "slides/day1-morning.html#obtaining-data-as-of-a-specific-date",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Obtaining data “as of” a specific date",
    "text": "Obtaining data “as of” a specific date\nDoctor Visits (from the covidcast endpoint)\n\nThe percentage of outpatient visits w/ Covid-like illness\nPennsylvania on May 1, 2020:\n\n\ndv_pa_as_of &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  time_values = \"2020-05-01\",\n  geo_type = \"state\",\n  geo_values = \"pa\",\n  as_of = \"2020-05-07\"\n)\n\n\n\n# A tibble: 1 × 7\n  geo_value signal           source        time_value issue        lag value\n  &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 pa        smoothed_adj_cli doctor-visits 2020-05-01 2020-05-07     6  2.58\n\n\n\nInitial estimate issued on May 7, 2020\nDue to delay from reporting and ingestion by the API"
  },
  {
    "objectID": "slides/day1-morning.html#versioning-is-important-for-forecasting",
    "href": "slides/day1-morning.html#versioning-is-important-for-forecasting",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Versioning is important for forecasting",
    "text": "Versioning is important for forecasting\n\n\nBacktesting requires using data that would have been available at the time\n\n\n\nNot later updates\n\n\n\nOverly optimistic"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#section",
    "href": "slides/appendix-tidyverse.html#section",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Background on {tidyverse}",
    "text": "Background on {tidyverse}\nMICOM Tooling Workshop 2025\n\nDavid Weber, Nat DeFries\nAdapted from slides by Alice Cima, Rachel Lobay, Daniel McDonald, Ryan Tibshirani, with huge thanks to Logan Brooks, Xueda Shen, and Dmitry Shemetov\n12 August 2025"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#down-with-spreadsheets-for-data-manipulation",
    "href": "slides/appendix-tidyverse.html#down-with-spreadsheets-for-data-manipulation",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Down with spreadsheets for data manipulation",
    "text": "Down with spreadsheets for data manipulation\n\nSpreadsheets make it difficult to rerun analyses consistently.\nUsing R (and {dplyr}) allows for:\n\nReproducibility\nEase of modification\n\nRecommendation: Avoid manual edits; instead, use code for transformations.\nLet’s see what we mean by this…"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#introduction-to-dplyr",
    "href": "slides/appendix-tidyverse.html#introduction-to-dplyr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Introduction to dplyr",
    "text": "Introduction to dplyr\n\ndplyr is a powerful package in R for data manipulation.\nIt is part of the tidyverse, which includes a collection of packages designed to work together… Here’s some of it’s greatest hits:\n\n\n  Source"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#introduction-to-dplyr-1",
    "href": "slides/appendix-tidyverse.html#introduction-to-dplyr-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Introduction to dplyr",
    "text": "Introduction to dplyr\n\nTo load dplyr you may simply load the tidyverse package:\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)  # Load tidyverse, which includes dplyr & tidyr"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#introduction-to-dplyr-2",
    "href": "slides/appendix-tidyverse.html#introduction-to-dplyr-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Introduction to dplyr",
    "text": "Introduction to dplyr\nOur focus will be on basic operations like selecting and filtering data.\n\n\nSource"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#downloading-jhu-csse-covid-19-case-data",
    "href": "slides/appendix-tidyverse.html#downloading-jhu-csse-covid-19-case-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Downloading JHU CSSE COVID-19 case data",
    "text": "Downloading JHU CSSE COVID-19 case data\n\nLet’s start with something familiar… Here’s a task for you:\nUse pub_covidcast() to download JHU CSSE COVID-19 confirmed case data (confirmed_incidence_num) for CA, NC, and NY from March 1, 2022 to March 31, 2022 as of January 1, 2024.\nTry this for yourself. Then click the dropdown on the next slide to check your work…"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#downloading-jhu-csse-covid-19-case-data-1",
    "href": "slides/appendix-tidyverse.html#downloading-jhu-csse-covid-19-case-data-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Downloading JHU CSSE COVID-19 case data",
    "text": "Downloading JHU CSSE COVID-19 case data\n\n\nCode\nlibrary(epidatr)\n\ncases_df &lt;- pub_covidcast(\n  source = \"jhu-csse\",\n  signals = \"confirmed_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"ca,nc,ny\",\n  time_values = epirange(20220301, 20220331),\n  as_of = as.Date(\"2024-01-01\")\n)\n\n\nNow we only really need a few columns here…\n\ncases_df &lt;- cases_df |&gt;\n  select(geo_value, time_value, raw_cases = value) # We'll talk more about this soon :)"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#ways-to-inspect-the-dataset",
    "href": "slides/appendix-tidyverse.html#ways-to-inspect-the-dataset",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ways to inspect the dataset",
    "text": "Ways to inspect the dataset\nUse head() to view the first six row of the data\n\nhead(cases_df)  # First 6 rows\n\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n4 ca        2022-03-02      7044\n5 nc        2022-03-02      2243\n6 ny        2022-03-02      1889\n\n\nand tail to view the last six\n\n\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-30      3785\n2 nc        2022-03-30      1067\n3 ny        2022-03-30      3127\n4 ca        2022-03-31      4533\n5 nc        2022-03-31      1075\n6 ny        2022-03-31      4763"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#ways-to-inspect-the-dataset-1",
    "href": "slides/appendix-tidyverse.html#ways-to-inspect-the-dataset-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ways to inspect the dataset",
    "text": "Ways to inspect the dataset\nNow, for our first foray into the tidyverse…\nUse glimpse() to get a compact overview of the dataset.\n\nglimpse(cases_df)\n\nRows: 93\nColumns: 3\n$ geo_value  &lt;chr&gt; \"ca\", \"nc\", \"ny\", \"ca\", \"nc\", \"ny\", \"ca\", \"nc\", \"ny\", \"ca\",…\n$ time_value &lt;date&gt; 2022-03-01, 2022-03-01, 2022-03-01, 2022-03-02, 2022-03-02…\n$ raw_cases  &lt;dbl&gt; 4310, 1231, 1487, 7044, 2243, 1889, 7509, 2377, 2390, 3586,…"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#creating-tibbles",
    "href": "slides/appendix-tidyverse.html#creating-tibbles",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Creating tibbles",
    "text": "Creating tibbles\n\nTibbles: Modern data frames with enhanced features.\nRows represent observations (or cases).\nColumns represent variables (or features).\nYou can create tibbles manually using the tibble() function.\n\n\ntibble(x = letters, y = 1:26)\n\n# A tibble: 26 × 2\n   x         y\n   &lt;chr&gt; &lt;int&gt;\n 1 a         1\n 2 b         2\n 3 c         3\n 4 d         4\n 5 e         5\n 6 f         6\n 7 g         7\n 8 h         8\n 9 i         9\n10 j        10\n# ℹ 16 more rows"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#selecting-columns-with-select",
    "href": "slides/appendix-tidyverse.html#selecting-columns-with-select",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nThe select() function is used to pick specific columns from your dataset.\n\nselect(cases_df, geo_value, time_value)  # Select the 'geo_value' and 'time_value' columns\n\n# A tibble: 93 × 2\n   geo_value time_value\n   &lt;chr&gt;     &lt;date&gt;    \n 1 ca        2022-03-01\n 2 nc        2022-03-01\n 3 ny        2022-03-01\n 4 ca        2022-03-02\n 5 nc        2022-03-02\n 6 ny        2022-03-02\n 7 ca        2022-03-03\n 8 nc        2022-03-03\n 9 ny        2022-03-03\n10 ca        2022-03-04\n# ℹ 83 more rows"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#selecting-columns-with-select-1",
    "href": "slides/appendix-tidyverse.html#selecting-columns-with-select-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nYou can exclude columns by prefixing the column names with a minus sign -.\n\nselect(cases_df, -raw_cases)  # Exclude the 'raw_cases' column from the dataset\n\n# A tibble: 93 × 2\n   geo_value time_value\n   &lt;chr&gt;     &lt;date&gt;    \n 1 ca        2022-03-01\n 2 nc        2022-03-01\n 3 ny        2022-03-01\n 4 ca        2022-03-02\n 5 nc        2022-03-02\n 6 ny        2022-03-02\n 7 ca        2022-03-03\n 8 nc        2022-03-03\n 9 ny        2022-03-03\n10 ca        2022-03-04\n# ℹ 83 more rows"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#extracting-columns-with-pull",
    "href": "slides/appendix-tidyverse.html#extracting-columns-with-pull",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Extracting columns with pull()",
    "text": "Extracting columns with pull()\n\npull(): Extract a column as a vector.\nLet’s try this with the cases column…\n\n\npull(cases_df, raw_cases)\n\n [1] 4310 1231 1487 7044 2243 1889 7509 2377 2390 3586 2646  350 1438    0 3372\n[16] 6465    0 2343 6690 4230 1033 3424  894 1025 4591 1833 1691 5359 1783 1747\n[31] 2713 1849 2229 1623    0 1396 5151    0 2202 4826 3130  982 1831  649 3128\n[46] 3706    0 2039 6143 2742 2356 4204 1740 2052 3256    0 2188 4659    0 2667\n[61] 5499 2508 1177 3004  819 1603 3943 1602  551 3550 1288 6596 1960 1224 3542\n[76] 1035    0    0 3384    0 5908 2811 2291 2286 1846  624 2394 3785 1067 3127\n[91] 4533 1075 4763"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#subsetting-rows-with-filter",
    "href": "slides/appendix-tidyverse.html#subsetting-rows-with-filter",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Subsetting rows with filter()",
    "text": "Subsetting rows with filter()\n\n  Artwork by @allison_horst"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#subsetting-rows-with-filter-1",
    "href": "slides/appendix-tidyverse.html#subsetting-rows-with-filter-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Subsetting rows with filter()",
    "text": "Subsetting rows with filter()\n\nThe filter() function allows you to subset rows that meet specific conditions.\nConditions regard column values, such as filtering for only NC or cases higher than some threshold.\nThis enables you to narrow down your dataset to focus on relevant data.\n\n\nfilter(cases_df, geo_value == \"nc\", raw_cases &gt; 500)  # Filter for NC with raw daily cases &gt; 500\n\n# A tibble: 22 × 3\n   geo_value time_value raw_cases\n   &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n 1 nc        2022-03-01      1231\n 2 nc        2022-03-02      2243\n 3 nc        2022-03-03      2377\n 4 nc        2022-03-04      2646\n 5 nc        2022-03-07      4230\n 6 nc        2022-03-08       894\n 7 nc        2022-03-09      1833\n 8 nc        2022-03-10      1783\n 9 nc        2022-03-11      1849\n10 nc        2022-03-14      3130\n# ℹ 12 more rows"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#combining-select-and-filter-functions",
    "href": "slides/appendix-tidyverse.html#combining-select-and-filter-functions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Combining select() and filter() functions",
    "text": "Combining select() and filter() functions\n\nYou can further combine select() and filter() to further refine the dataset.\nUse select() to choose columns and filter() to narrow down rows.\nThis helps in extracting the exact data needed for analysis.\n\n\nselect(filter(cases_df, geo_value == \"nc\", raw_cases &gt; 1000), time_value, raw_cases) |&gt;\n  head()\n\n# A tibble: 6 × 2\n  time_value raw_cases\n  &lt;date&gt;         &lt;dbl&gt;\n1 2022-03-01      1231\n2 2022-03-02      2243\n3 2022-03-03      2377\n4 2022-03-04      2646\n5 2022-03-07      4230\n6 2022-03-09      1833"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#using-the-pipe-operator",
    "href": "slides/appendix-tidyverse.html#using-the-pipe-operator",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Using the pipe operator |>",
    "text": "Using the pipe operator |&gt;\n\nThe pipe operator (|&gt;) makes code more readable by chaining multiple operations together.\nThe output of one function is automatically passed to the next function.\nThis allows you to perform multiple steps (e.g., filter() followed by select()) in a clear and concise manner.\n\n\n# This code reads more like poetry!\ncases_df |&gt;\n  filter(geo_value == \"nc\", raw_cases &gt; 1000) |&gt;\n  select(time_value, raw_cases) |&gt;\n  head()\n\n# A tibble: 6 × 2\n  time_value raw_cases\n  &lt;date&gt;         &lt;dbl&gt;\n1 2022-03-01      1231\n2 2022-03-02      2243\n3 2022-03-03      2377\n4 2022-03-04      2646\n5 2022-03-07      4230\n6 2022-03-09      1833"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#key-practices-in-dplyr",
    "href": "slides/appendix-tidyverse.html#key-practices-in-dplyr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Key practices in dplyr",
    "text": "Key practices in dplyr\n\nUse tibbles for easier data handling.\nUse select() and filter() for data manipulation.\nUse pull() to extract columns as vectors.\nUse head(), tail(), and glimpse() for quick data inspection.\nChain functions with |&gt; for cleaner code."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#grouping-data-with-group_by",
    "href": "slides/appendix-tidyverse.html#grouping-data-with-group_by",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Grouping data with group_by()",
    "text": "Grouping data with group_by()\n\nUse group_by() to group data by one or more columns.\nAllows performing operations on specific groups of data.\n\n\ncases_df |&gt;\n  group_by(geo_value) |&gt;\n  filter(raw_cases == max(raw_cases, na.rm = TRUE))\n\n# A tibble: 3 × 3\n# Groups:   geo_value [3]\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-03      7509\n2 nc        2022-03-07      4230\n3 ny        2022-03-24      6596"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#creating-new-columns-with-mutate",
    "href": "slides/appendix-tidyverse.html#creating-new-columns-with-mutate",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Creating new columns with mutate()",
    "text": "Creating new columns with mutate()\n\n  Artwork by @allison_horst"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#creating-new-columns-with-mutate-1",
    "href": "slides/appendix-tidyverse.html#creating-new-columns-with-mutate-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Creating new columns with mutate()",
    "text": "Creating new columns with mutate()\n\nmutate() is used to create new columns.\nPerform calculations using existing columns and assign to new columns.\n\n\nny_subset = cases_df |&gt;\n  filter(geo_value == \"ny\")\n\nny_subset |&gt;\n  mutate(cumulative_cases = cumsum(raw_cases)) |&gt;\n  head()\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases cumulative_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1 ny        2022-03-01      1487             1487\n2 ny        2022-03-02      1889             3376\n3 ny        2022-03-03      2390             5766\n4 ny        2022-03-04       350             6116\n5 ny        2022-03-05      3372             9488\n6 ny        2022-03-06      2343            11831"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#creating-new-columns-with-mutate-2",
    "href": "slides/appendix-tidyverse.html#creating-new-columns-with-mutate-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Creating new columns with mutate()",
    "text": "Creating new columns with mutate()\n\nmutate() can create multiple new columns in one step.\nLogical comparisons (e.g., over_5000 = raw_cases &gt; 5000) can be used within mutate().\n\n\nny_subset |&gt;\n  mutate(over_5000 = raw_cases &gt; 5000,\n         cumulative_cases = cumsum(raw_cases)) |&gt;\n  head()\n\n# A tibble: 6 × 5\n  geo_value time_value raw_cases over_5000 cumulative_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n1 ny        2022-03-01      1487 FALSE                 1487\n2 ny        2022-03-02      1889 FALSE                 3376\n3 ny        2022-03-03      2390 FALSE                 5766\n4 ny        2022-03-04       350 FALSE                 6116\n5 ny        2022-03-05      3372 FALSE                 9488\n6 ny        2022-03-06      2343 FALSE                11831"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#combining-group_by-and-mutate",
    "href": "slides/appendix-tidyverse.html#combining-group_by-and-mutate",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Combining group_by() and mutate()",
    "text": "Combining group_by() and mutate()\n\nFirst, group data using group_by().\nThen, use mutate to perform the calculations for each group.\nFinally, use arrange to display the output by geo_value.\n\n\ncases_df |&gt;\n  group_by(geo_value) |&gt;\n  mutate(cumulative_cases = cumsum(raw_cases)) |&gt;\n  arrange(geo_value) |&gt;\n  head()\n\n# A tibble: 6 × 4\n# Groups:   geo_value [1]\n  geo_value time_value raw_cases cumulative_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1 ca        2022-03-01      4310             4310\n2 ca        2022-03-02      7044            11354\n3 ca        2022-03-03      7509            18863\n4 ca        2022-03-04      3586            22449\n5 ca        2022-03-05      1438            23887\n6 ca        2022-03-06      6465            30352"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#conditional-calculations-with-if_else",
    "href": "slides/appendix-tidyverse.html#conditional-calculations-with-if_else",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Conditional calculations with if_else()",
    "text": "Conditional calculations with if_else()\n\nif_else() allows conditional logic within mutate().\nPerform different operations depending on conditions, like “high” or “low.”\n\n\nt &lt;- 5000\n\ncases_df |&gt;\n  mutate(high_low_cases = if_else(raw_cases &gt; t, \"high\", \"low\")) |&gt;\n  head()\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases high_low_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt; &lt;chr&gt;         \n1 ca        2022-03-01      4310 low           \n2 nc        2022-03-01      1231 low           \n3 ny        2022-03-01      1487 low           \n4 ca        2022-03-02      7044 high          \n5 nc        2022-03-02      2243 low           \n6 ny        2022-03-02      1889 low"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#summarizing-data-with-summarise",
    "href": "slides/appendix-tidyverse.html#summarizing-data-with-summarise",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Summarizing data with summarise()",
    "text": "Summarizing data with summarise()\n\nsummarise() reduces data to summary statistics (e.g., mean, median).\nTypically used after group_by() to summarize each group.\n\n\ncases_df |&gt;\n  group_by(geo_value) |&gt;\n  summarise(median_cases = median(raw_cases))\n\n# A tibble: 3 × 2\n  geo_value median_cases\n  &lt;chr&gt;            &lt;dbl&gt;\n1 ca                3785\n2 nc                1224\n3 ny                2188"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#using-count-to-aggregate-data",
    "href": "slides/appendix-tidyverse.html#using-count-to-aggregate-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Using count() to aggregate data",
    "text": "Using count() to aggregate data\ncount() is a shortcut for grouping and summarizing the data.\nFor example, if we want to get the total number of complete rows for each state, then\n\ncases_count &lt;- cases_df |&gt;\n  drop_na() |&gt; # Removes rows where any value is missing (from tidyr)\n  group_by(geo_value) |&gt;\n  summarize(count = n())\n\n is equivalent to\n\ncases_count &lt;- cases_df |&gt;\n  drop_na() |&gt;\n  count(geo_value)\n\ncases_count # Let's see what the counts are.\n\n# A tibble: 3 × 2\n  geo_value     n\n  &lt;chr&gt;     &lt;int&gt;\n1 ca           31\n2 nc           31\n3 ny           31"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#key-practices-in-dplyr-round-2",
    "href": "slides/appendix-tidyverse.html#key-practices-in-dplyr-round-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Key practices in dplyr: Round 2",
    "text": "Key practices in dplyr: Round 2\n\nUse group_by() to group data by one or more variables before applying functions.\nUse mutate to create new columns or modify existing ones by applying functions to existing data.\nUse summarise to reduce data to summary statistics (e.g., mean, median).\ncount() is a convenient shortcut for counting rows by group without needing group_by() and summarise()."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#tidy-data-and-tolstoy",
    "href": "slides/appendix-tidyverse.html#tidy-data-and-tolstoy",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Tidy data and Tolstoy",
    "text": "Tidy data and Tolstoy\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.” — Leo Tolstoy\n\n\nTidy datasets are like happy families: consistent, standardized, and easy to work with.\nMessy datasets are like unhappy families: each one messy in its own unique way. In this section:\nWe’ll define what makes data tidy and how to transform between the tidy and messy formats."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#tidy-data-and-tolstoy-1",
    "href": "slides/appendix-tidyverse.html#tidy-data-and-tolstoy-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Tidy data and Tolstoy",
    "text": "Tidy data and Tolstoy\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#what-is-tidy-data",
    "href": "slides/appendix-tidyverse.html#what-is-tidy-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "What is tidy data?",
    "text": "What is tidy data?\n\nTidy data follows a consistent structure: each row represents one observation, and each column represents one variable.\ncases_df is one classic example of tidy data.\n\n\n\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n4 ca        2022-03-02      7044\n5 nc        2022-03-02      2243\n6 ny        2022-03-02      1889\n\n\n\nTo convert between tidy and messy data, we can use the tidyr package in the tidyverse."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#pivot_wider-and-pivot_longer",
    "href": "slides/appendix-tidyverse.html#pivot_wider-and-pivot_longer",
    "title": "MICOM EpiData Workshop 2025",
    "section": "pivot_wider() and pivot_longer()",
    "text": "pivot_wider() and pivot_longer()\n\n  Artwork by @allison_horst"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#making-data-wider-with-pivot_wider",
    "href": "slides/appendix-tidyverse.html#making-data-wider-with-pivot_wider",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Making data wider with pivot_wider()",
    "text": "Making data wider with pivot_wider()\n\nTo convert data from long format to wide/messy format usepivot_wider().\nFor example, let’s try creating a column for each time value in cases_df:\n\n\n\nmessy_cases_df &lt;- cases_df |&gt;\n  pivot_wider(\n    names_from = time_value,   # Create new columns for each unique date\n    values_from = raw_cases    # Fill those columns with the raw_case values\n  )\n\n# View the result\nmessy_cases_df\n\n# A tibble: 3 × 32\n  geo_value `2022-03-01` `2022-03-02` `2022-03-03` `2022-03-04` `2022-03-05`\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 ca                4310         7044         7509         3586         1438\n2 nc                1231         2243         2377         2646            0\n3 ny                1487         1889         2390          350         3372\n# ℹ 26 more variables: `2022-03-06` &lt;dbl&gt;, `2022-03-07` &lt;dbl&gt;,\n#   `2022-03-08` &lt;dbl&gt;, `2022-03-09` &lt;dbl&gt;, `2022-03-10` &lt;dbl&gt;,\n#   `2022-03-11` &lt;dbl&gt;, `2022-03-12` &lt;dbl&gt;, `2022-03-13` &lt;dbl&gt;,\n#   `2022-03-14` &lt;dbl&gt;, `2022-03-15` &lt;dbl&gt;, `2022-03-16` &lt;dbl&gt;,\n#   `2022-03-17` &lt;dbl&gt;, `2022-03-18` &lt;dbl&gt;, `2022-03-19` &lt;dbl&gt;,\n#   `2022-03-20` &lt;dbl&gt;, `2022-03-21` &lt;dbl&gt;, `2022-03-22` &lt;dbl&gt;,\n#   `2022-03-23` &lt;dbl&gt;, `2022-03-24` &lt;dbl&gt;, `2022-03-25` &lt;dbl&gt;, …"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#tidying-messy-data-with-pivot_longer",
    "href": "slides/appendix-tidyverse.html#tidying-messy-data-with-pivot_longer",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Tidying messy data with pivot_longer()",
    "text": "Tidying messy data with pivot_longer()\n\nUse pivot_longer() to convert data from wide format (multiple columns for the same variable) to long format (one column per variable).\nLet’s try turning messy_cases_df back into the original tidy cases_df!\n\n\ntidy_cases_df &lt;- messy_cases_df |&gt;\n  pivot_longer(\n    cols = -geo_value,          # Keep the 'geo_value' column as it is\n    names_to = \"time_value\",    # Create a new 'time_value' column from the column names\n    values_to = \"raw_cases\"     # Values from the wide columns should go into 'raw_cases'\n  )\n\n# View the result\nhead(tidy_cases_df, n = 3) # Notice the class of time_value here\n\n# A tibble: 3 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 ca        2022-03-02      7044\n3 ca        2022-03-03      7509"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#tidying-messy-data-with-pivot_longer-1",
    "href": "slides/appendix-tidyverse.html#tidying-messy-data-with-pivot_longer-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Tidying messy data with pivot_longer()",
    "text": "Tidying messy data with pivot_longer()\n\nWhen we used pivot_longer(), the time_value column is converted to a character class because the column names are treated as strings.\nSo, to truly get the original cases_df we need to convert time_value back to the Date class.\nThen, we can use identical() to check if the two data frames are exactly the same.\n\n\ntidy_cases_df = tidy_cases_df |&gt; mutate(time_value = as.Date(time_value))\n\nidentical(tidy_cases_df |&gt; arrange(time_value), cases_df)\n\n[1] TRUE\n\n\nGreat. That was a success!"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#missing-data",
    "href": "slides/appendix-tidyverse.html#missing-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Missing data",
    "text": "Missing data\n\nSometimes you may have missing data in your time series.\nCan be due to actual missing data, or it can be due to the fact that the data is only reported on certain days.\nLet’s create a dataset with missing data & consider each of those cases:\n\n\nca_missing &lt;- cases_df |&gt;\n  filter(geo_value == \"ca\") |&gt;\n  slice(1:2, 4:6) # Subset rows 1 to 2 and 4 to 6; ie. omit 2022-03-03\n\nca_missing\n\n# A tibble: 5 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 ca        2022-03-02      7044\n3 ca        2022-03-04      3586\n4 ca        2022-03-05      1438\n5 ca        2022-03-06      6465"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#complete-and-fill-to-handle-missing-data",
    "href": "slides/appendix-tidyverse.html#complete-and-fill-to-handle-missing-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "complete() and fill() to handle missing data",
    "text": "complete() and fill() to handle missing data\nA simple workflow to handle missing data relies on one or both of these functions:\n\ncomplete(): Adds missing rows for combinations of specified variables.\nfill(): Fills missing values in columns, typically from previous or next available values (default is LOCF)."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#data-only-reported-on-certain-days",
    "href": "slides/appendix-tidyverse.html#data-only-reported-on-certain-days",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Data only reported on certain days",
    "text": "Data only reported on certain days\n\nIf the data is only reported on certain days, it is often useful to fill in the missing data with explicit zeros.\ncomplete() is enough to handle this:\n\n\n# First, use complete() to add missing time_value (2022-03-03)\nca_complete &lt;- ca_missing |&gt;\n  complete(geo_value, time_value = seq(min(time_value), max(time_value), by = \"day\"),\n           fill = list(raw_cases = 0))\nca_complete\n\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 ca        2022-03-02      7044\n3 ca        2022-03-03         0\n4 ca        2022-03-04      3586\n5 ca        2022-03-05      1438\n6 ca        2022-03-06      6465"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#data-is-genuinely-missing",
    "href": "slides/appendix-tidyverse.html#data-is-genuinely-missing",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Data is genuinely missing",
    "text": "Data is genuinely missing\n\nIf the data is truly missing, then there are multiple options (ex. omission, single imputation, multiple imputation).\nA common single imputation method used to handle missing data in time series or longitudinal datasets is LOCF.\nWe can easily perform LOCF using complete() followed by fill().\nStart with complete():\n\n\n# First, use complete() to add missing time_value (2022-03-03)\nca_complete &lt;- ca_missing |&gt;\n  complete(geo_value, time_value = seq(min(time_value), max(time_value), by = \"day\"))\nhead(ca_complete, n = 4) # notice no fill with 0s this time, NA by default\n\n# A tibble: 4 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 ca        2022-03-02      7044\n3 ca        2022-03-03        NA\n4 ca        2022-03-04      3586"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#data-is-genuinely-missing-1",
    "href": "slides/appendix-tidyverse.html#data-is-genuinely-missing-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Data is genuinely missing",
    "text": "Data is genuinely missing\nThen, use fill() to fill the counts using LOCF (default):\n\nca_complete |&gt;\n  fill(raw_cases)\n\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 ca        2022-03-02      7044\n3 ca        2022-03-03      7044\n4 ca        2022-03-04      3586\n5 ca        2022-03-05      1438\n6 ca        2022-03-06      6465"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#introduction-to-joins-in-dplyr",
    "href": "slides/appendix-tidyverse.html#introduction-to-joins-in-dplyr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Introduction to joins in dplyr",
    "text": "Introduction to joins in dplyr\n\nJoining datasets is a powerful tool for combining info. from multiple sources.\nIn R, dplyr provides several functions to perform different types of joins.\nWe’ll demonstrate joining a subset of cases_df (our case counts dataset) with state_census.\nMotivation: We can scale the case counts by population to make them comparable across regions of different sizes."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#subset-cases_df",
    "href": "slides/appendix-tidyverse.html#subset-cases_df",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Subset cases_df",
    "text": "Subset cases_df\nTo simplify things, let’s use filter() to only grab one date of cases_df:\n\ncases_df_sub = cases_df |&gt; filter(time_value == \"2022-03-01\")\ncases_df_sub\n\n# A tibble: 3 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n\n\nThough note that what we’re going to do can be applied to the entirety of cases_df."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#load-state-census-data",
    "href": "slides/appendix-tidyverse.html#load-state-census-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Load state census data",
    "text": "Load state census data\nThe state_census dataset from epidatasets contains state populations from the 2019 census.\n\n# State census dataset from epidatasets\nlibrary(epidatasets)\nstate_census = state_census |&gt; select(abbr, pop) |&gt; filter(abbr != \"us\")\n\nstate_census |&gt; head()\n\n# A tibble: 6 × 2\n  abbr       pop\n  &lt;chr&gt;    &lt;dbl&gt;\n1 al     4903185\n2 ak      731545\n3 az     7278717\n4 ar     3017804\n5 ca    39512223\n6 co     5758736\n\n\nNotice that this includes many states that are not in cases_df_sub."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#left-join-keep-all-rows-from-the-first-dataset",
    "href": "slides/appendix-tidyverse.html#left-join-keep-all-rows-from-the-first-dataset",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Left Join: Keep all rows from the first dataset",
    "text": "Left Join: Keep all rows from the first dataset\n\nA left join keeps all rows from the first dataset (cases_df_sub), and adds matching data from the second dataset (state_census).\nSo all rows from the first dataset (cases_df_sub) will be preserved.\nThe datasets are joined by matching the geo_value column, specified by the by argument.\n\n\n# Left join: combining March 1, 2022 state case data with the census data\ncases_left_join &lt;- cases_df_sub |&gt;\n  left_join(state_census, join_by(geo_value == abbr))\n\ncases_left_join\n\n# A tibble: 3 × 4\n  geo_value time_value raw_cases      pop\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#right-join-keep-all-rows-from-the-second-dataset",
    "href": "slides/appendix-tidyverse.html#right-join-keep-all-rows-from-the-second-dataset",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Right Join: Keep all rows from the second dataset",
    "text": "Right Join: Keep all rows from the second dataset\n\nA right join keeps all rows from the second dataset (state_census), and adds matching data from the first dataset (cases_df_sub).\nIf a row in the second dataset doesn’t have a match in the first, then the columns from the first will be filled with NA.\nFor example, can see this for the al row from state_census…\n\n\n# Right join: keep all rows from state_census\ncases_right_join &lt;- cases_df_sub |&gt;\n  right_join(state_census, join_by(geo_value == abbr))\n\nhead(cases_right_join)\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases      pop\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n4 al        NA                NA  4903185\n5 ak        NA                NA   731545\n6 az        NA                NA  7278717"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#inner-join-only-keeping-matching-rows",
    "href": "slides/appendix-tidyverse.html#inner-join-only-keeping-matching-rows",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Inner Join: Only keeping matching rows",
    "text": "Inner Join: Only keeping matching rows\n\nAn inner join will only keep rows where there is a match in both datasets.\nSo, if a state in state_census does not have a corresponding entry in cases_df_sub, then that row will be excluded.\n\n\n# Inner join: only matching rows are kept\ncases_inner_join &lt;- cases_df_sub |&gt;\n  inner_join(state_census, join_by(geo_value == abbr))\n\ncases_inner_join\n\n# A tibble: 3 × 4\n  geo_value time_value raw_cases      pop\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#full-join-keeping-all-rows-from-both-datasets",
    "href": "slides/appendix-tidyverse.html#full-join-keeping-all-rows-from-both-datasets",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Full Join: Keeping all rows from both datasets",
    "text": "Full Join: Keeping all rows from both datasets\n\nA full join will keep all rows from both datasets.\nIf a state in either dataset has no match in the other, the missing values will be filled with NA.\n\n\n# Full join: keep all rows from both datasets\ncases_full_join &lt;- cases_df_sub |&gt;\n  full_join(state_census, join_by(geo_value == abbr))\n\nhead(cases_full_join)\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases      pop\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n4 al        NA                NA  4903185\n5 ak        NA                NA   731545\n6 az        NA                NA  7278717"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#pictorial-summary-of-the-four-join-functions",
    "href": "slides/appendix-tidyverse.html#pictorial-summary-of-the-four-join-functions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Pictorial summary of the four join functions",
    "text": "Pictorial summary of the four join functions\n\n\n\nSource"
  },
  {
    "objectID": "slides/appendix-tidyverse.html#final-thoughts-on-joins",
    "href": "slides/appendix-tidyverse.html#final-thoughts-on-joins",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Final thoughts on joins",
    "text": "Final thoughts on joins\n\nJoins are an essential part of data wrangling in R.\nThe choice of join depends on the analysis you need to perform:\n\nUse left joins when you want to keep all data from the first dataset.\nUse right joins when you want to keep all data from the second dataset.\nUse inner joins when you’re only interested in matching rows.\nUse full joins when you want to preserve all information from both datasets."
  },
  {
    "objectID": "slides/appendix-tidyverse.html#three-review-questions",
    "href": "slides/appendix-tidyverse.html#three-review-questions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Three review questions",
    "text": "Three review questions\nQ1): What can we use to fill in the missing time_value for the states in cases_full_join?\n\n\nCode\ncases_full_join |&gt;\n     fill(time_value)\n\n\nQ2): Now, what join function should you use if your goal is to scale the cases by population in cases_df?\n\n\nCode\n# Either left_join\ncases_left_join &lt;- cases_df |&gt;\n  left_join(state_census, join_by(geo_value == abbr))\n\ncases_left_join\ncases_df = cases_left_join\n\n# Or inner_join\ncases_inner_join &lt;- cases_df |&gt;\n  inner_join(state_census, join_by(geo_value == abbr))\n\ncases_inner_join\n\n\nQ3): Finally, please create a new column in cases_df where you scale the cases by population and multiply by 1e5 to get cases / 100k.\n\n\nCode\ncases_df &lt;- cases_df |&gt;\n  mutate(scaled_cases = raw_cases / pop * 1e5) # cases / 100K\nhead(cases_df)"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Schedule",
    "text": "Schedule\nIn this short workshop, we plan to demonstrate how to use R to load, process, inspect, and forecast aggregate epi surveillance data. We will be presenting a few case studies to motivate the entire pipeline from signal discovery to the production of nowcasts and forecasts.\n\nUnderstanding Data\nNowcasting\nTime Series\nForecasting\nAppendix: Background on tidyverse"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Instructors",
    "text": "Instructors\n\nDavid Weber\nNat DeFries\nLogan C. Brooks"
  },
  {
    "objectID": "slides/day1-afternoon.html#section",
    "href": "slides/day1-afternoon.html#section",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Data Cleaning, Versioning, Nowcasting With {epiprocess}",
    "text": "Data Cleaning, Versioning, Nowcasting With {epiprocess}\nMICOM Tooling Workshop 2025\n\nDavid Weber, Nat DeFries\nAdapted from slides by Alice Cima, Rachel Lobay, Daniel McDonald, Ryan Tibshirani, with huge thanks to Logan Brooks, Xueda Shen, and Dmitry Shemetov\n12 August 2025"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi.-data-processing-with-epiprocess",
    "href": "slides/day1-afternoon.html#epi.-data-processing-with-epiprocess",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Epi. data processing with epiprocess",
    "text": "Epi. data processing with epiprocess\n\nepiprocess is a package that offers functionality to pre-process epidemiological data.\nYou can work with an epi_df like you can with a tibble by using dplyr verbs."
  },
  {
    "objectID": "slides/day1-afternoon.html#epi.-data-processing-with-epiprocess-1",
    "href": "slides/day1-afternoon.html#epi.-data-processing-with-epiprocess-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Epi. data processing with epiprocess",
    "text": "Epi. data processing with epiprocess\n\nFor example, on cases_df, we can use epi_slide_mean() to calculate trailing 14 day averages of cases:\n\n\n3\n\n[1] 3\n\ncase_rates_df &lt;- case_rates_df |&gt;\n  as_epi_df(as_of = as.Date(\"2024-01-01\")) |&gt;\n  group_by(geo_value) |&gt;\n  epi_slide_mean(\n    scaled_cases,\n    .window_size = 14,\n    na.rm = TRUE\n  ) |&gt;\n  rename(smoothed_scaled_cases = scaled_cases_14dav)"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi.-data-processing-with-epiprocess-2",
    "href": "slides/day1-afternoon.html#epi.-data-processing-with-epiprocess-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Epi. data processing with epiprocess",
    "text": "Epi. data processing with epiprocess\nIt is easy to produce an autoplot of the smoothed confirmed daily cases for each geo_value:\n\n3\n\n[1] 3\n\ncase_rates_df |&gt;\n  autoplot(smoothed_scaled_cases)"
  },
  {
    "objectID": "slides/day1-afternoon.html#the-epiverse-ecosystem",
    "href": "slides/day1-afternoon.html#the-epiverse-ecosystem",
    "title": "MICOM EpiData Workshop 2025",
    "section": "The epiverse ecosystem",
    "text": "The epiverse ecosystem\nInterworking, community-driven, packages for epi tracking & forecasting."
  },
  {
    "objectID": "slides/day1-afternoon.html#what-is-panel-data",
    "href": "slides/day1-afternoon.html#what-is-panel-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "What is panel data?",
    "text": "What is panel data?\n\nRecall that panel data, or longitudinal data, contain cross-sectional measurements of subjects over time.\nBuilt-in example: covid_case_death_rates dataset, which is a snapshot as of May 31, 2022 that contains daily state-wise measures of case_rate and death_rate for COVID-19 over 2021:\n\n\nedf &lt;- covid_case_death_rates\n# Only consider the 50 US states (no territories)\nedf &lt;- edf |&gt; filter(geo_value %in% tolower(state.abb))\nhead(edf |&gt; as_tibble())\n\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 az        2020-12-31      76.8      1.10 \n5 ca        2020-12-31      95.9      0.755\n6 co        2020-12-31      37.8      0.376\n\n\n\nHow do we store & work with such snapshots in the epiverse software ecosystem?"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_df-snapshot-of-a-dataset",
    "href": "slides/day1-afternoon.html#epi_df-snapshot-of-a-dataset",
    "title": "MICOM EpiData Workshop 2025",
    "section": "epi_df: Snapshot of a dataset",
    "text": "epi_df: Snapshot of a dataset\n\nYou can convert panel data into an epi_df with the required geo_value and time_value columns\n\nTherefore, an epi_df is…\n\na tibble that requires columns geo_value and time_value.\narbitrary additional columns containing measured values\nadditional keys to index (age_group, ethnicity, etc.)\n\n\n\n\n\n\n\nepi_df\n\n\nRepresents a snapshot that contains the most up-to-date values of the signal variables, as of a given time."
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_df-snapshot-of-a-dataset-1",
    "href": "slides/day1-afternoon.html#epi_df-snapshot-of-a-dataset-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "epi_df: Snapshot of a dataset",
    "text": "epi_df: Snapshot of a dataset\n\nConsider the same dataset we just encountered on JHU daily COVID-19 cases and deaths rates from all states as of May 31, 2022.\nWe can see that it meets the criteria epi_df (has geo_value and time_value columns) and that it contains additional metadata (i.e. geo_type, time_type, as_of, and other_keys).\n\n\n\n\nedf |&gt; head()\n\nAn `epi_df` object, 6 x 4 with metadata:\n* geo_type  = state\n* time_type = day\n* as_of     = 2023-03-10\n\n# A tibble: 6 × 4\n  geo_value time_value case_rate death_rate\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 ak        2020-12-31      35.9      0.158\n2 al        2020-12-31      65.1      0.438\n3 ar        2020-12-31      66.0      1.27 \n4 az        2020-12-31      76.8      1.10 \n5 ca        2020-12-31      95.9      0.755\n6 co        2020-12-31      37.8      0.376\n\n\n\n\nattr(edf, \"metadata\")\n\n$geo_type\n[1] \"state\"\n\n$time_type\n[1] \"day\"\n\n$as_of\n[1] \"2023-03-10\"\n\n$other_keys\ncharacter(0)"
  },
  {
    "objectID": "slides/day1-afternoon.html#examples-of-preprocessing",
    "href": "slides/day1-afternoon.html#examples-of-preprocessing",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Examples of preprocessing",
    "text": "Examples of preprocessing"
  },
  {
    "objectID": "slides/day1-afternoon.html#features---correlations-at-different-lags",
    "href": "slides/day1-afternoon.html#features---correlations-at-different-lags",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features - Correlations at different lags",
    "text": "Features - Correlations at different lags\n\nThe below plot addresses the question: “For each state, are case and death rates linearly associated across all days?”\nTo explore lagged correlations and how case rates associate with future death rates, we can use the dt1 parameter in epi_cor() to shift case rates by a specified number of days.\n\n\nedf &lt;- covid_case_death_rates %&gt;% filter(geo_value %in% tolower(state.abb))\ncor0 &lt;- epi_cor(edf, case_rate, death_rate, cor_by = geo_value)\ncor14 &lt;- epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -14)\n\n\n\nWe can see that, in general, lagging the case rates back by 14 days improves the correlations."
  },
  {
    "objectID": "slides/day1-afternoon.html#features---systematic-lag-analysis",
    "href": "slides/day1-afternoon.html#features---systematic-lag-analysis",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features - Systematic lag analysis",
    "text": "Features - Systematic lag analysis\nThe analysis helps identify the lag at which case rates from the past have the strongest correlation with future death rates.\n\nlags &lt;- 0:95\n\nz &lt;- map_dfr(lags, function(lag) {\n  epi_cor(edf, case_rate, death_rate, cor_by = geo_value, dt1 = -lag) %&gt;%\n    mutate(lag = .env$lag)\n})\n\nz_summary &lt;- z %&gt;%\n  group_by(lag) %&gt;%\n  summarize(mean = mean(cor, na.rm = TRUE))\n\n# Find the lag with the maximum correlation\nmax_lag &lt;- z_summary$lag[which.max(z_summary$mean)]\nmax_lag\n\n[1] 23"
  },
  {
    "objectID": "slides/day1-afternoon.html#features---compute-growth-rates",
    "href": "slides/day1-afternoon.html#features---compute-growth-rates",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features - Compute growth rates",
    "text": "Features - Compute growth rates\n\nGrowth rate measures the relative change in a signal over time. Indicating how quickly a quantity (like case rates) is increasing or decreasing.\nGetting the color like this is a bit messy\nAs expected, the peak growth rates for both states occurred during the January 2022 Omicron wave, reflecting the sharp rise in cases over that period.\n\n\nWe can compute time-varying growth rates for two states:\n\n\nedfg &lt;- filter(edf, geo_value %in% c(\"ut\", \"ca\")) |&gt;\n  group_by(geo_value) |&gt;\n  mutate(gr_cases = growth_rate(case_rate, time_value, method = \"smooth_spline\")) |&gt;\n  ungroup()\n\n\n\nTry on your favorite dataset so far, and plot using:\n\n\nedfg %&gt;% autoplot(case_rate, gr_cases)\n\n\nAlso see rtestim for estimating RT values instead of the growth rate. https://dajmcdon.github.io/rtestim/ Similar usage"
  },
  {
    "objectID": "slides/day1-afternoon.html#features---outlier-detection",
    "href": "slides/day1-afternoon.html#features---outlier-detection",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features - Outlier detection",
    "text": "Features - Outlier detection\n\nThe detect_outlr() function offers multiple outlier detection methods on a signal.\nThe simplest is detect_outlr_rm(), which works by calculating an outlier threshold using the rolling median and the rolling Interquartile Range (IQR) for each time point:\n\nThreshold = Rolling Median ± (Detection Multiplier × Rolling IQR)\n\nNote that the default number of time steps to use in the rolling window by default is 21 and is centrally aligned.\nThe detection multiplier default is 2 and controls how far away a data point must be from the median to be considered an outlier.\n\n\nedfo &lt;- filter(edf, geo_value %in% c(\"ca\", \"ut\")) |&gt;\n  select(geo_value, time_value, case_rate) |&gt;\n  as_epi_df() |&gt;\n  group_by(geo_value) |&gt;\n  mutate(outlier_info = detect_outlr_rm(\n    x = time_value, y = case_rate\n  )) |&gt;\n  ungroup()"
  },
  {
    "objectID": "slides/day1-afternoon.html#features---outlier-detection-1",
    "href": "slides/day1-afternoon.html#features---outlier-detection-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features - Outlier detection",
    "text": "Features - Outlier detection\n\n\nSame idea as the previous slide; you can plot using autoplot"
  },
  {
    "objectID": "slides/day1-afternoon.html#features-sliding-a-computation-on-an-epi_df",
    "href": "slides/day1-afternoon.html#features-sliding-a-computation-on-an-epi_df",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features – sliding a computation on an epi_df",
    "text": "Features – sliding a computation on an epi_df\n\nThe simplest way to use epi_slide is tidy evaluation.\nFor a grouped epi_df, epi_slide() applies the computation to groups separately.\n\n\ncases_7dav &lt;- epi_slide(\n  .x = cases_edf,\n  cases_7dav = mean(raw_cases, na.rm = TRUE),\n  .window_size = 7,\n  .align = \"right\"\n)\n\n\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases cases_7dav\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 ca        2022-03-01      4310       4310\n2 ca        2022-03-02      7044       5677\n3 nc        2022-03-01      1231       1231\n4 nc        2022-03-02      2243       1737\n5 ny        2022-03-01      1487       1487\n6 ny        2022-03-02      1889       1688"
  },
  {
    "objectID": "slides/day1-afternoon.html#features-sliding-a-computation-on-an-epi_df-1",
    "href": "slides/day1-afternoon.html#features-sliding-a-computation-on-an-epi_df-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features – sliding a computation on an epi_df",
    "text": "Features – sliding a computation on an epi_df\n\nThe simplest way to use epi_slide is tidy evaluation.\nFor a grouped epi_df, epi_slide() applies the computation to groups separately.\n\n\ncases_7dav &lt;- epi_slide(\n  .x = cases_edf,\n  cases_7dav = mean(raw_cases, na.rm = TRUE),\n  .window_size = 7,\n  .align = \"right\"\n)\n\n\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases cases_7dav\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 ca        2022-03-01      4310       4310\n2 ca        2022-03-02      7044       5677\n3 nc        2022-03-01      1231       1231\n4 nc        2022-03-02      2243       1737\n5 ny        2022-03-01      1487       1487\n6 ny        2022-03-02      1889       1688"
  },
  {
    "objectID": "slides/day1-afternoon.html#features-sliding-a-computation-on-an-epi_df-2",
    "href": "slides/day1-afternoon.html#features-sliding-a-computation-on-an-epi_df-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features – sliding a computation on an epi_df",
    "text": "Features – sliding a computation on an epi_df\nepi_slide also accepts custom functions of a certain form.\n\ncustom_function &lt;- function(x, g, t, ...) {\n\n  # Function body\n\n}\n\n\nx: the data frame with all the columns with original object except groupping vars.\ng: the one-row tibble with values of gropping vars of the given group.\nt: the .ref_time_value of the current window.\n...: additional arguments."
  },
  {
    "objectID": "slides/day1-afternoon.html#features-sliding-a-computation-on-an-epi_df-3",
    "href": "slides/day1-afternoon.html#features-sliding-a-computation-on-an-epi_df-3",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features – sliding a computation on an epi_df",
    "text": "Features – sliding a computation on an epi_df\n\nmean_by_hand &lt;- function(x, g, t, ...) {\n  data.frame(cases_7dav = mean(x$raw_cases, na.rm = TRUE))\n}\n\ncases_mean_custom_f = epi_slide(\n    .x = cases_edf,\n    .f = mean_by_hand,\n    .window_size = 7,\n    .align = \"right\"\n)\n\n\n\n# A tibble: 3 × 4\n  geo_value time_value raw_cases cases_7dav\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 ca        2022-03-01      4310       4310\n2 nc        2022-03-01      1231       1231\n3 ny        2022-03-01      1487       1487"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs",
    "href": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs",
    "title": "MICOM EpiData Workshop 2025",
    "section": "epi_archive: Collection of epi_dfs",
    "text": "epi_archive: Collection of epi_dfs\n\nfull version history of a data set\nacts like a bunch of epi_dfs — but stored compactly\nallows similar functionality as epi_df but using only data that would have been available at the time\n\n\n\n\n\n\n\nRevisions\n\n\nEpidemiology data gets revised frequently.\n\nWe may want to use the data as it looked in the past.\nor we may want to examine the history of revisions."
  },
  {
    "objectID": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs-1",
    "href": "slides/day1-afternoon.html#epi_archive-collection-of-epi_dfs-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "epi_archive: Collection of epi_dfs",
    "text": "epi_archive: Collection of epi_dfs\nSubset of daily COVID-19 doctor visits (Optum) and cases (JHU CSSE) from all U.S. states in archive format:\n\narchive_cases_dv_subset_all_states \n\n→ An `epi_archive` object, with metadata:\nℹ Min/max time values: 2020-06-01 / 2021-11-30\nℹ First/last version with update: 2020-06-02 / 2021-12-01\nℹ Versions end: 2021-12-01\nℹ A preview of the table (1514489 rows x 5 columns):\nKey: &lt;geo_value, time_value, version&gt;\n         geo_value time_value    version percent_cli case_rate_7d_av\n            &lt;char&gt;     &lt;Date&gt;     &lt;Date&gt;       &lt;num&gt;           &lt;num&gt;\n      1:        ak 2020-06-01 2020-06-02          NA        1.145652\n      2:        ak 2020-06-01 2020-06-06    0.136815        1.145652\n      3:        ak 2020-06-01 2020-06-08    0.136249        1.145652\n      4:        ak 2020-06-01 2020-06-09    0.106744        1.145652\n      5:        ak 2020-06-01 2020-06-10    0.106676        1.145652\n     ---                                                            \n1514485:        wy 2021-11-26 2021-11-29    3.739819       23.207343\n1514486:        wy 2021-11-27 2021-11-28          NA       23.207343\n1514487:        wy 2021-11-28 2021-11-29          NA       23.207343\n1514488:        wy 2021-11-29 2021-11-30          NA       25.071781\n1514489:        wy 2021-11-30 2021-12-01          NA       25.464294"
  },
  {
    "objectID": "slides/day1-afternoon.html#features-sliding-computation-over-epi_archives",
    "href": "slides/day1-afternoon.html#features-sliding-computation-over-epi_archives",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features – sliding computation over epi_archives",
    "text": "Features – sliding computation over epi_archives\n\nWe can apply a computation over different snapshots in an epi_archive.\n\n\nepix_slide(\n  .x,\n  .f,\n  ...,\n  .before = Inf,\n  .versions = NULL,\n  .new_col_name = NULL,\n  .all_versions = FALSE\n)\n\nThis functionality is very helpful in version aware forecasting. We will return with a concrete example."
  },
  {
    "objectID": "slides/day1-afternoon.html#features-summarize-revision-behavior",
    "href": "slides/day1-afternoon.html#features-summarize-revision-behavior",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features – summarize revision behavior",
    "text": "Features – summarize revision behavior\n\nrevision_analysis() is a helper function that summarizes revision behavior of an epix_archive.\n\n\nrevision_data &lt;- revision_analysis(\n  archive_cases_dv_subset,\n  case_rate_7d_av,\n  drop_nas = TRUE,\n  min_waiting_period = as.difftime(60, units = \"days\"),\n  within_latest = 0.2,\n  compactify_abs_tol = .Machine$double.eps^0.5,\n)\nhead(revision_data$revision_behavior)\n\n# A tibble: 6 × 11\n  time_value geo_value n_revisions min_lag max_lag  lag_near_latest spread\n  &lt;date&gt;     &lt;chr&gt;           &lt;dbl&gt; &lt;drtn&gt;  &lt;drtn&gt;   &lt;drtn&gt;           &lt;dbl&gt;\n1 2020-06-01 ca                 12 1 days  546 days 1 days          0.248 \n2 2020-06-02 ca                 12 1 days  545 days 1 days          0.416 \n3 2020-06-03 ca                 11 1 days  544 days 1 days          0.115 \n4 2020-06-04 ca                 11 1 days  543 days 1 days          0.342 \n5 2020-06-05 ca                  7 1 days  520 days 1 days          0.0982\n6 2020-06-06 ca                  8 1 days  519 days 1 days          0.188 \n# ℹ 4 more variables: rel_spread &lt;dbl&gt;, min_value &lt;dbl&gt;, max_value &lt;dbl&gt;,\n#   median_value &lt;dbl&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#features-summarize-revision-behavior-1",
    "href": "slides/day1-afternoon.html#features-summarize-revision-behavior-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features – summarize revision behavior",
    "text": "Features – summarize revision behavior\n\nrevision_data\n\n\n\n\n── An epi_archive spanning 2020-06-01 to 2021-11-30. ──\n\n\n\n\n\n── Min lag (time to first version): \n\n\n     min median   mean    max\n  1 days 1 days 1 days 2 days\n\n\n\n\n\n── Fraction of epi_key + time_values with \n\n\nNo revisions:\n• 523 out of 1,956 (26.74%)\n\nQuick revisions (last revision within 3 days of the `time_value`):\n• 531 out of 1,956 (27.15%)\n\nFew revisions (At most 3 revisions for that `time_value`):\n• 1,199 out of 1,956 (61.3%)\n\n\n\n── Fraction of revised epi_key + time_values which have: \n\nLess than 0.1 spread in relative value:\n• 1,312 out of 1,433 (91.56%)\n\nSpread of more than 6.351 in actual value (when revised):\n• 39 out of 1,433 (2.72%)\n\n\n\n── Days until within 20% of the latest value: \n\n\n     min median     mean     max\n  1 days 1 days 1.7 days 84 days\n\n\nTry on your favorite archive so far"
  },
  {
    "objectID": "slides/day1-afternoon.html#visualize-revision-patterns",
    "href": "slides/day1-afternoon.html#visualize-revision-patterns",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Visualize revision patterns",
    "text": "Visualize revision patterns\n\narchive_cases_dv_subset_all_states %&gt;%\n  autoplot(.facet_filter = geo_value %in% c(\"ca\", \"ut\"), .versions = \"1 month\")"
  },
  {
    "objectID": "slides/day1-afternoon.html#finalized-data",
    "href": "slides/day1-afternoon.html#finalized-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Finalized data",
    "text": "Finalized data\n\nCounts are revised as time proceeds\nWant to know the final value\nOften not available until weeks/months later\n\nForecasting\n\nAt time \\(t\\), predict the final value for time \\(t+h\\), \\(h &gt; 0\\)\n\n\n\n\nBackcasting\n\nAt time \\(t\\), predict the final value for time \\(t-h\\), \\(h &lt; 0\\)\n\n\n\n\nNowcasting\n\nAt time \\(t\\), predict the final value for time \\(t\\)"
  },
  {
    "objectID": "slides/day1-afternoon.html#backfill-canadian-edition",
    "href": "slides/day1-afternoon.html#backfill-canadian-edition",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Backfill Canadian edition",
    "text": "Backfill Canadian edition\n\nEvery week the BC CDC releases COVID-19 hospitalization data.\nFollowing week they revise the number upward (by ~25%) due to lagged reports.\n\n\n\nTakeaway: Once the data is backfilled, hospitalizations rarely show a decline, challenging the common media narrative."
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-and-its-mathematical-setup",
    "href": "slides/day1-afternoon.html#nowcasting-and-its-mathematical-setup",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting and its mathematical setup",
    "text": "Nowcasting and its mathematical setup\n\nNowcasting: Predict a finalized value from a provisional value.\nSuppose today is time \\(t\\)\nLet \\(y_i\\) denote a series of interest observed at times \\(i=1,\\ldots, t\\).\n\n\n\n\nOur goal\n\n\n\nProduce a point nowcast for the finalized values of \\(y_t\\).\nAccompany with time-varying prediction intervals\n\n\n\n\n\nWe may also have access to \\(p\\) other time series \\(x_{ij},\\; i=1,\\ldots,t, \\; j = 1,\\ldots, p\\) which may be subject to revisions."
  },
  {
    "objectID": "slides/day1-afternoon.html#case-study-nchs-mortality",
    "href": "slides/day1-afternoon.html#case-study-nchs-mortality",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Case study: NCHS mortality",
    "text": "Case study: NCHS mortality\n\nIn this example, we’ll demonstrate the concept of nowcasting using NHCS mortality data. (the number of weekly new deaths with confirmed or presumed COVID-19, per 100,000 population).\nWe will work with provisional data (real-time reports) and compare them to finalized data (final reports).\nThe goal is to estimate or nowcast the mortality rate for weeks when only provisional data is available."
  },
  {
    "objectID": "slides/day1-afternoon.html#fetch-versioned-data",
    "href": "slides/day1-afternoon.html#fetch-versioned-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Fetch versioned data",
    "text": "Fetch versioned data\nLet’s fetch versioned mortality data from the API (pub_covidcast) for CA (geo_values = \"ca\") and the signal of interest (deaths_covid_incidence_num) over early 2024.\n\n# Fetch the versioned NCHS mortality data (weekly)\nnchs_archive &lt;- pub_covidcast(\n  source = \"nchs-mortality\",\n  signals = \"deaths_covid_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"week\",\n  geo_values = c(\"ca\", \"tx\"),\n  time_values = epirange(202001, 202440),\n  issues = \"*\"\n) |&gt;\n  select(geo_value, time_value, version = issue, mortality = value) |&gt;\n  as_epi_archive(compactify = TRUE)\n\nYou’ll need an API key to actually run this, unfortunately"
  },
  {
    "objectID": "slides/day1-afternoon.html#analysis-of-versioning-behavior",
    "href": "slides/day1-afternoon.html#analysis-of-versioning-behavior",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Analysis of versioning behavior",
    "text": "Analysis of versioning behavior\nRecall, we need to watch out for:\n\nLatency the time difference between date of reference and date of the initial report\nBackfill how data for a given date is updated after initial report.\n\nrevision_analysis() provides a summary of both aspects.\n\nrevision_data &lt;- revision_analysis(nchs_archive, mortality)"
  },
  {
    "objectID": "slides/day1-afternoon.html#versioning-analysis-latency",
    "href": "slides/day1-afternoon.html#versioning-analysis-latency",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Versioning analysis – latency",
    "text": "Versioning analysis – latency\n\nQuestion: What is the latency of NCHS data?\n\n\nrevision_data$revision_behavior |&gt; select(geo_value, time_value, min_lag) |&gt; slice_sample(n = 10)\n\n# A tibble: 10 × 3\n   geo_value time_value min_lag \n   &lt;chr&gt;     &lt;date&gt;     &lt;drtn&gt;  \n 1 ca        2024-04-07  1 weeks\n 2 ut        2021-09-12  1 weeks\n 3 ca        2022-03-27  2 weeks\n 4 ca        2021-02-28  1 weeks\n 5 ca        2020-08-02 18 weeks\n 6 ut        2021-11-07  1 weeks\n 7 ut        2022-04-24  1 weeks\n 8 ut        2022-06-19  2 weeks\n 9 ut        2023-02-19  1 weeks\n10 ca        2022-05-15  2 weeks\n\n\n\nWe randomly sampled some dates to check if there is a consistent latency pattern.\nUnderstanding latency prevents us from using data that we shouldn’t have access to."
  },
  {
    "objectID": "slides/day1-afternoon.html#versioning-analysis-backfill",
    "href": "slides/day1-afternoon.html#versioning-analysis-backfill",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Versioning analysis – backfill",
    "text": "Versioning analysis – backfill\n\nQuestion: How long does it take for the reported value to be close to the finalized value?\n\n\nrevision_data$revision_behavior |&gt; select(geo_value, time_value, lag_near_latest) |&gt; slice_sample(n = 10)\n\n# A tibble: 10 × 3\n   geo_value time_value lag_near_latest\n   &lt;chr&gt;     &lt;date&gt;     &lt;drtn&gt;         \n 1 ut        2020-05-31 27 weeks       \n 2 ca        2021-04-04  5 weeks       \n 3 ut        2022-08-07  2 weeks       \n 4 ca        2022-08-21  3 weeks       \n 5 ca        2023-07-23  3 weeks       \n 6 ut        2020-03-29 36 weeks       \n 7 ca        2020-11-01  5 weeks       \n 8 ca        2024-03-31  4 weeks       \n 9 ca        2024-07-14  3 weeks       \n10 ca        2024-08-18  3 weeks       \n\n\n\nIt generally takes at least 4 weeks for reported value to be within 20% (default in revision_analysis()) of the finalized value.\nWe can change the threshold of percentage difference by specifying the within_latest argument of revision_analysis()."
  },
  {
    "objectID": "slides/day1-afternoon.html#versioning-analysis---backfill",
    "href": "slides/day1-afternoon.html#versioning-analysis---backfill",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Versioning analysis - backfill",
    "text": "Versioning analysis - backfill\n\nQuestion: When is the finalized value first attained for each date? Would we have access to any in real-time?\nHow fast are the final values attained & what’s the pattern for these times, if any?\n\n\n\n# A tibble: 6 × 4\n  geo_value time_value min_version time_to_final\n  &lt;chr&gt;     &lt;date&gt;     &lt;date&gt;      &lt;drtn&gt;       \n1 ca        2020-01-26 2020-12-06  315 days     \n2 ca        2020-02-09 2020-12-06  301 days     \n3 ca        2020-02-23 2020-12-06  287 days     \n4 ca        2020-03-15 2020-12-06  266 days     \n5 ca        2020-03-22 2021-05-16  420 days     \n6 ca        2020-03-29 2021-04-04  371 days     \n\n\n\nConclusion: The revision behavior is pretty long-tailed. Value reported 4 weeks later is reasonably close to the finalized value."
  },
  {
    "objectID": "slides/day1-afternoon.html#revision-pattern-visualization",
    "href": "slides/day1-afternoon.html#revision-pattern-visualization",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Revision pattern visualization",
    "text": "Revision pattern visualization\nThis shows the finalized rates in comparison to multiple revisions to see how the data changes over time:"
  },
  {
    "objectID": "slides/day1-afternoon.html#revision-pattern-visualization-1",
    "href": "slides/day1-afternoon.html#revision-pattern-visualization-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Revision pattern visualization",
    "text": "Revision pattern visualization"
  },
  {
    "objectID": "slides/day1-afternoon.html#ratio-nowcaster-jumping-from-provisional-to-finalized-value",
    "href": "slides/day1-afternoon.html#ratio-nowcaster-jumping-from-provisional-to-finalized-value",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ratio nowcaster: jumping from provisional to finalized value",
    "text": "Ratio nowcaster: jumping from provisional to finalized value\n\nRecall, the goal of nowcast at date \\(t\\) is to use project the finalized value of \\(y_t,\\) given the information available on date \\(t\\).\nA very simple nowcaster is the ratio between finalized and provisional value.\n\nHow can we sensibly estimate this quantity?"
  },
  {
    "objectID": "slides/day1-afternoon.html#ratio-nowcaster-building-training-samples",
    "href": "slides/day1-afternoon.html#ratio-nowcaster-building-training-samples",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ratio nowcaster: building training samples",
    "text": "Ratio nowcaster: building training samples\n\nAt nowcast date \\(t,\\) would have received reports with versions up to and including \\(t.\\)\nWe need to build training samples, which\n\ncorrectly aligns finalized value against provisional value\nuses features that would have been available at test time\nhave enough samples to ensure sensible estimation results\n\n\n\n\nBuild training samples by treating dates prior to date \\(t\\) as actual nowcast dates.\n\nWhat is the provisional data on that date?\nHave we received finalized value for that date?"
  },
  {
    "objectID": "slides/day1-afternoon.html#ratio-nowcaster-building-training-samples-1",
    "href": "slides/day1-afternoon.html#ratio-nowcaster-building-training-samples-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ratio nowcaster: building training samples",
    "text": "Ratio nowcaster: building training samples\n\nAt an earlier nowcast date \\(t_0,\\) we define\n\nProvisional value as the reported value of \\(Y_{s_0}\\) with version \\(t_0.\\) Here \\(s_0\\) is the largest occurence date among all values reported up until \\(t_0.\\)\nFinalized value as the (potentially unobserved) finalized value of \\(Y_{s_0}.\\)\n\nWe only know in hindsight when reported value of \\(Y_{s_0}\\) is finalized – need an approximation."
  },
  {
    "objectID": "slides/day1-afternoon.html#revisiting-revision_analysis",
    "href": "slides/day1-afternoon.html#revisiting-revision_analysis",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Revisiting revision_analysis()",
    "text": "Revisiting revision_analysis()\nRecall, revision_analysis() reports the number of days to be within 20% (default value) of finalized value\n\n\n# A tibble: 5 × 3\n  geo_value time_value lag_near_latest\n  &lt;chr&gt;     &lt;date&gt;     &lt;drtn&gt;         \n1 ca        2024-02-18 4 weeks        \n2 ut        2023-12-03 3 weeks        \n3 ut        2022-10-16 3 weeks        \n4 ca        2020-12-20 8 weeks        \n5 ut        2022-09-11 3 weeks        \n\n\n  0%  25%  50%  75% 100% \n  NA   NA   NA   NA   NA \n\n\n\nLet’s say data reported NA days after reference date is good enough to be considered finalized."
  },
  {
    "objectID": "slides/day1-afternoon.html#ratio-nowcaster-test-time-feature",
    "href": "slides/day1-afternoon.html#ratio-nowcaster-test-time-feature",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ratio nowcaster: test time feature",
    "text": "Ratio nowcaster: test time feature\n\nDue to latency, provisional values may not be available at lag 0\nWe use last-observation-carried-forward (LOCF) to impute missing values at test time\nPrecisely, at test time \\(t,\\) we use last observed data point (among all those reported up through time \\(t\\))"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-at-a-single-date-building-training-samples",
    "href": "slides/day1-afternoon.html#nowcasting-at-a-single-date-building-training-samples",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting at a single date: building training samples",
    "text": "Nowcasting at a single date: building training samples\n\n\nSearching for provisional values, at previous hypothetical nowcast dates.\n\n\nnowcast_date &lt;- as.Date(\"2022-01-02\"); window_length = 180\n\ninitial_data &lt;- nchs_archive$DT |&gt;\n  group_by(geo_value, time_value) |&gt;\n  filter(version == min(version)) |&gt;\n  rename(initial_val = mortality) |&gt;\n  select(geo_value, time_value, initial_val)\n\n\n\n\nSearching for finalized values, at previous hypothetical nowcast dates.\n\n\n#|\nfinalized_data &lt;- epix_as_of(nchs_archive, nowcast_date) |&gt;\n  filter(time_value &gt;= nowcast_date - approx_final_lag - window_length & time_value &lt;= nowcast_date - approx_final_lag) |&gt;\n  rename(finalized_val = mortality) |&gt;\n  select(geo_value, time_value, finalized_val)"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-at-a-single-date-estimating-ratio-model",
    "href": "slides/day1-afternoon.html#nowcasting-at-a-single-date-estimating-ratio-model",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting at a single date: estimating ratio model",
    "text": "Nowcasting at a single date: estimating ratio model\n\nAfter searching for both provisional and finalized values, we merge them together and estimate the ratio. ::: {.cell layout-align=“center”}\n\nratio &lt;- finalized_data |&gt;\n  inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |&gt;\n  mutate(ratio = finalized_val / initial_val) |&gt;\n  pull(ratio) |&gt;\n  median(na.rm = TRUE)\n:::"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-at-a-single-date-test-feature-construction",
    "href": "slides/day1-afternoon.html#nowcasting-at-a-single-date-test-feature-construction",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting at a single date: test feature construction",
    "text": "Nowcasting at a single date: test feature construction\n\nlast_avail &lt;- epix_as_of(nchs_archive, nowcast_date) |&gt;\n  slice_max(time_value) |&gt;\n  pull(mortality)"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-at-a-single-date-producing-the-nowcast",
    "href": "slides/day1-afternoon.html#nowcasting-at-a-single-date-producing-the-nowcast",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting at a single date: producing the nowcast",
    "text": "Nowcasting at a single date: producing the nowcast\n\nnowcast &lt;- last_avail * ratio\nfinalized_val &lt;- epix_as_of(nchs_archive, nchs_archive$versions_end) |&gt;\n  filter(time_value == nowcast_date) |&gt;\npull(mortality)\n\nnowcast_final = data.frame(Nowcast = nowcast, `Finalized value` = finalized_val, check.names=FALSE)\nknitr::kable(nowcast_final)\n\n\n\n\nNowcast\nFinalized value\n\n\n\n\nNA\n948"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-for-multiple-dates",
    "href": "slides/day1-afternoon.html#nowcasting-for-multiple-dates",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting for multiple dates",
    "text": "Nowcasting for multiple dates\n\nAll previous manipulations should really be seen as a template for all nowcast dates.\nThe template computation sould be applied over all nowcast dates, but we must respect data versioning!\nepix_slide() is designed just for this! It behaves similarly to epi_slide.\nKey exception: epix_slide() is version aware: the sliding computation at any reference time \\(t\\) is performed on data that would have been available as of t."
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-for-multiple-dates-via-epix_slide",
    "href": "slides/day1-afternoon.html#nowcasting-for-multiple-dates-via-epix_slide",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting for multiple dates via epix_slide()",
    "text": "Nowcasting for multiple dates via epix_slide()\nWe begin by templatizing our previous operations.\n\nnowcaster &lt;- function(x, g, t, wl=180, appx=approx_final_lag) {\n  initial_data &lt;- x$DT |&gt;\n    group_by(geo_value, time_value) |&gt;\n    filter(version ==  min(version)) |&gt;\n    filter(time_value &gt;= t - wl - appx & time_value &lt;= t - appx) |&gt;\n    rename(initial_val = mortality) |&gt;\n    select(geo_value, time_value, initial_val)\n  finalized_data &lt;- x$DT |&gt;\n    group_by(geo_value, time_value) |&gt;\n    filter(version ==  max(version)) |&gt;\n    filter(time_value &gt;= t - wl - appx & time_value &lt;= t - appx) |&gt;\n    rename(finalized_val = mortality) |&gt;\n    select(geo_value, time_value, finalized_val)\n  ratio &lt;- finalized_data |&gt;\n    inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |&gt;\n    mutate(ratio = finalized_val / initial_val) |&gt;\n    pull(ratio) |&gt;\n    median(na.rm = TRUE)\n  last_avail &lt;-  epix_as_of(x, t) |&gt;\n    slice_max(time_value) |&gt;\n    pull(mortality)\n  tibble(geo_value = x$geo_value, target_date = t, nowcast = last_avail * ratio)\n}"
  },
  {
    "objectID": "slides/day1-afternoon.html#sanity-check-of-epix_slide",
    "href": "slides/day1-afternoon.html#sanity-check-of-epix_slide",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Sanity check of epix_slide()",
    "text": "Sanity check of epix_slide()\n\nslided_nowcast_1d = epix_slide(\n  .x = nchs_archive,\n  .f = nowcaster,\n  .before = Inf,\n  .versions = nowcast_date,\n  .all_versions = TRUE\n)\n\nnowcast_check = data.frame(`Manual nowcast` = nowcast, `Slided nowcast` = slided_nowcast_1d$nowcast, check.names = FALSE)\nknitr::kable(nowcast_check)\n\n\n\n\nManual nowcast\nSlided nowcast\n\n\n\n\nNA\nNA"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-for-multiple-dates-via-epix_slide-1",
    "href": "slides/day1-afternoon.html#nowcasting-for-multiple-dates-via-epix_slide-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting for multiple dates via epix_slide()",
    "text": "Nowcasting for multiple dates via epix_slide()\n\nnowcasts = nchs_archive |&gt;\n  group_by(geo_value) |&gt;\n  epix_slide(\n    nowcaster,\n    .before=Inf,\n    .versions = all_nowcast_dates,\n    .all_versions = TRUE\n)"
  },
  {
    "objectID": "slides/day1-afternoon.html#details-of-epix_slide",
    "href": "slides/day1-afternoon.html#details-of-epix_slide",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Details of epix_slide()",
    "text": "Details of epix_slide()\n\nepix_slide(\n  .x,\n  .f,\n  ...,\n  .before = Inf,\n  .versions = NULL,\n  .new_col_name = NULL,\n  .all_versions = FALSE\n)\n\n\n.f in epix_slide() can be specified with the same form of custom function as epi_slide().\n\n\nfunction(x, g, t) {\n  # function body\n}\n\n\nMandatory variables of .f would have different forms depending on the value of .all_versions."
  },
  {
    "objectID": "slides/day1-afternoon.html#details-of-epix_slide-1",
    "href": "slides/day1-afternoon.html#details-of-epix_slide-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Details of epix_slide()",
    "text": "Details of epix_slide()\n\nepix_slide(\n  .x,\n  .f,\n  ...,\n  .before = Inf,\n  .versions = NULL,\n  .new_col_name = NULL,\n  .all_versions = FALSE\n)\n\n\n\nWhen .all_versions = FALSE, epix_slide() essentially iterates the templatized computation over snapshots.\nSaid differently, when .all_versions = FALSE, data accessed at any sliding iteration only involves a single version.\n\n\n\n\nHence:\n\nx: an epi_df with same column names as archive’s DT, minus the version column.\ng: a one-row tibble containing the values of groupping variables of the associated group.\nt: the ref_time_value of the current window.\n...: additional arguments."
  },
  {
    "objectID": "slides/day1-afternoon.html#details-of-epix_slide-2",
    "href": "slides/day1-afternoon.html#details-of-epix_slide-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Details of epix_slide()",
    "text": "Details of epix_slide()\n\nepix_slide(\n  .x,\n  .f,\n  ...,\n  .before = Inf,\n  .versions = NULL,\n  .new_col_name = NULL,\n  .all_versions = TRUE\n)\n\n\n\nWhen .all_versions = FALSE, data accessed at any sliding iteration involves versions up to and including .version.\n\n\n\n\nHence:\n\nx: an epi_archive, with version up to and including .version.\ng: a one-row tibble containing the values of groupping variables of the associated group.\nt: the .version of the current window.\n...: additional arguments."
  },
  {
    "objectID": "slides/day1-afternoon.html#details-of-epix_slide-3",
    "href": "slides/day1-afternoon.html#details-of-epix_slide-3",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Details of epix_slide()",
    "text": "Details of epix_slide()\n\nnowcasts &lt;- nchs_archive |&gt;\n  group_by(geo_value) |&gt;\n  epix_slide(\n    nowcaster,\n    .before=Inf,\n    .versions = all_nowcast_dates,\n    .all_versions = TRUE\n)"
  },
  {
    "objectID": "slides/day1-afternoon.html#details-of-epix_slide-4",
    "href": "slides/day1-afternoon.html#details-of-epix_slide-4",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Details of epix_slide()",
    "text": "Details of epix_slide()\n\nnowcaster &lt;- function(x, g, t, wl=180, appx=approx_final_lag) {\n  initial_data &lt;- x$DT |&gt;\n    group_by(geo_value, time_value) |&gt;\n    filter(version ==  min(version)) |&gt;\n    filter(time_value &gt;= t - wl - appx & time_value &lt;= t - appx) |&gt;\n    rename(initial_val = mortality) |&gt;\n    select(geo_value, time_value, initial_val)\n  finalized_data &lt;- x$DT |&gt;\n    group_by(geo_value, time_value) |&gt;\n    filter(version ==  max(version)) |&gt;\n    filter(time_value &gt;= t - wl - appx & time_value &lt;= t - appx) |&gt;\n    rename(finalized_val = mortality) |&gt;\n    select(geo_value, time_value, finalized_val)\n  ratio &lt;- finalized_data |&gt;\n    inner_join(initial_data, by = c(\"geo_value\", \"time_value\")) |&gt;\n    mutate(ratio = finalized_val / initial_val) |&gt;\n    pull(ratio) |&gt;\n    median(na.rm=TRUE)\n  last_avail &lt;- epix_as_of(x, t) |&gt;\n    slice_max(time_value) |&gt;\n    pull(mortality)\n  tibble(geo_value = x$geo_value, target_date = t, nowcast = last_avail * ratio)\n}"
  },
  {
    "objectID": "slides/day1-afternoon.html#visualize-nowcasts",
    "href": "slides/day1-afternoon.html#visualize-nowcasts",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Visualize nowcasts",
    "text": "Visualize nowcasts\nWe are now finally able to compare nowcasts against first available reports:\n\n\nThe real-time counts tend to be biased below the finalized counts. Nowcasted values tend to provide a much better approximation of the truth (at least for these dates)."
  },
  {
    "objectID": "slides/day1-afternoon.html#smoothing-nowcasts",
    "href": "slides/day1-afternoon.html#smoothing-nowcasts",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Smoothing nowcasts",
    "text": "Smoothing nowcasts\n\nNowcasts are quite volatile, reflecting the provisional counts are far from complete.\nWe can use a trailing average to smooth them.\n\n\nsmoothed_nowcasts &lt;- epi_slide(\n  nowcasts |&gt; as_epi_df(),\n  smoothed_nowcasts = mean(nowcast, na.rm = TRUE),\n  .window_size = as.difftime(3, units = \"weeks\")\n)"
  },
  {
    "objectID": "slides/day1-afternoon.html#evaluation-using-mae",
    "href": "slides/day1-afternoon.html#evaluation-using-mae",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Evaluation using MAE",
    "text": "Evaluation using MAE\n\nAssume we have prediction \\(\\hat y_{t}\\) for the provisional value at time \\(t\\).\nThen for \\(y_{t}\\) over times \\(t = 1, \\dots, N\\), then we may compute error metrics like mean absolute error (MAE).\nMAE measures the average absolute difference between the nowcast and finalized values.\n\n\\[MAE = \\frac{1}{N} \\sum_{t=1}^N |y_{t}- \\hat y_{t}|\\]\n\nNote that it’s scale-dependent, meaning it can vary depending on the units of the data (e.g., cases, deaths, etc.)."
  },
  {
    "objectID": "slides/day1-afternoon.html#evaluation-using-mae-1",
    "href": "slides/day1-afternoon.html#evaluation-using-mae-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Evaluation using MAE",
    "text": "Evaluation using MAE\nLet’s numerically evaluate our point nowcasts for the provisional values of a time series (e.g., COVID-19 mortality) using MAE.\n\n\n\n\n\nSmoothed MAE\nUnsmoothed nowcast MAE\nProvisional value MAE\n\n\n\n\nNaN\nNaN\n201.0909\n\n\n\n\n\n&lt;!– # Nowcasting with Regression"
  },
  {
    "objectID": "slides/day2-afternoon.html#section",
    "href": "slides/day2-afternoon.html#section",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Forecasting With {epipredict}",
    "text": "Forecasting With {epipredict}\nMICOM Tooling Workshop 2025\n\nDavid Weber, Nat DeFries\nAdapted from slides by Alice Cima, Rachel Lobay, Daniel McDonald, Ryan Tibshirani, with huge thanks to Logan Brooks, Xueda Shen, and Dmitry Shemetov\n12 August 2025"
  },
  {
    "objectID": "slides/day2-afternoon.html#outline",
    "href": "slides/day2-afternoon.html#outline",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Outline",
    "text": "Outline\n\nOverview of {epipredict}\nPreprocessing\nBasic Usage of arx_forecaster()\nExtending arx_forecaster()"
  },
  {
    "objectID": "slides/day2-afternoon.html#epipredict",
    "href": "slides/day2-afternoon.html#epipredict",
    "title": "MICOM EpiData Workshop 2025",
    "section": "{epipredict}",
    "text": "{epipredict}\nhttps://cmu-delphi.github.io/epipredict\nInstallation\n\n# Stable version\n# We're using this.\npak::pkg_install(\"cmu-delphi/epipredict@main\")\n\n\n# Development version\npak::pkg_install(\"cmu-delphi/epipredict@dev\")"
  },
  {
    "objectID": "slides/day2-afternoon.html#what-epipredict-provides-i",
    "href": "slides/day2-afternoon.html#what-epipredict-provides-i",
    "title": "MICOM EpiData Workshop 2025",
    "section": "What {epipredict} provides (i)",
    "text": "What {epipredict} provides (i)\nBasic and easy to use “canned” forecasters. There are several “basline” forecasters:\n\nBaseline flat forecaster\nCDC FluSight flatline forecaster\nClimatological model\n\nAs well as two flexible autoregressive forecasters:\n\nAutoregressive classifier\nAutoregressive forecaster (ARX)\n\n\nPlanning on adding more."
  },
  {
    "objectID": "slides/day2-afternoon.html#what-epipredict-provides-ii",
    "href": "slides/day2-afternoon.html#what-epipredict-provides-ii",
    "title": "MICOM EpiData Workshop 2025",
    "section": "What {epipredict} provides (ii)",
    "text": "What {epipredict} provides (ii)\n\nA framework for creating custom forecasters out of modular components.\nThis is highly customizable, extends {tidymodels} to panel data\nGood for building a new forecaster from scratch\nUsed to construct\nThere are four types of components:\n\nPreprocessor: do things to the data before model training\nTrainer: train a model on data, resulting in a fitted model object\nPredictor: make predictions, using a fitted model object\nPostprocessor: do things to the predictions before returning\n\n\nIf you want examples of usage, see https://cmu-delphi.github.io/epipredict/articles/custom_epiworkflows.html"
  },
  {
    "objectID": "slides/day2-afternoon.html#examples-of-pre-processing",
    "href": "slides/day2-afternoon.html#examples-of-pre-processing",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Examples of pre-processing",
    "text": "Examples of pre-processing\n\nExploratory Data Analysis (EDA)\n\nMaking locations/signals commensurate (scaling)\nDealing with revisions\nDetecting and removing outliers\nImputing or removing missing data\n\n\n\nFeature engineering\n\nCreating lagged predictors\nDay of Week effects\nRolling averages for smoothing\nLagged differences\nGrowth rates instead of raw signals\nThe sky’s the limit"
  },
  {
    "objectID": "slides/day2-afternoon.html#get-some-data",
    "href": "slides/day2-afternoon.html#get-some-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Get some data",
    "text": "Get some data\n\nlibrary(epidatr)\nlibrary(epiprocess)\nlibrary(epipredict)\n\ncases &lt;- pub_covidcast(\n  source = \"jhu-csse\",\n  signals = \"confirmed_incidence_num\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20200401, 20230401),\n  geo_values = \"*\") |&gt;\n  select(geo_value, time_value, cases = value)\n\ndeaths &lt;- pub_covidcast(\n  source = \"jhu-csse\",\n  signals = \"deaths_incidence_num\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20200401, 20230401),\n  geo_values = \"*\") |&gt;\n  select(geo_value, time_value, deaths = value)\n\ncases_deaths &lt;- full_join(cases, deaths, by = c(\"time_value\", \"geo_value\")) |&gt;\n  as_epi_df()"
  },
  {
    "objectID": "slides/day2-afternoon.html#pre-processing-data-scaling",
    "href": "slides/day2-afternoon.html#pre-processing-data-scaling",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Pre-processing: data scaling",
    "text": "Pre-processing: data scaling\nScale cases and deaths by population and multiply by 100K\n\ncases_deaths &lt;- left_join(\n  x = cases_deaths,\n  y = state_census |&gt; select(pop, abbr),   # state_census is available in epipredict\n  by = join_by(geo_value == abbr)\n) |&gt;\n  mutate(\n    cases = cases / pop * 1e5,\n    deaths = deaths / pop * 1e5\n  ) |&gt;\n  select(-pop)"
  },
  {
    "objectID": "slides/day2-afternoon.html#scaled-covid-cases-and-deaths",
    "href": "slides/day2-afternoon.html#scaled-covid-cases-and-deaths",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Scaled COVID cases and deaths",
    "text": "Scaled COVID cases and deaths\n\n\nCode\ncases_deaths |&gt;\n  filter(geo_value %in% c(\"ca\", \"ma\", \"ny\", \"tx\")) |&gt;\n  autoplot(cases, deaths) +\n  scale_color_delphi(name = \"\") +\n  xlab(\"Reference date\")"
  },
  {
    "objectID": "slides/day2-afternoon.html#pre-processing-smoothing",
    "href": "slides/day2-afternoon.html#pre-processing-smoothing",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Pre-processing: smoothing",
    "text": "Pre-processing: smoothing\nSmooth the data by computing 7-day averages of cases and deaths for each state\n\ncases_deaths &lt;- cases_deaths |&gt;\n  group_by(geo_value) |&gt;\n  epi_slide(\n    cases_7dav = mean(cases, na.rm = TRUE),\n    deaths_7dav = mean(deaths, na.rm = TRUE),\n    .window_size = 7\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(cases = NULL, deaths = NULL) |&gt;\n  rename(cases = cases_7dav, deaths = deaths_7dav)"
  },
  {
    "objectID": "slides/day2-afternoon.html#scaled-and-smoothed-covid-cases-deaths",
    "href": "slides/day2-afternoon.html#scaled-and-smoothed-covid-cases-deaths",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Scaled and smoothed COVID cases deaths",
    "text": "Scaled and smoothed COVID cases deaths\n\n\nCode\ncases_deaths |&gt;\n  filter(geo_value %in% c(\"ca\", \"ma\", \"ny\", \"tx\")) |&gt;\n  autoplot(cases, deaths)  +\n  scale_color_delphi(name = \"\") +\n  xlab(\"Reference date\")"
  },
  {
    "objectID": "slides/day2-afternoon.html#pre-processing-fix-outliers-and-negative-values",
    "href": "slides/day2-afternoon.html#pre-processing-fix-outliers-and-negative-values",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Pre-processing: fix outliers and negative values",
    "text": "Pre-processing: fix outliers and negative values\n\n\nCode\ndeaths_outlr &lt;- cases_deaths |&gt;\n  group_by(geo_value) |&gt;\n  mutate(outlr = detect_outlr_rm(time_value, deaths, detect_negatives = TRUE)) |&gt;\n  unnest(outlr) |&gt;\n  ungroup()"
  },
  {
    "objectID": "slides/day2-afternoon.html#fit-arx_forecaster-on-training-set",
    "href": "slides/day2-afternoon.html#fit-arx_forecaster-on-training-set",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Fit arx_forecaster on training set",
    "text": "Fit arx_forecaster on training set\n\nBack to the ARX(1) model for COVID deaths: \\(\\quad \\hat y_{t+28} = \\hat\\phi + \\hat\\phi_0 y_{t} + \\hat\\beta_0 x_{t}\\)\nUsing {epipredict}\n\n\n# split into train and test\nt0_date &lt;- as.Date('2021-04-01')\ntrain &lt;- cases_deaths |&gt; filter(time_value &lt;= t0_date)\ntest &lt;- cases_deaths |&gt; filter(time_value &gt; t0_date)\n\n# fit ARX\nepi_arx &lt;- arx_forecaster(\n  epi_data = train |&gt; as_epi_df(),\n  outcome = \"deaths\",\n  predictors = c(\"cases\", \"deaths\"),\n  trainer = quantile_reg(),\n  args_list = arx_args_list(lags = 0, ahead = 28)\n)"
  },
  {
    "objectID": "slides/day2-afternoon.html#arx_forecaster-output",
    "href": "slides/day2-afternoon.html#arx_forecaster-output",
    "title": "MICOM EpiData Workshop 2025",
    "section": "arx_forecaster output",
    "text": "arx_forecaster output\n\nA forecast (point prediction + quantiles) for 28 days after the last available time value in the data ($predictions).\n\n\nepi_arx$predictions\n\n# A tibble: 56 × 5\n   geo_value  .pred .pred_distn forecast_date target_date\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;qtls(3)&gt; &lt;date&gt;        &lt;date&gt;     \n 1 ak        0.265      [0.265] 2021-04-01    2021-04-29 \n 2 al        0.133      [0.133] 2021-04-01    2021-04-29 \n 3 ar        0.141      [0.141] 2021-04-01    2021-04-29 \n 4 as        0.0213    [0.0213] 2021-04-01    2021-04-29 \n 5 az        0.149      [0.149] 2021-04-01    2021-04-29 \n 6 ca        0.172      [0.172] 2021-04-01    2021-04-29 \n 7 co        0.284      [0.284] 2021-04-01    2021-04-29 \n 8 ct        0.418      [0.418] 2021-04-01    2021-04-29 \n 9 dc        0.281      [0.281] 2021-04-01    2021-04-29 \n10 de        0.414      [0.414] 2021-04-01    2021-04-29 \n# ℹ 46 more rows\n\n\n\nA workflow object which could be used to create forecasts using the same training data on new observations ($epi_workflow).\n\n\n\nepi_arx$epi_workflow\n\n\n\n\n══ Epi Workflow [trained] ══════════════════════════════════════════════════════\n\n\nPreprocessor: Recipe\n\n\nModel: quantile_reg()\n\n\nPostprocessor: Frosting\n\n\n\n\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n\n\n\n\n\n7 Recipe steps.\n\n\n1. step_epi_lag()\n\n\n2. step_epi_lag()\n\n\n3. step_epi_ahead()\n\n\n4. step_naomit()\n\n\n5. step_naomit()\n\n\n6. step_training_window()\n\n\n7. check_enough_data()\n\n\n\n\n\n── Model ───────────────────────────────────────────────────────────────────────\n\n\nCall:\nquantreg::rq(formula = ..y ~ ., tau = ~c(0.1, 0.5, 0.9), data = data, \n    na.action = stats::na.omit, method = ~\"br\", model = FALSE)\n\nCoefficients:\n                 tau= 0.1   tau= 0.5   tau= 0.9\n(Intercept)  -0.005542771 0.02129189 0.11733391\nlag_0_cases   0.004567999 0.01011764 0.01575699\nlag_0_deaths  0.176866144 0.21916250 0.34641310\n\nDegrees of freedom: 18928 total; 18925 residual\n\n\n\n\n\n── Postprocessor ───────────────────────────────────────────────────────────────\n\n\n\n\n\n6 Frosting layers.\n\n\n1. layer_predict()\n\n\n2. layer_quantile_distn()\n\n\n3. layer_point_from_distn()\n\n\n4. layer_add_forecast_date()\n\n\n5. layer_add_target_date()\n\n\n6. layer_threshold()\n\n\n\n\n\n\n\n\nAll necessary preprocessing; both the sequence of steps, and any necessary statistics\nThe fitted model object\nThe sequence of steps for postprocessing\ntypically, if you’re making a forecast at a later date, you’d want to retrain using the newly released data however!"
  },
  {
    "objectID": "slides/day2-afternoon.html#extract-predictions",
    "href": "slides/day2-afternoon.html#extract-predictions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Extract predictions",
    "text": "Extract predictions\n.pred_distin is a distribution, which we can extract the distribution into a “long” epi_df:\n\nepi_arx$predictions |&gt;\n  pivot_quantiles_longer(.pred_distn) |&gt;\n  select(-.pred) |&gt;\n  filter(geo_value == \"ca\")\n\n# A tibble: 3 × 5\n  geo_value forecast_date target_date .pred_distn_value .pred_distn_quantile_l…¹\n  &lt;chr&gt;     &lt;date&gt;        &lt;date&gt;                  &lt;dbl&gt;                    &lt;dbl&gt;\n1 ca        2021-04-01    2021-04-29             0.0917                      0.1\n2 ca        2021-04-01    2021-04-29             0.172                       0.5\n3 ca        2021-04-01    2021-04-29             0.354                       0.9\n# ℹ abbreviated name: ¹​.pred_distn_quantile_level\n\n\nor into a “wide” epi_df\n\nepi_arx$predictions |&gt;\n  pivot_quantiles_wider(.pred_distn) |&gt;\n  filter(geo_value == \"ca\")\n\n# A tibble: 1 × 7\n  geo_value .pred forecast_date target_date  `0.1` `0.5` `0.9`\n  &lt;chr&gt;     &lt;dbl&gt; &lt;date&gt;        &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ca        0.172 2021-04-01    2021-04-29  0.0917 0.172 0.354"
  },
  {
    "objectID": "slides/day2-afternoon.html#arx_forecaster-output-1",
    "href": "slides/day2-afternoon.html#arx_forecaster-output-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "arx_forecaster output",
    "text": "arx_forecaster output\n\nepi_arx\n\n══ A basic forecaster of type ARX Forecaster ═══════════════════════════════════\n\n\n\n\n\nThis forecaster was fit on 2025-08-08 16:35:14.\n\n\n\n\n\nTraining data was an &lt;epi_df&gt; with:\n\n\n• Geography: state,\n\n\n• Time type: day,\n\n\n• Using data up-to-date as of: 2025-08-08 11:18:52.\n\n\n• With the last data available on 2021-04-01\n\n\n\n\n\n── Predictions ─────────────────────────────────────────────────────────────────\n\n\n\n\n\nA total of 56 predictions are available for\n\n\n• 56 unique geographic regions,\n\n\n• At forecast date: 2021-04-01,\n\n\n• For target date: 2021-04-29,"
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx-when-re-fitting",
    "href": "slides/day2-afternoon.html#predict-with-arx-when-re-fitting",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predict with ARX (when re-fitting)",
    "text": "Predict with ARX (when re-fitting)\n\nCould use predict(epi_arx$epi_workflow, new_data) but better to retrain on latest data\nWe fit and predict combining arx_forecaster with epix_slide\nFrom now on, we will only used versioned data, and make predictions once a week"
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window",
    "href": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predict with ARX (re-fitting on trailing window)",
    "text": "Predict with ARX (re-fitting on trailing window)\n\nh &lt;- 28         # horizon\n\n# Specify the forecast dates\nfc_time_values &lt;- seq(from = t0_date, to = as.Date(\"2023-02-09\"), by = \"1 week\")\n\n# Slide the arx_forecaster over the epi_archive\npred_arx &lt;- covid_archive |&gt; epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\",\n                   predictors = c(\"cases\", \"deaths\"),\n                   trainer = quantile_reg(),\n                   args_list = arx_args_list(lags = 0, ahead = h)\n  )$predictions |&gt;\n    pivot_quantiles_wider(.pred_distn),\n  .before = Inf,\n  .versions = fc_time_values\n)\n\n\nNot a great way to read the output, but the new column is version, which forecast_date is always before."
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window-1",
    "href": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predict with ARX (re-fitting on trailing window)",
    "text": "Predict with ARX (re-fitting on trailing window)\n\n\npred_arx |&gt; filter(geo_value == \"ca\")\n\n# A tibble: 98 × 8\n   version    geo_value  .pred forecast_date target_date  `0.1`  `0.5` `0.9`\n   &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;date&gt;        &lt;date&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 2021-04-01 ca        0.191  2021-03-31    2021-04-28  0.108  0.191  0.374\n 2 2021-04-08 ca        0.146  2021-04-07    2021-05-05  0.0703 0.146  0.303\n 3 2021-04-15 ca        0.154  2021-04-14    2021-05-12  0.0742 0.154  0.316\n 4 2021-04-22 ca        0.116  2021-04-21    2021-05-19  0.0498 0.116  0.257\n 5 2021-04-29 ca        0.0968 2021-04-28    2021-05-26  0.0396 0.0968 0.227\n 6 2021-05-06 ca        0.0999 2021-05-05    2021-06-02  0.0435 0.0999 0.230\n 7 2021-05-13 ca        0.0936 2021-05-12    2021-06-09  0.0381 0.0936 0.220\n 8 2021-05-20 ca        0.0726 2021-05-19    2021-06-16  0.0262 0.0726 0.187\n 9 2021-05-27 ca        0.0648 2021-05-26    2021-06-23  0.0218 0.0648 0.174\n10 2021-06-03 ca        0.0739 2021-06-02    2021-06-30  0.0266 0.0739 0.190\n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window-2",
    "href": "slides/day2-afternoon.html#predict-with-arx-re-fitting-on-trailing-window-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predict with ARX (re-fitting on trailing window)",
    "text": "Predict with ARX (re-fitting on trailing window)\n\nAligned with the target_date, forecasted 4 weeks earlier on the forecast_date.\n\n\nFirst date gives the distance between forecast_date and target_date.\nBands are the 10-90 quantile interval.\nDoes reasonably well outside of early 2022, where it was overly pessimistic; also training on data from other states."
  },
  {
    "objectID": "slides/day2-afternoon.html#customizing-arx_forecaster",
    "href": "slides/day2-afternoon.html#customizing-arx_forecaster",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Customizing arx_forecaster()",
    "text": "Customizing arx_forecaster()\n\narx_forecaster(\n  epi_data = train,\n  outcome = \"deaths\",\n  predictors = c(\"cases\", \"deaths\"),\n  trainer = quantile_reg(),\n  args_list = arx_args_list(lags = 0, ahead = 28)\n)\n\n\n\nModify predictors to add/drop predictors\n\ne.g. drop cases to get AR model, or drop deaths for regression with a lagged predictor\ndefault: predictors = outcome"
  },
  {
    "objectID": "slides/day2-afternoon.html#customizing-arx_forecaster-1",
    "href": "slides/day2-afternoon.html#customizing-arx_forecaster-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Customizing arx_forecaster()",
    "text": "Customizing arx_forecaster()\n\narx_forecaster(\n  epi_data = train,\n  outcome = \"deaths\",\n  predictors = c(\"cases\", \"deaths\"),\n  trainer = quantile_reg(),\n  args_list = arx_args_list(lags = 0, ahead = 28)\n)\n\n\nModify arx_args_list to change lags, horizon, quantile levels, latency adjustment, …\n\n\n\narx_args_list(\n  lags = c(0L, 7L, 14L),\n  ahead = 7L,\n  n_training = Inf,\n  forecast_date = NULL,\n  target_date = NULL,\n  adjust_latency = c(\"none\", \"extend_ahead\", \"extend_lags\", \"locf\"),\n  warn_latency = TRUE,\n  quantile_levels = c(0.05, 0.95),\n  symmetrize = TRUE,\n  nonneg = TRUE,\n  quantile_by_key = character(0L),\n  check_enough_data_n = NULL,\n  check_enough_data_epi_keys = NULL,\n  ...\n)"
  },
  {
    "objectID": "slides/day2-afternoon.html#customizing-arx_forecaster-2",
    "href": "slides/day2-afternoon.html#customizing-arx_forecaster-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Customizing arx_forecaster",
    "text": "Customizing arx_forecaster\nChange predictors: doctor visits instead of cases\n\ndv_archive &lt;- pub_covidcast(\n  source = \"doctor-visits\",\n  signals = \"smoothed_adj_cli\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20200401, 20230401),\n  geo_values = \"*\",\n  issues = epirange(20200401, 20230401)) |&gt;\n  select(geo_value, time_value, version = issue, doctor_visits = value) |&gt;\n  arrange(geo_value, time_value) |&gt;\n  as_epi_archive(compactify = FALSE)"
  },
  {
    "objectID": "slides/day2-afternoon.html#customizing-arx_forecaster-3",
    "href": "slides/day2-afternoon.html#customizing-arx_forecaster-3",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Customizing arx_forecaster",
    "text": "Customizing arx_forecaster\nChange predictors: doctor visits instead of cases\n\npred_arx_hosp &lt;- covid_archive_dv |&gt; epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\",\n                   predictors = c(\"deaths\", \"doctor_visits\"),\n                   trainer = quantile_reg(),\n                   args_list = arx_args_list(lags = 0, ahead = 28)\n  )$predictions |&gt;\n    pivot_quantiles_wider(.pred_distn),\n  .before = Inf,\n  .versions = fc_time_values\n)"
  },
  {
    "objectID": "slides/day2-afternoon.html#predictions-doctor-visits-instead-of-cases-in-predictor-set",
    "href": "slides/day2-afternoon.html#predictions-doctor-visits-instead-of-cases-in-predictor-set",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions (doctor visits instead of cases in predictor set)",
    "text": "Predictions (doctor visits instead of cases in predictor set)\n\n\n\n        MAE     MASE  Coverage\n 0.07889637 264.5207 0.4285714\n\n\nQuite different predictions!"
  },
  {
    "objectID": "slides/day2-afternoon.html#customizing-arx_forecaster-4",
    "href": "slides/day2-afternoon.html#customizing-arx_forecaster-4",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Customizing arx_forecaster",
    "text": "Customizing arx_forecaster\nAdd more lags\n\npred_arx_more_lags &lt;- covid_archive_dv |&gt; epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\",\n                   predictors = c(\"deaths\", \"doctor_visits\"),\n                   trainer = quantile_reg(quantile_levels = c(0.1, 0.5, 0.9)),\n                   args_list = arx_args_list(\n                     lags = list(deaths=c(0, 7, 14), doctor_visits=c(0, 7)),\n                     ahead = 28,\n                     quantile_levels=c(0.1, 0.5, 0.9)\n                   )\n  )$predictions |&gt;\n    pivot_quantiles_wider(.pred_distn),\n  .before = Inf,\n  .versions = fc_time_values\n)"
  },
  {
    "objectID": "slides/day2-afternoon.html#predictions-more-lags",
    "href": "slides/day2-afternoon.html#predictions-more-lags",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions (more lags)",
    "text": "Predictions (more lags)\n\nMore isn’t always better"
  },
  {
    "objectID": "slides/day2-afternoon.html#arx_forecaster-for-multiple-horizons",
    "href": "slides/day2-afternoon.html#arx_forecaster-for-multiple-horizons",
    "title": "MICOM EpiData Workshop 2025",
    "section": "arx_forecaster for multiple horizons",
    "text": "arx_forecaster for multiple horizons\nMultiple horizons\n\nforecast_times &lt;- seq(from = t0_date, to = as.Date(\"2023-02-23\"), by = \"2 month\")\npred_h_days_ahead &lt;- function(epi_archive, ahead = 7) {\n  epi_archive |&gt;\n    epix_slide(\n      ~ arx_forecaster(epi_data = .x,\n                       outcome = \"deaths\",\n                       predictors = c(\"deaths\", \"doctor_visits\"),\n                       trainer = quantile_reg(quantile_levels = c(.1, .5, .9)),\n                       args_list = arx_args_list(\n                         lags = 0,\n                         ahead = ahead,\n                         quantile_levels =  c(.1, .5, .9)\n                       )\n      )$predictions |&gt;\n        pivot_quantiles_wider(.pred_distn),\n  .before = Inf,\n  .versions = forecast_times\n  )\n}\nh &lt;- 7 * c(0, 1, 2, 3, 4, 5, 6)\nforecasts &lt;- bind_rows(map(h, ~ pred_h_days_ahead(ca_archive_dv, ahead = .x)))"
  },
  {
    "objectID": "slides/day2-afternoon.html#predictions-multiple-horizons",
    "href": "slides/day2-afternoon.html#predictions-multiple-horizons",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions (multiple horizons)",
    "text": "Predictions (multiple horizons)\n\nColor corresponds to forecast_date"
  },
  {
    "objectID": "slides/day2-afternoon.html#geo-pooling",
    "href": "slides/day2-afternoon.html#geo-pooling",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Geo-pooling",
    "text": "Geo-pooling\n\nWhen we observe data over time from multiple locations (e.g. states or counties), we could:\n\nEstimate coefficients separately for each location, or\nFit one model using all locations together at each time point (geo-pooling).\n\n\nEpipredict geo-pools"
  },
  {
    "objectID": "slides/day2-afternoon.html#predictions-with-geo-pooling-reprise",
    "href": "slides/day2-afternoon.html#predictions-with-geo-pooling-reprise",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions with geo-pooling (reprise)",
    "text": "Predictions with geo-pooling (reprise)\nWe’ve been geo-pooling in the previous examples, here’s the version using doctor’s visits.\n\npred_arx_hosp &lt;- covid_archive_dv |&gt; epix_slide(\n  ~ arx_forecaster(epi_data = .x,\n                   outcome = \"deaths\",\n                   predictors = c(\"deaths\", \"doctor_visits\"),\n                   trainer = quantile_reg(),\n                   args_list = arx_args_list(lags = 0, ahead = 28)\n  )$predictions |&gt;\n    pivot_quantiles_wider(.pred_distn),\n  .before = Inf,\n  .versions = fc_time_values\n)"
  },
  {
    "objectID": "slides/day2-afternoon.html#predictions-geo-pooling-h28",
    "href": "slides/day2-afternoon.html#predictions-geo-pooling-h28",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions (geo-pooling, \\(h=28\\))",
    "text": "Predictions (geo-pooling, \\(h=28\\))\n\n\n\n         MAE     MASE  Coverage\nCA 0.1342220 450.0143 0.8673469\nMA 0.1200344 296.9929 0.8041237\nNY 0.1328566 327.4008 0.8556701\nTX 0.1601115 343.0498 0.8350515"
  },
  {
    "objectID": "slides/day2-afternoon.html#predict-without-geo-pooling",
    "href": "slides/day2-afternoon.html#predict-without-geo-pooling",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predict without geo-pooling",
    "text": "Predict without geo-pooling\n\npred_arx_no_geo_pool &lt;- function(archive, ahead = 28, lags = 0){\n  archive |&gt;\n    epix_slide(\n      ~ group_by(.x, geo_value) |&gt;\n        group_map(.keep = TRUE, function(group_data, group_key) {\n          arx_forecaster(epi_data = group_data,\n                         outcome = \"deaths\",\n                         predictors = c(\"deaths\", \"doctor_visits\"),\n                         trainer = quantile_reg(quantile_levels=c(0.1, 0.9)),\n                         args_list = arx_args_list(\n                           lags = lags,\n                           ahead = ahead,\n                           quantile_levels = c(0.1, 0.9))\n                         )$predictions |&gt;\n            pivot_quantiles_wider(.pred_distn)\n        }) |&gt;\n        list_rbind(),\n    .before = Inf,\n    .versions = fc_time_values\n    )}\n\npred_no_geo_pool_28 &lt;- pred_arx_no_geo_pool(usa_archive_dv$DT |&gt;\n                                              filter(geo_value %in% c(\"ca\", \"ma\", \"ny\", \"tx\")) |&gt;\n                                              as_epi_archive())"
  },
  {
    "objectID": "slides/day2-afternoon.html#predictions-without-geo-pooling-h28",
    "href": "slides/day2-afternoon.html#predictions-without-geo-pooling-h28",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions (without geo-pooling, \\(h=28\\))",
    "text": "Predictions (without geo-pooling, \\(h=28\\))\n\n\n\n          MAE     MASE  Coverage\nCA 0.06040473 202.5227 0.5510204\nMA 0.33532528 823.3607 0.3367347\nNY 0.21072571 516.3337 0.5408163\nTX 0.14144058 306.2029 0.4285714"
  },
  {
    "objectID": "slides/day2-afternoon.html#geo-pooling-or-not",
    "href": "slides/day2-afternoon.html#geo-pooling-or-not",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Geo-pooling or not?",
    "text": "Geo-pooling or not?\n\nGeo-pooled predictions tend to be more stable\nGenerally with wider intervals (and better coverage)\nMeanwhile, predictions from state-wise models tend to be more volatile\n\nThe extent to which this occurs differs based on the horizon.\nPreviously we studied \\(h=28\\). What happens for \\(h=7\\)?"
  },
  {
    "objectID": "slides/day2-afternoon.html#predictions-geo-pooling-h-7",
    "href": "slides/day2-afternoon.html#predictions-geo-pooling-h-7",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions (geo-pooling, \\(h = 7\\))",
    "text": "Predictions (geo-pooling, \\(h = 7\\))\n\n\n\n[1] 3\n\n\n\n\n          MAE     MASE  Coverage\nCA 0.04890282 162.2161 0.9693878\nMA 0.05944117 150.5289 0.8865979\nNY 0.05871580 144.5808 0.9072165\nTX 0.07368293 155.8373 0.8969072"
  },
  {
    "objectID": "slides/day2-afternoon.html#predictions-without-geo-pooling-h7",
    "href": "slides/day2-afternoon.html#predictions-without-geo-pooling-h7",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Predictions (without geo-pooling, \\(h=7\\))",
    "text": "Predictions (without geo-pooling, \\(h=7\\))\n\n\n\n          MAE     MASE  Coverage\nCA 0.03971988 131.7553 0.6734694\nMA 0.06579422 168.3530 0.5714286\nNY 0.05005395 124.5359 0.6836735\nTX 0.05756194 122.0824 0.7040816"
  },
  {
    "objectID": "slides/day2-afternoon.html#plot-our-forecasts",
    "href": "slides/day2-afternoon.html#plot-our-forecasts",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Plot our forecasts",
    "text": "Plot our forecasts"
  },
  {
    "objectID": "slides/day2-afternoon.html#ensembling-1",
    "href": "slides/day2-afternoon.html#ensembling-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ensembling",
    "text": "Ensembling\nInstead of choosing one model, we can combine the predictions from multiple base models. Ensemble types:\n\nuntrained: combine base models, agnostic to past performance\ntrained: weight base models, accounting for past performance\n\nSimplest untrained method: simple average of base model forecasts\n\\[\n\\hat{y}^{\\text{avg}}_{t+h|t} = \\frac{1}{p} \\sum_{j=1}^p \\hat{y}^j_{t+h|t}\n\\]\nA more robust option: simple median of base model forecasts\n\\[\n\\hat{y}^{\\text{med}}_{t+h|t} = \\mathrm{median}\\Big\\{ \\hat{y}^j_{t+h|t} : j = 1,\\dots,p \\Big\\}\n\\]"
  },
  {
    "objectID": "slides/day2-afternoon.html#example-from-the-covid-19-forecast-hub",
    "href": "slides/day2-afternoon.html#example-from-the-covid-19-forecast-hub",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Example from the Covid-19 Forecast Hub",
    "text": "Example from the Covid-19 Forecast Hub"
  },
  {
    "objectID": "slides/day2-afternoon.html#two-key-goals-of-ensembling",
    "href": "slides/day2-afternoon.html#two-key-goals-of-ensembling",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Two key goals of ensembling",
    "text": "Two key goals of ensembling\n1 Compete-with-best: ensemble should have accuracy competitive with best individual constituent model\n\nRobustness-over-all: ensemble should have greater robustness than any individual constituent model\n\nTypically these are hard to accomplish simultaneously, and untrained methods excel at point 2, whereas trained methods can achieve point 1"
  },
  {
    "objectID": "slides/day1-afternoon.html#down-with-spreadsheets-for-data-manipulation",
    "href": "slides/day1-afternoon.html#down-with-spreadsheets-for-data-manipulation",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Down with spreadsheets for data manipulation",
    "text": "Down with spreadsheets for data manipulation\n\nSpreadsheets make it difficult to rerun analyses consistently.\nUsing R (and {dplyr}) allows for:\n\nReproducibility\nEase of modification\n\nRecommendation: Avoid manual edits; instead, use code for transformations.\nLet’s see what we mean by this…"
  },
  {
    "objectID": "slides/day1-afternoon.html#introduction-to-dplyr",
    "href": "slides/day1-afternoon.html#introduction-to-dplyr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Introduction to dplyr",
    "text": "Introduction to dplyr\n\ndplyr is a powerful package in R for data manipulation.\nIt is part of the tidyverse, which includes a collection of packages designed to work together… Here’s some of its greatest hits:\n\n\n  Source"
  },
  {
    "objectID": "slides/day1-afternoon.html#introduction-to-dplyr-1",
    "href": "slides/day1-afternoon.html#introduction-to-dplyr-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Introduction to dplyr",
    "text": "Introduction to dplyr\nTo load dplyr, you may simply load the tidyverse package:\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nOur focus will be on basic operations like selecting and filtering data.\n\n\nSource"
  },
  {
    "objectID": "slides/day1-afternoon.html#downloading-jhu-csse-covid-19-case-data",
    "href": "slides/day1-afternoon.html#downloading-jhu-csse-covid-19-case-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Downloading JHU CSSE COVID-19 case data",
    "text": "Downloading JHU CSSE COVID-19 case data\nFirst, a detour to get a dataset for this section:\n\nlibrary(epidatr)\n\ncases_df_api &lt;- pub_covidcast(\n  source = \"jhu-csse\",\n  signals = \"confirmed_incidence_num\",\n  geo_type = \"state\",\n  time_type = \"day\",\n  geo_values = \"*\",\n  time_values = epirange(20220301, 20220331),\n)\n\nNow we only really need a few columns here…\n\ncases_df &lt;- cases_df_api %&gt;%\n  select(geo_value, time_value, raw_cases = value)\ncases_df\n\n# A tibble: 1,736 × 3\n   geo_value time_value raw_cases\n   &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n 1 ak        2022-03-01         0\n 2 al        2022-03-01       786\n 3 ar        2022-03-01       693\n 4 as        2022-03-01         0\n 5 az        2022-03-01         0\n 6 ca        2022-03-01      4310\n 7 co        2022-03-01       616\n 8 ct        2022-03-01       170\n 9 dc        2022-03-01       114\n10 de        2022-03-01        35\n# ℹ 1,726 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#downloading-jhu-csse-covid-19-case-data-1",
    "href": "slides/day1-afternoon.html#downloading-jhu-csse-covid-19-case-data-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Downloading JHU CSSE COVID-19 case data",
    "text": "Downloading JHU CSSE COVID-19 case data\nNow we only really need a few columns here… ::: {.cell layout-align=“center”}\n:::"
  },
  {
    "objectID": "slides/day1-afternoon.html#ways-to-inspect-the-dataset",
    "href": "slides/day1-afternoon.html#ways-to-inspect-the-dataset",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ways to inspect the dataset",
    "text": "Ways to inspect the dataset\nUse head() to view the first six row of the data ::: {.cell layout-align=“center”}\nhead(cases_df)  # First 6 rows\n\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n4 ca        2022-03-02      7044\n5 nc        2022-03-02      2243\n6 ny        2022-03-02      1889\n\n:::\nand tail to view the last six\n\n\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-30      3785\n2 nc        2022-03-30      1067\n3 ny        2022-03-30      3127\n4 ca        2022-03-31      4533\n5 nc        2022-03-31      1075\n6 ny        2022-03-31      4763"
  },
  {
    "objectID": "slides/day1-afternoon.html#ways-to-inspect-the-dataset-1",
    "href": "slides/day1-afternoon.html#ways-to-inspect-the-dataset-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Ways to inspect the dataset",
    "text": "Ways to inspect the dataset\nUse glimpse() to get a compact overview of the dataset.\n\nglimpse(cases_df)\n\nRows: 93\nColumns: 3\n$ geo_value  &lt;chr&gt; \"ca\", \"nc\", \"ny\", \"ca\", \"nc\", \"ny\", \"ca\", \"nc\", \"ny\", \"ca\",…\n$ time_value &lt;date&gt; 2022-03-01, 2022-03-01, 2022-03-01, 2022-03-02, 2022-03-02…\n$ raw_cases  &lt;dbl&gt; 4310, 1231, 1487, 7044, 2243, 1889, 7509, 2377, 2390, 3586,…"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-tibbles",
    "href": "slides/day1-afternoon.html#creating-tibbles",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Creating tibbles",
    "text": "Creating tibbles\n\nTibbles: Modern data frames with enhanced features.\nRows represent observations (or cases).\nColumns represent variables (or features).\nYou can create tibbles manually using the tibble() function.\n\n\ntibble(x = letters, y = 1:26)\n\n# A tibble: 26 × 2\n   x         y\n   &lt;chr&gt; &lt;int&gt;\n 1 a         1\n 2 b         2\n 3 c         3\n 4 d         4\n 5 e         5\n 6 f         6\n 7 g         7\n 8 h         8\n 9 i         9\n10 j        10\n# ℹ 16 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#selecting-columns-with-select",
    "href": "slides/day1-afternoon.html#selecting-columns-with-select",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nThe select() function is used to pick specific columns from your dataset.\n\nselect(cases_df, time_value, raw_cases)  # Select the 'time_value' and 'raw_cases' columns\n\n# A tibble: 93 × 2\n   time_value raw_cases\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2022-03-01      4310\n 2 2022-03-01      1231\n 3 2022-03-01      1487\n 4 2022-03-02      7044\n 5 2022-03-02      2243\n 6 2022-03-02      1889\n 7 2022-03-03      7509\n 8 2022-03-03      2377\n 9 2022-03-03      2390\n10 2022-03-04      3586\n# ℹ 83 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#selecting-columns-with-select-1",
    "href": "slides/day1-afternoon.html#selecting-columns-with-select-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nYou can exclude columns by prefixing the column names with a minus sign -.\n\nselect(cases_df, -geo_value)  # Exclude the 'geo_value' column from the dataset\n\n# A tibble: 93 × 2\n   time_value raw_cases\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2022-03-01      4310\n 2 2022-03-01      1231\n 3 2022-03-01      1487\n 4 2022-03-02      7044\n 5 2022-03-02      2243\n 6 2022-03-02      1889\n 7 2022-03-03      7509\n 8 2022-03-03      2377\n 9 2022-03-03      2390\n10 2022-03-04      3586\n# ℹ 83 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#extracting-columns-with-pull",
    "href": "slides/day1-afternoon.html#extracting-columns-with-pull",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Extracting columns with pull()",
    "text": "Extracting columns with pull()\n\npull(): Extract a column as a vector.\nLet’s try this with the cases column…\n\n\npull(cases_df, raw_cases)\n\n [1] 4310 1231 1487 7044 2243 1889 7509 2377 2390 3586 2646  350 1438    0 3372\n[16] 6465    0 2343 6690 4230 1033 3424  894 1025 4591 1833 1691 5359 1783 1747\n[31] 2713 1849 2229 1623    0 1396 5151    0 2202 4826 3130  982 1831  649 3128\n[46] 3706    0 2039 6143 2742 2356 4204 1740 2052 3256    0 2188 4659    0 2667\n[61] 5499 2508 1177 3004  819 1603 3943 1602  551 3550 1288 6596 1960 1224 3542\n[76] 1035    0    0 3384    0 5908 2811 2291 2286 1846  624 2394 3785 1067 3127\n[91] 4533 1075 4763"
  },
  {
    "objectID": "slides/day1-afternoon.html#subsetting-rows-with-filter",
    "href": "slides/day1-afternoon.html#subsetting-rows-with-filter",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Subsetting rows with filter()",
    "text": "Subsetting rows with filter()\n\n  Artwork by @allison_horst"
  },
  {
    "objectID": "slides/day1-afternoon.html#subsetting-rows-with-filter-1",
    "href": "slides/day1-afternoon.html#subsetting-rows-with-filter-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Subsetting rows with filter()",
    "text": "Subsetting rows with filter()\n\nThe filter() function allows you to subset rows that meet specific conditions.\nConditions regard column values, such as filtering for only NC or cases higher than some threshold.\nThis enables you to narrow down your dataset to focus on relevant data.\n\n\nfilter(cases_df, geo_value == \"nc\", raw_cases &gt; 500)  # Filter for NC with raw daily cases &gt; 500\n\n# A tibble: 22 × 3\n   geo_value time_value raw_cases\n   &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n 1 nc        2022-03-01      1231\n 2 nc        2022-03-02      2243\n 3 nc        2022-03-03      2377\n 4 nc        2022-03-04      2646\n 5 nc        2022-03-07      4230\n 6 nc        2022-03-08       894\n 7 nc        2022-03-09      1833\n 8 nc        2022-03-10      1783\n 9 nc        2022-03-11      1849\n10 nc        2022-03-14      3130\n# ℹ 12 more rows"
  },
  {
    "objectID": "slides/day1-afternoon.html#combining-select-and-filter-functions",
    "href": "slides/day1-afternoon.html#combining-select-and-filter-functions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Combining select() and filter() functions",
    "text": "Combining select() and filter() functions\n\nYou can further combine select() and filter() to further refine the dataset.\nUse select() to choose columns and filter() to narrow down rows.\nThis helps in extracting the exact data needed for analysis.\n\n\nselect(filter(cases_df, geo_value == \"nc\", raw_cases &gt; 500), time_value, raw_cases) |&gt;\n  head()\n\n# A tibble: 6 × 2\n  time_value raw_cases\n  &lt;date&gt;         &lt;dbl&gt;\n1 2022-03-01      1231\n2 2022-03-02      2243\n3 2022-03-03      2377\n4 2022-03-04      2646\n5 2022-03-07      4230\n6 2022-03-08       894"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-the-pipe-operator",
    "href": "slides/day1-afternoon.html#using-the-pipe-operator",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Using the pipe operator |>",
    "text": "Using the pipe operator |&gt;\n\nThe pipe operator (|&gt;) makes code more readable by chaining multiple operations together.\nThe output of one function is automatically passed to the next function.\nThis allows you to perform multiple steps (e.g., filter() followed by select()) in a clear and concise manner.\n\n\n# This code reads more like poetry!\ncases_df |&gt;\n  filter(geo_value == \"nc\", raw_cases &gt; 500) |&gt;\n  select(time_value, raw_cases) |&gt;\n  head()\n\n# A tibble: 6 × 2\n  time_value raw_cases\n  &lt;date&gt;         &lt;dbl&gt;\n1 2022-03-01      1231\n2 2022-03-02      2243\n3 2022-03-03      2377\n4 2022-03-04      2646\n5 2022-03-07      4230\n6 2022-03-08       894"
  },
  {
    "objectID": "slides/day1-afternoon.html#grouping-data-with-group_by",
    "href": "slides/day1-afternoon.html#grouping-data-with-group_by",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Grouping data with group_by()",
    "text": "Grouping data with group_by()\n\nUse group_by() to group data by one or more columns.\nAllows performing operations on specific groups of data.\n\n\ncases_df |&gt;\n  group_by(geo_value) |&gt;\n  filter(raw_cases == max(raw_cases, na.rm = TRUE))\n\n# A tibble: 3 × 3\n# Groups:   geo_value [3]\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-03      7509\n2 nc        2022-03-07      4230\n3 ny        2022-03-24      6596"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-new-columns-with-mutate",
    "href": "slides/day1-afternoon.html#creating-new-columns-with-mutate",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Creating new columns with mutate()",
    "text": "Creating new columns with mutate()\n\n  Artwork by @allison_horst"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-new-columns-with-mutate-1",
    "href": "slides/day1-afternoon.html#creating-new-columns-with-mutate-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Creating new columns with mutate()",
    "text": "Creating new columns with mutate()\n\nmutate() is used to create new columns.\nPerform calculations using existing columns and assign to new columns.\n\n\nny_subset = cases_df |&gt;\n  filter(geo_value == \"ny\")\n\nny_subset |&gt;\n  mutate(cumulative_cases = cumsum(raw_cases)) |&gt;\n  head()\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases cumulative_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1 ny        2022-03-01      1487             1487\n2 ny        2022-03-02      1889             3376\n3 ny        2022-03-03      2390             5766\n4 ny        2022-03-04       350             6116\n5 ny        2022-03-05      3372             9488\n6 ny        2022-03-06      2343            11831"
  },
  {
    "objectID": "slides/day1-afternoon.html#creating-new-columns-with-mutate-2",
    "href": "slides/day1-afternoon.html#creating-new-columns-with-mutate-2",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Creating new columns with mutate()",
    "text": "Creating new columns with mutate()\n\nmutate() can create multiple new columns in one step.\nLogical comparisons (e.g., over_5000 = raw_cases &gt; 5000) can be used within mutate().\n\n\nny_subset |&gt;\n  mutate(over_5000 = raw_cases &gt; 5000,\n         cumulative_cases = cumsum(raw_cases)) |&gt;\n  head()\n\n# A tibble: 6 × 5\n  geo_value time_value raw_cases over_5000 cumulative_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n1 ny        2022-03-01      1487 FALSE                 1487\n2 ny        2022-03-02      1889 FALSE                 3376\n3 ny        2022-03-03      2390 FALSE                 5766\n4 ny        2022-03-04       350 FALSE                 6116\n5 ny        2022-03-05      3372 FALSE                 9488\n6 ny        2022-03-06      2343 FALSE                11831"
  },
  {
    "objectID": "slides/day1-afternoon.html#combining-group_by-and-mutate",
    "href": "slides/day1-afternoon.html#combining-group_by-and-mutate",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Combining group_by() and mutate()",
    "text": "Combining group_by() and mutate()\n\nFirst, group data using group_by().\nThen, use mutate to perform the calculations for each group.\nFinally, use arrange to display the output by geo_value.\n\n\ncases_df |&gt;\n  group_by(geo_value) |&gt;\n  mutate(cumulative_cases = cumsum(raw_cases)) |&gt;\n  arrange(geo_value) |&gt;\n  head()\n\n# A tibble: 6 × 4\n# Groups:   geo_value [1]\n  geo_value time_value raw_cases cumulative_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1 ca        2022-03-01      4310             4310\n2 ca        2022-03-02      7044            11354\n3 ca        2022-03-03      7509            18863\n4 ca        2022-03-04      3586            22449\n5 ca        2022-03-05      1438            23887\n6 ca        2022-03-06      6465            30352"
  },
  {
    "objectID": "slides/day1-afternoon.html#conditional-calculations-with-if_else",
    "href": "slides/day1-afternoon.html#conditional-calculations-with-if_else",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Conditional calculations with if_else()",
    "text": "Conditional calculations with if_else()\n\nif_else() allows conditional logic within mutate().\nPerform different operations depending on conditions, like “high” or “low.”\n\n\nt &lt;- 5000\n\ncases_df |&gt;\n  mutate(high_low_cases = if_else(raw_cases &gt; t, \"high\", \"low\")) |&gt;\n  head()\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases high_low_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt; &lt;chr&gt;         \n1 ca        2022-03-01      4310 low           \n2 nc        2022-03-01      1231 low           \n3 ny        2022-03-01      1487 low           \n4 ca        2022-03-02      7044 high          \n5 nc        2022-03-02      2243 low           \n6 ny        2022-03-02      1889 low"
  },
  {
    "objectID": "slides/day1-afternoon.html#summarizing-data-with-summarise",
    "href": "slides/day1-afternoon.html#summarizing-data-with-summarise",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Summarizing data with summarise()",
    "text": "Summarizing data with summarise()\n\nsummarise() reduces data to summary statistics (e.g., mean, median).\nTypically used after group_by() to summarize each group.\n\n\ncases_df |&gt;\n  group_by(geo_value) |&gt;\n  summarise(median_cases = median(raw_cases))\n\n# A tibble: 3 × 2\n  geo_value median_cases\n  &lt;chr&gt;            &lt;dbl&gt;\n1 ca                3785\n2 nc                1224\n3 ny                2188"
  },
  {
    "objectID": "slides/day1-afternoon.html#using-count-to-aggregate-data",
    "href": "slides/day1-afternoon.html#using-count-to-aggregate-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Using count() to aggregate data",
    "text": "Using count() to aggregate data\ncount() is a shortcut for grouping and summarizing the data.\nFor example, if we want to get the total number of complete rows for each state, then ::: {.cell layout-align=“center”}\ncases_count &lt;- cases_df |&gt;\n  drop_na() |&gt; # Removes rows where any value is missing (from tidyr)\n  group_by(geo_value) |&gt;\n  summarize(count = n())\n:::  is equivalent to\n\ncases_count &lt;- cases_df |&gt;\n  drop_na() |&gt;\n  count(geo_value)\n\ncases_count # Let's see what the counts are.\n\n# A tibble: 3 × 2\n  geo_value     n\n  &lt;chr&gt;     &lt;int&gt;\n1 ca           31\n2 nc           31\n3 ny           31"
  },
  {
    "objectID": "slides/day1-afternoon.html#key-practices-learned",
    "href": "slides/day1-afternoon.html#key-practices-learned",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Key practices learned",
    "text": "Key practices learned\n\nUse head() and tail() for quick data inspection.\nUse select() and filter() for data manipulation.\nChain functions with |&gt;.\nUse group_by() to group data by one or more variables before applying functions.\nUse mutate to create new columns or modify existing ones by applying functions to existing data.\nUse summarise to reduce data to summary statistics (e.g., mean, median)."
  },
  {
    "objectID": "slides/day1-afternoon.html#to-2-word-summaries-of-the-dplyr-functions",
    "href": "slides/day1-afternoon.html#to-2-word-summaries-of-the-dplyr-functions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "1 to 2 word summaries of the dplyr functions",
    "text": "1 to 2 word summaries of the dplyr functions\nHere are 1 to 2 word summaries of the key dplyr functions:\n\nselect(): Choose columns\nfilter(): Subset rows\nmutate(): Create columns\ngroup_by(): Group by\nsummarise(): Numerical summary"
  },
  {
    "objectID": "slides/day1-afternoon.html#tidy-data-and-tolstoy",
    "href": "slides/day1-afternoon.html#tidy-data-and-tolstoy",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Tidy data and Tolstoy",
    "text": "Tidy data and Tolstoy\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.” — Leo Tolstoy\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/day1-afternoon.html#what-is-tidy-data",
    "href": "slides/day1-afternoon.html#what-is-tidy-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "What is tidy data?",
    "text": "What is tidy data?\n\nTidy data follows a consistent structure: each row represents one observation, and each column represents one variable.\ncases_df is one classic example of tidy data.\n\n\n\n# A tibble: 6 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n4 ca        2022-03-02      7044\n5 nc        2022-03-02      2243\n6 ny        2022-03-02      1889\n\n\n\nTo convert between tidy and messy data, we can use the tidyr package in the tidyverse."
  },
  {
    "objectID": "slides/day1-afternoon.html#pivot_wider-and-pivot_longer",
    "href": "slides/day1-afternoon.html#pivot_wider-and-pivot_longer",
    "title": "MICOM EpiData Workshop 2025",
    "section": "pivot_wider() and pivot_longer()",
    "text": "pivot_wider() and pivot_longer()\n\n  Artwork by @allison_horst"
  },
  {
    "objectID": "slides/day1-afternoon.html#making-data-wider-with-pivot_wider",
    "href": "slides/day1-afternoon.html#making-data-wider-with-pivot_wider",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Making data wider with pivot_wider()",
    "text": "Making data wider with pivot_wider()\n\nTo convert data from long format to wide/messy format use pivot_wider().\nFor example, let’s try creating a column for each time value in cases_df:\n\n\nmessy_cases_df &lt;- cases_df |&gt;\n  pivot_wider(\n    names_from = time_value,   # Create new columns for each unique date\n    values_from = raw_cases    # Fill those columns with the raw_case values\n  )\n\n# View the result\nmessy_cases_df\n\n# A tibble: 3 × 32\n  geo_value `2022-03-01` `2022-03-02` `2022-03-03` `2022-03-04` `2022-03-05`\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 ca                4310         7044         7509         3586         1438\n2 nc                1231         2243         2377         2646            0\n3 ny                1487         1889         2390          350         3372\n# ℹ 26 more variables: `2022-03-06` &lt;dbl&gt;, `2022-03-07` &lt;dbl&gt;,\n#   `2022-03-08` &lt;dbl&gt;, `2022-03-09` &lt;dbl&gt;, `2022-03-10` &lt;dbl&gt;,\n#   `2022-03-11` &lt;dbl&gt;, `2022-03-12` &lt;dbl&gt;, `2022-03-13` &lt;dbl&gt;,\n#   `2022-03-14` &lt;dbl&gt;, `2022-03-15` &lt;dbl&gt;, `2022-03-16` &lt;dbl&gt;,\n#   `2022-03-17` &lt;dbl&gt;, `2022-03-18` &lt;dbl&gt;, `2022-03-19` &lt;dbl&gt;,\n#   `2022-03-20` &lt;dbl&gt;, `2022-03-21` &lt;dbl&gt;, `2022-03-22` &lt;dbl&gt;,\n#   `2022-03-23` &lt;dbl&gt;, `2022-03-24` &lt;dbl&gt;, `2022-03-25` &lt;dbl&gt;, …"
  },
  {
    "objectID": "slides/day1-afternoon.html#tidying-messy-data-with-pivot_longer",
    "href": "slides/day1-afternoon.html#tidying-messy-data-with-pivot_longer",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Tidying messy data with pivot_longer()",
    "text": "Tidying messy data with pivot_longer()\n\nUse pivot_longer() to convert data from wide format (multiple columns for the same variable) to long format (one column per variable).\nLet’s try turning messy_cases_df back into the original tidy cases_df!\n\n\ntidy_cases_df &lt;- messy_cases_df |&gt;\n  pivot_longer(\n    cols = -geo_value,          # Keep the 'geo_value' column as it is\n    names_to = \"time_value\",    # Create a new 'time_value' column from the column names\n    values_to = \"raw_cases\"     # Values from the wide columns should go into 'raw_cases'\n  )\n\n# View the result\nhead(tidy_cases_df, n = 3) # Notice the class of time_value here\n\n# A tibble: 3 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 ca        2022-03-02      7044\n3 ca        2022-03-03      7509"
  },
  {
    "objectID": "slides/day1-afternoon.html#introduction-to-joins-in-dplyr",
    "href": "slides/day1-afternoon.html#introduction-to-joins-in-dplyr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Introduction to joins in dplyr",
    "text": "Introduction to joins in dplyr\n\nJoining datasets is a powerful tool for combining info. from multiple sources.\nIn R, dplyr provides several functions to perform different types of joins.\nWe’ll demonstrate joining a subset of cases_df (our case counts dataset) with state_census.\nMotivation: We can scale the case counts by population to make them comparable across regions of different sizes."
  },
  {
    "objectID": "slides/day1-afternoon.html#subset-cases_df",
    "href": "slides/day1-afternoon.html#subset-cases_df",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Subset cases_df",
    "text": "Subset cases_df\nTo simplify things, let’s use filter() to only grab one date of cases_df: ::: {.cell layout-align=“center”}\ncases_df_sub &lt;- cases_df |&gt; filter(time_value == \"2022-03-01\")\ncases_df_sub\n\n# A tibble: 3 × 3\n  geo_value time_value raw_cases\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 ca        2022-03-01      4310\n2 nc        2022-03-01      1231\n3 ny        2022-03-01      1487\n\n:::\nThough note that what we’re going to do can be applied to the entirety of cases_df."
  },
  {
    "objectID": "slides/day1-afternoon.html#load-state-census-data",
    "href": "slides/day1-afternoon.html#load-state-census-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Load state census data",
    "text": "Load state census data\nThe state_census dataset from epidatasets contains state populations from the 2019 census.\nNotice that this includes many states that are not in cases_df_sub."
  },
  {
    "objectID": "slides/day1-afternoon.html#left-join-keep-all-rows-from-the-first-dataset",
    "href": "slides/day1-afternoon.html#left-join-keep-all-rows-from-the-first-dataset",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Left Join: Keep all rows from the first dataset",
    "text": "Left Join: Keep all rows from the first dataset\n\nA left join keeps all rows from the first dataset (cases_df_sub), and adds matching data from the second dataset (state_census).\nSo all rows from the first dataset (cases_df_sub) will be preserved.\nThe datasets are joined by matching the geo_value column, specified by the by argument.\n\n\n# Left join: combining March 1, 2022 state case data with the census data\ncases_sub_left_join &lt;- cases_df_sub |&gt;\n  left_join(state_census, join_by(geo_value == abbr))\n\ncases_sub_left_join\n\n# A tibble: 3 × 4\n  geo_value time_value raw_cases      pop\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561"
  },
  {
    "objectID": "slides/day1-afternoon.html#right-join-keep-all-rows-from-the-second-dataset",
    "href": "slides/day1-afternoon.html#right-join-keep-all-rows-from-the-second-dataset",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Right Join: Keep all rows from the second dataset",
    "text": "Right Join: Keep all rows from the second dataset\n\nA right join keeps all rows from the second dataset (state_census), and adds matching data from the first dataset (cases_df_sub).\nIf a row in the second dataset doesn’t have a match in the first, then the columns from the first will be filled with NA.\nFor example, can see this for the al row from state_census…\n\n\n# Right join: keep all rows from state_census\ncases_sub_right_join &lt;- cases_df_sub |&gt;\n  right_join(state_census, join_by(geo_value == abbr))\n\nhead(cases_sub_right_join)\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases      pop\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n4 al        NA                NA  4903185\n5 ak        NA                NA   731545\n6 az        NA                NA  7278717"
  },
  {
    "objectID": "slides/day1-afternoon.html#inner-join-only-keep-matching-rows",
    "href": "slides/day1-afternoon.html#inner-join-only-keep-matching-rows",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Inner Join: Only keep matching rows",
    "text": "Inner Join: Only keep matching rows\n\nAn inner join will only keep rows where there is a match in both datasets.\nSo, if a state in state_census does not have a corresponding entry in cases_df_sub, then that row will be excluded. ::: {.cell layout-align=“center”}\n\n# Inner join: only matching rows are kept\ncases_sub_inner_join &lt;- cases_df_sub |&gt;\n  inner_join(state_census, join_by(geo_value == abbr))\n\ncases_sub_inner_join\n\n# A tibble: 3 × 4\n  geo_value time_value raw_cases      pop\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n\n:::"
  },
  {
    "objectID": "slides/day1-afternoon.html#full-join-keep-all-rows-from-both-datasets",
    "href": "slides/day1-afternoon.html#full-join-keep-all-rows-from-both-datasets",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Full Join: Keep all rows from both datasets",
    "text": "Full Join: Keep all rows from both datasets\n\nA full join will keep all rows from both datasets.\nIf a state in either dataset has no match in the other, the missing values will be filled with NA. ::: {.cell layout-align=“center”}\n\n# Full join: keep all rows from both datasets\ncases_sub_full_join &lt;- cases_df_sub |&gt;\n  full_join(state_census, join_by(geo_value == abbr))\n\nhead(cases_sub_full_join)\n\n# A tibble: 6 × 4\n  geo_value time_value raw_cases      pop\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 ca        2022-03-01      4310 39512223\n2 nc        2022-03-01      1231 10488084\n3 ny        2022-03-01      1487 19453561\n4 al        NA                NA  4903185\n5 ak        NA                NA   731545\n6 az        NA                NA  7278717\n\n:::"
  },
  {
    "objectID": "slides/day1-afternoon.html#pictorial-summary-of-the-four-join-functions",
    "href": "slides/day1-afternoon.html#pictorial-summary-of-the-four-join-functions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Pictorial summary of the four join functions",
    "text": "Pictorial summary of the four join functions\n\nLeft join: All rows from the left dataset and matching rows from the right dataset.\nRight join: All rows from the right dataset and matching rows from the left dataset.\nInner join: Only matching rows from both datasets.\nFull join: All rows from both datasets, with NA where no match exists.\n\n\n\nSource"
  },
  {
    "objectID": "slides/day1-afternoon.html#final-thoughts-on-joins",
    "href": "slides/day1-afternoon.html#final-thoughts-on-joins",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Final thoughts on joins",
    "text": "Final thoughts on joins\n\nJoins are an essential part of data wrangling in R.\nThe choice of join depends on the analysis you need to perform:\n\nUse left joins when you want to keep all data from the first dataset.\nUse right joins when you want to keep all data from the second dataset.\nUse inner joins when you’re only interested in matching rows.\nUse full joins when you want to preserve all information from both datasets."
  },
  {
    "objectID": "slides/day1-afternoon.html#two-review-questions",
    "href": "slides/day1-afternoon.html#two-review-questions",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Two review questions",
    "text": "Two review questions\nQ1): What join function should you use if your goal is to scale the cases by population in cases_df?\nQ2): Lastly, please create a new column in cases_df where you scale the cases by population and multiply by 1e5 to get cases / 100k.\nCongratulations for making it through this crash course! That’s all for this glimpse() into the tidyverse."
  },
  {
    "objectID": "slides/day1-afternoon.html#backfill-american-edition",
    "href": "slides/day1-afternoon.html#backfill-american-edition",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Backfill American edition",
    "text": "Backfill American edition\n\nAgain, we can see a similar systematic underestimation problem for COVID-19 mortality rates in CA.\nThis plot also illustrates the revision process - how the reported mortality changes & increases across multiple updates until it stabilizes at the final value (black line).\n\n\n\nThese two examples show the problem and now we need a solution…"
  },
  {
    "objectID": "slides/day1-afternoon.html#nowcasting-moving-from-one-predictor-to-multiple",
    "href": "slides/day1-afternoon.html#nowcasting-moving-from-one-predictor-to-multiple",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Nowcasting: Moving from one predictor to multiple",
    "text": "Nowcasting: Moving from one predictor to multiple\n\nThe ratio model predicts the finalized value of \\(Y_t\\) from \\(Y_{s}\\), the last value included in the version \\(t\\) report.\n\\(Y_s\\) is the closest in time we can get to \\(Y_t\\), but we also expect it to be the least reliable value in version \\(t\\).\nCan we add \\(Y_{s - 1}\\), \\(Y_{s - 2}\\), and even other data sources to the model to try to find a good mix of relevant and reliable signals?\nRegressions models will let us do that.\nLet’s start “simple”: predicting \\(Y_t\\) with whichever of \\(Y_{t - 1}\\) and \\(Y_{t -\n2}\\) are available in version \\(t\\)."
  },
  {
    "objectID": "slides/day1-afternoon.html#start-with-a-single-nowcast-date",
    "href": "slides/day1-afternoon.html#start-with-a-single-nowcast-date",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Start with a single nowcast date",
    "text": "Start with a single nowcast date\nWe’re only looking at California in this example: ::: {.cell layout-align=“center”}\nnchs_ca_archive &lt;- nchs_archive$DT[geo_value == \"ca\",] |&gt;\n  as_epi_archive()\n:::\nWe’ll start experimenting with just a single nowcast date:\n\nEarlier nowcast dates often encounter extra difficulties regarding data availability, so let’s make sure we work on the first one.\n\n\ntrial_nowcast_date &lt;- all_nowcast_dates[[1]]\n\nWhat data would we have had available then? ::: {.cell layout-align=“center”}\n# This is the version history we'd have seen at that point:\nnchs_ca_past_archive &lt;- nchs_ca_archive |&gt;\n  epix_as_of(trial_nowcast_date, all_versions = TRUE)\n\n# And this is what the latest version was at that point:\nnchs_ca_past_latest &lt;- nchs_ca_past_archive |&gt;\n  epix_as_of(trial_nowcast_date)\n:::"
  },
  {
    "objectID": "slides/day1-afternoon.html#what-predictors-were-available-at-test-time",
    "href": "slides/day1-afternoon.html#what-predictors-were-available-at-test-time",
    "title": "MICOM EpiData Workshop 2025",
    "section": "What predictors were available at test time?",
    "text": "What predictors were available at test time?\n\n# At version t, our target is finalized Y_t:\ntarget_time_value &lt;- trial_nowcast_date\n\n# Check which of Y_{t-1} and Y_{t-2} are available, assign distinct names:\npredictor_descriptions &lt;- nchs_ca_past_latest |&gt;\n  filter(as.integer(target_time_value - time_value) &lt;= 2 * 7) |&gt;\n  drop_na(mortality) |&gt;\n  transmute(\n    varname = \"mortality\",\n    lag_days = as.integer(trial_nowcast_date - time_value),\n    predictor_name = paste0(varname, \"_lag\", lag_days, \"_realtime\")\n  )\npredictor_descriptions\n\n# A tibble: 2 × 3\n  varname   lag_days predictor_name          \n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;                   \n1 mortality       14 mortality_lag14_realtime\n2 mortality        7 mortality_lag7_realtime"
  },
  {
    "objectID": "slides/day1-afternoon.html#line-up-with-training-data",
    "href": "slides/day1-afternoon.html#line-up-with-training-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Line up with training data",
    "text": "Line up with training data\nWe need to make sure to line up our predictors in nchs_ca_past_latest with training data that is analogous (e.g., “equally unreliable”).\n\nlibrary(data.table)\nget_predictor_training_data &lt;- function(archive, varname, lag_days, predictor_name) {\n  ...\n  ...\n  ...\n  return (training_data_edf_for_this_predictor)\n}\n\nActual implementation: ::: {.cell layout-align=“center”}\n\nCode\nlibrary(data.table)\nget_predictor_training_data &lt;- function(archive, varname, lag_days, predictor_name) {\n  epikeytime_names &lt;- setdiff(key(archive$DT), \"version\")\n  requests &lt;- unique(archive$DT, by = epikeytime_names, cols = character())[\n  , version := time_value + ..lag_days\n  ]\n  setkeyv(requests, c(epikeytime_names, \"version\"))\n  result &lt;- archive$DT[\n    requests, c(key(archive$DT), varname), roll = TRUE, nomatch = NULL, allow.cartesian = TRUE, with = FALSE\n  ][\n  , time_value := version\n  ][\n  , version := NULL\n  ]\n  nms &lt;- names(result)\n  nms[[match(varname, nms)]] &lt;- predictor_name\n  setnames(result, nms)\n  setDF(result)\n  as_tibble(result)\n}\n\n:::\n\nget_predictor_training_data(nchs_ca_past_archive, \"mortality\", 7, \"mortality_lag7_realtime\")\n\n# A tibble: 36 × 3\n   geo_value time_value mortality_lag7_realtime\n   &lt;chr&gt;     &lt;date&gt;                       &lt;dbl&gt;\n 1 ca        2020-12-06                      13\n 2 ca        2021-02-28                      78\n 3 ca        2021-03-07                      54\n 4 ca        2021-03-14                      43\n 5 ca        2021-03-21                      19\n 6 ca        2021-03-28                      21\n 7 ca        2021-04-04                      23\n 8 ca        2021-04-11                      20\n 9 ca        2021-04-18                      18\n10 ca        2021-04-25                      20\n# ℹ 26 more rows\n\n\nThe first value here is a version of \\(Y_{\\text{2020-11-30}}\\) as it was reported in version 2020-12-06. We expect it to have similar characteristics as \\(Y_{t - 7\\text{ days}}\\) as reported in version \\(t\\) for other values of \\(t\\)."
  },
  {
    "objectID": "slides/day1-afternoon.html#get-multiple-predictors",
    "href": "slides/day1-afternoon.html#get-multiple-predictors",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Get multiple predictors",
    "text": "Get multiple predictors\n\npredictors &lt;- predictor_descriptions |&gt;\n  pmap(function(varname, lag_days, predictor_name) {\n    get_predictor_training_data(nchs_ca_past_archive, varname, lag_days, predictor_name)\n  }) |&gt;\n  reduce(full_join, by = c(\"geo_value\", \"time_value\"))\npredictors\n\n# A tibble: 50 × 4\n   geo_value time_value mortality_lag14_realtime mortality_lag7_realtime\n   &lt;chr&gt;     &lt;date&gt;                        &lt;dbl&gt;                   &lt;dbl&gt;\n 1 ca        2020-12-06                       66                      13\n 2 ca        2020-12-13                       13                      NA\n 3 ca        2021-02-14                      557                      NA\n 4 ca        2021-02-21                      474                      NA\n 5 ca        2021-02-28                      478                      78\n 6 ca        2021-03-07                      415                      54\n 7 ca        2021-03-14                      282                      43\n 8 ca        2021-03-21                      176                      19\n 9 ca        2021-03-28                      164                      21\n10 ca        2021-04-04                      117                      23\n# ℹ 40 more rows\n\n\n\nA full join is nice to show differences in missingness\nBut before training we’re going to drop_na() and end up with something more like an inner join"
  },
  {
    "objectID": "slides/day1-afternoon.html#combine-with-target-data",
    "href": "slides/day1-afternoon.html#combine-with-target-data",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Combine with target data",
    "text": "Combine with target data\n\ntarget &lt;- nchs_ca_past_latest |&gt;\n  filter(time_value &lt;= max(time_value) - 49) |&gt;\n  rename(mortality_semistable = mortality)\n\nFor each training time \\(t'\\), approximate finalized \\(Y_{t'}\\) with \\(Y_{t'}\\) as reported at our trial nowcast date \\(t\\). * Based on earlier analysis, we shouldn’t really trust this for \\(t'\\) within 49 days of \\(t\\), so filter those training times out."
  },
  {
    "objectID": "slides/day1-afternoon.html#fit-the-regression-model",
    "href": "slides/day1-afternoon.html#fit-the-regression-model",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Fit the regression model",
    "text": "Fit the regression model\n\ntraining_test &lt;- full_join(predictors, target, by = c(\"geo_value\", \"time_value\"))\n\ntraining &lt;- training_test |&gt; drop_na()\ntest &lt;- training_test |&gt; filter(time_value == trial_nowcast_date)\n\nfit &lt;- training |&gt;\n  select(all_of(predictor_descriptions$predictor_name), mortality_semistable) |&gt;\n  lm(formula = mortality_semistable ~ .)\n\npred &lt;- tibble(\n  nowcast_date = trial_nowcast_date,\n  target_date = target_time_value,\n  prediction = unname(predict(fit, test))\n)\n\npred\n\n# A tibble: 1 × 3\n  nowcast_date target_date prediction\n  &lt;date&gt;       &lt;date&gt;           &lt;dbl&gt;\n1 2022-01-02   2022-01-02        483."
  },
  {
    "objectID": "slides/day1-afternoon.html#our-prediction",
    "href": "slides/day1-afternoon.html#our-prediction",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Our prediction",
    "text": "Our prediction\n\npred\n\n# A tibble: 1 × 3\n  nowcast_date target_date prediction\n  &lt;date&gt;       &lt;date&gt;           &lt;dbl&gt;\n1 2022-01-02   2022-01-02        483."
  },
  {
    "objectID": "slides/day1-afternoon.html#backtesting-our-nowcaster",
    "href": "slides/day1-afternoon.html#backtesting-our-nowcaster",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Backtesting our nowcaster",
    "text": "Backtesting our nowcaster\nWe’ll wrap our nowcasting code in a function and epix_slide() again.\n\nAnd get an error — some versions \\(t\\) don’t include a value \\(Y_{t-1}\\) or \\(Y_{t-2}\\) (e.g., version 2022-06-26 doesn’t).\n\nSo let’s try looking farther into the past at \\(Y_{t-3}\\), etc.\n… but don’t look too far: \\(Y_{t-5}\\) is the limit.\nThe same regression approach applies to models with 3 or more features.\nIncluding more features tends to improve performance, up to a point."
  },
  {
    "objectID": "slides/day1-afternoon.html#some-other-modifications",
    "href": "slides/day1-afternoon.html#some-other-modifications",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Some other modifications",
    "text": "Some other modifications\n\nAdd some basic checks throughout our nowcasting function.\nMake sure we have “enough” training data to fit a model.\nAdd ability to look not just at provisional \\(Y_{t-k}\\), but also provisional \\(Z_{t-k}\\) for some other signal \\(Z\\).\n\n\\(Z\\) here is HHS/NHSN COVID-19 hospitalization reporting.\n\nThis was daily-resolution and daily-reporting-cadence for some time; it’s possible but a bit tricky to combine with our weekly-resolution weekly-cadence archive.\n\nExclude a potential predictor if it doesn’t have much training data available.\n\nAllow for linear regression or quantile regression at the median level (tau = 0.5)\n\n\n\nCode\nregression_nowcaster &lt;- function(archive, settings, return_info = FALSE) {\n  if (!inherits(archive, \"epi_archive\")) {\n    stop(\"`archive` isn't an `epi_archive`\")\n  }\n  if (length(unique(archive$DT$geo_value)) != 1L) {\n    stop(\"Expected exactly one unique `geo_value`\")\n  }\n  if (archive$time_type == \"day\") {\n    archive &lt;- thin_daily_to_weekly_archive(archive)\n  }\n\n  nowcast_date &lt;- archive$versions_end\n  target_time_value &lt;- nowcast_date\n  latest_edf &lt;- archive |&gt; epix_as_of(nowcast_date)\n  # print(nowcast_date)\n\n  predictor_descriptions &lt;-\n    latest_edf |&gt;\n    mutate(lag_days = as.integer(nowcast_date - time_value)) |&gt;\n    select(-c(geo_value, time_value)) |&gt;\n    pivot_longer(-lag_days, names_to = \"varname\", values_to = \"value\") |&gt;\n    drop_na(value) |&gt;\n    inner_join(settings$predictors, by = \"varname\", unmatched = \"error\") |&gt;\n    filter(abs(lag_days) &lt;= max_abs_shift_days) |&gt;\n    arrange(varname, abs(lag_days)) |&gt;\n    group_by(varname) |&gt;\n    filter(seq_len(n()) &lt;= max_n_shifts[[1]]) |&gt;\n    ungroup() |&gt;\n    mutate(predictor_name = paste0(varname, \"_lag\", lag_days, \"_realtime\")) |&gt;\n    select(varname, lag_days, predictor_name)\n\n  predictor_edfs &lt;- predictor_descriptions |&gt;\n    pmap(function(varname, lag_days, predictor_name) {\n      get_predictor_training_data(archive, varname, lag_days, predictor_name)\n    }) |&gt;\n    lapply(na.omit) |&gt;\n    keep(~ nrow(.x) &gt;= settings$min_n_training_per_predictor)\n\n  if (length(predictor_edfs) == 0) {\n    stop(\"Couldn't find acceptable predictors in the latest data.\")\n  }\n\n  predictors &lt;- predictor_edfs |&gt;\n    reduce(full_join, by = c(\"geo_value\", \"time_value\"))\n\n  target &lt;- latest_edf |&gt;\n    filter(time_value &lt;= max(time_value) - settings$days_until_target_semistable) |&gt;\n    select(geo_value, time_value, mortality_semistable = mortality)\n\n  training_test &lt;- full_join(predictors, target, by = c(\"geo_value\", \"time_value\"))\n\n  training &lt;- training_test |&gt;\n    drop_na() |&gt;\n    slice_max(time_value, n = settings$max_n_training_intersection)\n\n  test &lt;- training_test |&gt;\n    filter(time_value == nowcast_date)\n\n  if (isTRUE(settings$median)) {\n    fit &lt;- training |&gt;\n      select(any_of(predictor_descriptions$predictor_name), mortality_semistable) |&gt;\n      quantreg::rq(formula = mortality_semistable ~ ., tau = 0.5)\n  } else {\n    fit &lt;- training |&gt;\n      select(any_of(predictor_descriptions$predictor_name), mortality_semistable) |&gt;\n      lm(formula = mortality_semistable ~ .)\n  }\n\n  pred &lt;- tibble(\n    geo_value = \"ca\",\n    nowcast_date = nowcast_date,\n    target_date = target_time_value,\n    prediction = unname(predict(fit, test))\n  )\n\n  if (return_info) {\n    return(tibble(\n      coefficients = list(coef(fit)),\n      predictions = list(pred)\n    ))\n  } else {\n    return(pred)\n  }\n}\n\n# We can apply this separately for each nowcast_date to ensure that we consider\n# the latest possible value for every signal, though whether that is advisable\n# or not may depend on revision characteristics of the signals.\nthin_daily_to_weekly_archive &lt;- function(archive) {\n  key_nms &lt;- key(archive$DT)\n  val_nms &lt;- setdiff(names(archive$DT), key_nms)\n  update_tbl &lt;- as_tibble(archive$DT)\n  val_nms |&gt;\n    lapply(function(val_nm) {\n      update_tbl[c(key_nms, val_nm)] |&gt;\n        # thin out to weekly, making sure that we keep the max time_value with non-NA value:\n        filter(as.POSIXlt(time_value)$wday == as.POSIXlt(max(time_value[!is.na(.data[[val_nm]])]))$wday) |&gt;\n        # re-align:\n        mutate(\n          time_value = time_value - as.POSIXlt(time_value)$wday, # Sunday of same epiweek\n          old_version = version,\n          version = version - as.POSIXlt(version)$wday # Sunday of same epiweek\n        ) |&gt;\n        slice_max(old_version, by = all_of(key_nms)) |&gt;\n        select(-old_version) |&gt;\n        as_epi_archive(other_keys = setdiff(key_nms, c(\"geo_value\", \"time_value\", \"version\")),\n                       compactify = TRUE)\n    }) |&gt;\n    reduce(epix_merge, sync = \"locf\")\n}\n\n# Baseline model:\nlocf_nowcaster &lt;- function(archive) {\n  nowcast_date &lt;- archive$versions_end\n  target_time_value &lt;- nowcast_date\n  latest_edf &lt;- archive |&gt; epix_as_of(nowcast_date)\n\n  latest_edf |&gt;\n    complete(geo_value, time_value = target_time_value) |&gt;\n    arrange(geo_value, time_value) |&gt;\n    group_by(geo_value) |&gt;\n    fill(mortality) |&gt;\n    ungroup() |&gt;\n    filter(time_value == target_time_value) |&gt;\n    transmute(\n      geo_value,\n      nowcast_date = nowcast_date,\n      target_date = time_value,\n      prediction = mortality\n    )\n}"
  },
  {
    "objectID": "slides/day1-afternoon.html#model-settings",
    "href": "slides/day1-afternoon.html#model-settings",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Model settings",
    "text": "Model settings\nAfter fixing, enhancing, and parameterizing our regression nowcaster, we’ll compare two different configurations:\n\none with just mortality-based predictions\none that also uses hospitalizations as a predictor\nand two that use quantile reg instead of linear reg"
  },
  {
    "objectID": "slides/day1-afternoon.html#model-settings-1",
    "href": "slides/day1-afternoon.html#model-settings-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Model settings",
    "text": "Model settings\n\nreg1_settings &lt;- list(\n  predictors = tribble(\n    ~varname,    ~max_abs_shift_days, ~max_n_shifts,\n    \"mortality\",                  35,             3,\n    ),\n  min_n_training_per_predictor = 30, # or else exclude predictor\n  days_until_target_semistable = 7 * 7, # filter out unstable when training (and evaluating)\n  min_n_training_intersection = 20, # or else raise error\n  max_n_training_intersection = Inf # or else filter down rows\n)\n\nreg2_settings &lt;- list(\n  predictors = tribble(\n    ~varname,     ~max_abs_shift_days, ~max_n_shifts,\n    \"admissions\",                  35,             3,\n    \"mortality\",                   35,             3,\n    ),\n  min_n_training_per_predictor = 30, # or else exclude predictor\n  days_until_target_semistable = 7 * 7, # filter out unstable when training (and evaluating)\n  min_n_training_intersection = 20, # or else raise error\n  max_n_training_intersection = Inf # or else filter down rows\n)\n\nreg3_settings &lt;- c(reg1_settings, median = TRUE)\nreg4_settings &lt;- c(reg2_settings, median = TRUE)"
  },
  {
    "objectID": "slides/day1-afternoon.html#comparison-linear-regression",
    "href": "slides/day1-afternoon.html#comparison-linear-regression",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Comparison: linear regression",
    "text": "Comparison: linear regression"
  },
  {
    "objectID": "slides/day1-afternoon.html#comparison-quantile-regression",
    "href": "slides/day1-afternoon.html#comparison-quantile-regression",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Comparison: quantile regression",
    "text": "Comparison: quantile regression"
  },
  {
    "objectID": "slides/day1-afternoon.html#evaluations",
    "href": "slides/day1-afternoon.html#evaluations",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Evaluations",
    "text": "Evaluations\n\nn_models &lt;- length(unique(nowcast_comparison$Nowcaster))\nnowcast_comparison |&gt;\n  # Filter evaluation based on target stability\n  filter(target_date &lt;= nchs_ca_archive$versions_end - 49) |&gt;\n  # Filter evaluated tasks to those with all models available\n  group_by(target_date) |&gt;\n  filter(sum(!is.na(prediction)) == n_models) |&gt;\n  ungroup() |&gt;\n  summarize(.by = Nowcaster,\n            MAE = mean(abs(mortality - prediction)),\n            MAPE = 100*mean(abs(mortality - prediction)/abs(mortality)))\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Nowcaster &lt;chr&gt;, MAE &lt;dbl&gt;, MAPE &lt;dbl&gt;"
  },
  {
    "objectID": "slides/day1-afternoon.html#mea-culpa",
    "href": "slides/day1-afternoon.html#mea-culpa",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Mea culpa",
    "text": "Mea culpa\nThis quickly became complicated and we’ve glossed over some core concepts. We’ll explain concepts of regression, lagged features, and evaluation more carefully tomorrow."
  },
  {
    "objectID": "slides/day1-afternoon.html#aside-on-nowcasting",
    "href": "slides/day1-afternoon.html#aside-on-nowcasting",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Aside on nowcasting",
    "text": "Aside on nowcasting\n\nTo some Epis, “nowcasting” can be equated with “estimate the time-varying instantaneous reproduction number, \\(R_t\\)”\nEx. using the number of reported COVID-19 cases in British Columbia between Jan. 2020 and Apr. 15, 2023.\n\nThis data is the number of reported COVID-19 cases in British Columbia between January 2020 and April 15, 2023. The values are.up-to-date as of August 2023. ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\n\nGroup built {rtestim} doing for this nonparametrically.\nWe may come back to this later…"
  },
  {
    "objectID": "slides/day1-afternoon.html#essentials-of-dplyr-and-tidyr",
    "href": "slides/day1-afternoon.html#essentials-of-dplyr-and-tidyr",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Essentials of {dplyr} and {tidyr}",
    "text": "Essentials of {dplyr} and {tidyr}"
  },
  {
    "objectID": "slides/day1-afternoon.html#epi.-data-processing-with-epiprocess-3",
    "href": "slides/day1-afternoon.html#epi.-data-processing-with-epiprocess-3",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Epi. data processing with epiprocess",
    "text": "Epi. data processing with epiprocess\nAlternatively, we can display both the smoothed and the original daily case rates:\n\nNow, before exploring some more features of epiprocess, let’s have a look at the epiverse software ecosystem it’s part of…"
  },
  {
    "objectID": "slides/day1-afternoon.html#features---systematic-lag-analysis-1",
    "href": "slides/day1-afternoon.html#features---systematic-lag-analysis-1",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Features - Systematic lag analysis",
    "text": "Features - Systematic lag analysis\n\nThe strongest correlation occurs at a lag of about 23 days, indicating that case rates are best correlated with death rates 23 days from now."
  },
  {
    "objectID": "slides/day1-afternoon.html#autoplots-rollout",
    "href": "slides/day1-afternoon.html#autoplots-rollout",
    "title": "MICOM EpiData Workshop 2025",
    "section": "autoplots rollout!",
    "text": "autoplots rollout!\n\narchive_cases_dv_subset_all_states %&gt;%\n  autoplot(.facet_filter = geo_value %in% c(\"ca\", \"ut\"), .versions = \"1 month\")"
  },
  {
    "objectID": "slides/day1-afternoon.html#visualize-revision-patterns-autoplots-roll-out",
    "href": "slides/day1-afternoon.html#visualize-revision-patterns-autoplots-roll-out",
    "title": "MICOM EpiData Workshop 2025",
    "section": "Visualize revision patterns: Autoplots roll out!",
    "text": "Visualize revision patterns: Autoplots roll out!\n\narchive_cases_dv_subset_all_states %&gt;%\n  autoplot(\n    .facet_filter = geo_value %in% c(\"ca\", \"ut\"),\n    .versions = \"1 month\",\n    .mark_versions = TRUE)"
  }
]